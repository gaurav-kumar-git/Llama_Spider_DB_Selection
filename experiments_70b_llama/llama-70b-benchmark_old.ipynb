{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c999f2fd-56f0-4ae9-949c-4803f663a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.32.3)\n",
      "Requirement already satisfied: tqdm in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: psutil in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: sympy in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate bitsandbytes sentencepiece pandas datasets huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc106017-bfb9-40d0-8409-ff72140b945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets version: 8.1.5\n",
      "ipywidgets location: /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/ipywidgets/__init__.py\n",
      "tqdm version: 4.67.1\n",
      "tqdm location: /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/tqdm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "  import ipywidgets\n",
    "  print(f\"ipywidgets version: {ipywidgets.__version__}\")\n",
    "  print(f\"ipywidgets location: {ipywidgets.__file__}\")\n",
    "\n",
    "  import tqdm\n",
    "  print(f\"tqdm version: {tqdm.__version__}\")\n",
    "  print(f\"tqdm location: {tqdm.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48f920e-9675-4ae6-ac4a-e221b3e0175a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm imported successfully from .auto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c272cbb6d8b743b2968d2e062b4485e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minimal Auto Test:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tqdm .auto loop completed\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"tqdm imported successfully from .auto\")\n",
    "my_list = list(range(3))\n",
    "for i in tqdm(my_list, desc=\"Minimal Auto Test\"):\n",
    "    time.sleep(0.2)\n",
    "print(\"Simple tqdm .auto loop completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51bd609-5bef-4bae-9977-8909136a2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
      "PyTorch Version: 2.2.0\n",
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8aed063-c3c2-49b9-ba1c-3ec07ec68adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0\n",
      "CUDA available: True\n",
      "CUDA version PyTorch compiled with: 11.8\n",
      "Number of GPUs available to PyTorch: 8\n",
      "  GPU 0: NVIDIA A100-SXM4-80GB\n",
      "  GPU 1: NVIDIA A100-SXM4-80GB\n",
      "  GPU 2: NVIDIA A100-SXM4-80GB\n",
      "  GPU 3: NVIDIA A100-SXM4-80GB\n",
      "  GPU 4: NVIDIA A100-SXM4-80GB\n",
      "  GPU 5: NVIDIA A100-SXM4-80GB\n",
      "  GPU 6: NVIDIA A100-SXM4-80GB\n",
      "  GPU 7: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch compiled with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs available to PyTorch: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"ERROR: PyTorch cannot see the GPUs! Check installation and CUDA compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba6d845-eeb6-4dc3-8936-00a0686991c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
      "PyTorch Version: 2.2.0\n",
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac563a7e-b468-4b6e-9b89-ea76a46e229b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad89ac3a1c44351a5bb00678d014d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful or already authenticated.\n",
      "\n",
      "--- Cell 2: Hugging Face Login Attempt Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Hugging Face Hub Authentication ---\n",
    "# You MUST have requested access to Llama 2 models via Meta's form on Hugging Face\n",
    "# AND have your request approved.\n",
    "\n",
    "# Option 1: If you've stored your token as an environment variable on the server\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "# if HF_TOKEN:\n",
    "#     print(\"Logging into Hugging Face Hub using token from environment variable...\")\n",
    "#     login(token=HF_TOKEN)\n",
    "# else:\n",
    "#     print(\"HF_TOKEN environment variable not set. Attempting widget login if in interactive environment, or manual CLI login might be needed.\")\n",
    "#     login() # Will prompt if in an environment that supports it\n",
    "\n",
    "# Option 2: Paste token directly (less secure, use with caution)\n",
    "# HF_TOKEN = \"YOUR_HF_READ_TOKEN_HERE\"\n",
    "# login(token=HF_TOKEN)\n",
    "\n",
    "# Option 3: Use huggingface-cli login in a server terminal beforehand (Recommended)\n",
    "# If already logged in via CLI, this cell might not be strictly necessary,\n",
    "# but running login() can confirm status or refresh credentials.\n",
    "try:\n",
    "    login() # Will use cached token or prompt if needed\n",
    "    print(\"Hugging Face login successful or already authenticated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}. Ensure you are authenticated to download Llama 2.\")\n",
    "\n",
    "print(\"\\n--- Cell 2: Hugging Face Login Attempt Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "339f0694-4129-4a0a-9bec-150838594c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model: meta-llama/Llama-2-70b-chat-hf\n",
      "BitsAndBytesConfig: load_in_4bit=True, compute_dtype=torch.bfloat16\n",
      "System and User prompt templates defined.\n",
      "Hugging Face model cache directory set to: /raid/infolab/gaurav/Llama_Spider_A100_Project/experiments_70b_llama/.hf_model_cache_70b\n",
      "\n",
      "--- Cell 3: Model and Prompt Configuration Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Tokenizer Configuration ---\n",
    "import os\n",
    "\n",
    "# 3.1. Specify the Llama 2 70B Chat Model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "print(f\"Target Model: {MODEL_NAME}\")\n",
    "\n",
    "# 3.2. Configure 4-bit Quantization (essential for 70B, even on A100s for single/few GPU use)\n",
    "# A100s support bfloat16, which is excellent for mixed-precision.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # nf4 is a good default\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation on A100s\n",
    "    bnb_4bit_use_double_quant=True,   # Can save a bit more memory\n",
    ")\n",
    "print(f\"BitsAndBytesConfig: load_in_4bit={bnb_config.load_in_4bit}, compute_dtype={bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# 3.3. Define Prompt Templates\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert data analyst. Your task is to determine if a given natural language query \"\n",
    "    \"can be answered *solely* based on the provided database schema. \"\n",
    "    \"Do not attempt to answer the query itself. Your entire response must be only the word 'Yes' or the word 'No'.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Database Schema:\n",
    "---\n",
    "{schema_string}\n",
    "---\n",
    "Natural Language Query: \"{nl_query}\"\n",
    "---\n",
    "Can the query be answered using *only* the provided schema and its potential contents? Answer with either \"Yes\" or \"No\".\n",
    "\"\"\"\n",
    "print(\"System and User prompt templates defined.\")\n",
    "\n",
    "# 3.4. Define Cache Directory for Hugging Face downloads (optional, but good for managing large models)\n",
    "# Create it within your project directory on the A100 server.\n",
    "HF_MODEL_CACHE_DIR = os.path.join(os.getcwd(), \".hf_model_cache_70b\") # Assumes current dir is project root\n",
    "os.makedirs(HF_MODEL_CACHE_DIR, exist_ok=True)\n",
    "print(f\"Hugging Face model cache directory set to: {HF_MODEL_CACHE_DIR}\")\n",
    "\n",
    "print(\"\\n--- Cell 3: Model and Prompt Configuration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd45122-14da-4134-8b16-2732bbe33a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-2-70b-chat-hf...\n",
      "Set tokenizer.pad_token to tokenizer.eos_token ('</s>')\n",
      "Tokenizer loaded successfully.\n",
      "Found single token for 'Yes': ID 3869\n",
      "Found single token for 'No': ID 1939\n",
      "GLOBAL YES_TOKEN_ID: 3869 ('Yes')\n",
      "GLOBAL NO_TOKEN_ID: 1939 ('No')\n",
      "\n",
      "--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer and Define Yes/No Token Logic ---\n",
    "\n",
    "# 4.1. Load Tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HF_MODEL_CACHE_DIR)\n",
    "    # Set pad token if not already set (Llama tokenizers often don't have one)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set tokenizer.pad_token to tokenizer.eos_token ('{tokenizer.eos_token}')\")\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load tokenizer for {MODEL_NAME}: {e}\")\n",
    "\n",
    "\n",
    "# 4.2. Define Helper Function to get Yes/No Token IDs\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"Determines token IDs for 'Yes'/'No', preferring those with a leading space.\"\"\"\n",
    "    # Try with leading space first for chat models\n",
    "    yes_variants = [\" Yes\", \"Yes\"]\n",
    "    no_variants = [\" No\", \"No\"]\n",
    "    \n",
    "    final_yes_id = None\n",
    "    final_no_id = None\n",
    "\n",
    "    for variant in yes_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_yes_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_yes_id}\")\n",
    "            break\n",
    "            \n",
    "    for variant in no_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_no_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_no_id}\")\n",
    "            break\n",
    "\n",
    "    if final_yes_id is None or final_no_id is None:\n",
    "        print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or variants.\")\n",
    "        # You might want to print detailed tokenization attempts here if this error occurs\n",
    "        raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Cannot proceed.\")\n",
    "    \n",
    "    return final_yes_id, final_no_id\n",
    "\n",
    "# 4.3. Define Global YES_TOKEN_ID and NO_TOKEN_ID\n",
    "try:\n",
    "    YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "    print(f\"GLOBAL YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID]).strip()}')\")\n",
    "    print(f\"GLOBAL NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID]).strip()}')\")\n",
    "except ValueError as e:\n",
    "    raise RuntimeError(f\"Failed to set YES/NO token IDs: {e}\")\n",
    "\n",
    "print(\"\\n--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546dc473-9b23-4b0d-aff2-a1064c893deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-70b-chat-hf with 4-bit quantization. This will take significant time and memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa72e2cbbb74039a7292c3df605ea04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n",
      "Time taken to load model: 47.00 seconds.\n",
      "Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 4, 'model.layers.46': 4, 'model.layers.47': 4, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 5, 'model.layers.55': 5, 'model.layers.56': 5, 'model.layers.57': 5, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 6, 'model.layers.64': 6, 'model.layers.65': 6, 'model.layers.66': 6, 'model.layers.67': 6, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.layers.72': 7, 'model.layers.73': 7, 'model.layers.74': 7, 'model.layers.75': 7, 'model.layers.76': 7, 'model.layers.77': 7, 'model.layers.78': 7, 'model.layers.79': 7, 'model.norm': 7, 'model.rotary_emb': 7, 'lm_head': 7}\n",
      "Performed memory cleanup (torch.cuda.empty_cache(), gc.collect())\n",
      "\n",
      "--- Cell 5: Llama 2 70B Model Loading Complete ---\n",
      "Model max_position_embeddings: 4096\n",
      "Tokenizer model_max_length: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Llama 2 70B Model ---\n",
    "# This is a memory-intensive step. `device_map=\"auto\"` will attempt to distribute\n",
    "# the model across available GPUs if one is insufficient.\n",
    "# Ensure CUDA_VISIBLE_DEVICES is set in your shell if you want to restrict which GPUs are used.\n",
    "import gc\n",
    "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization. This will take significant time and memory...\")\n",
    "model_load_start_time = time.time()\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,    # Apply 4-bit quantization\n",
    "        torch_dtype=torch.bfloat16,        # Use bfloat16 on A100s\n",
    "        device_map=\"auto\",                 # Distribute model across available GPUs automatically\n",
    "        trust_remote_code=True,            # Often needed for newer models\n",
    "        cache_dir=HF_MODEL_CACHE_DIR\n",
    "    )\n",
    "    model_load_end_time = time.time()\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Time taken to load model: {model_load_end_time - model_load_start_time:.2f} seconds.\")\n",
    "    print(f\"Model device map: {model.hf_device_map}\") # Shows how layers are distributed\n",
    "    # For a 70B model, this should show parts on different GPUs if more than one is used.\n",
    "    \n",
    "    # Perform a quick memory cleanup after loading large model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Performed memory cleanup (torch.cuda.empty_cache(), gc.collect())\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(f\"Failed to load model {MODEL_NAME}: {e}. Check VRAM, CUDA setup, and Hugging Face authentication.\")\n",
    "\n",
    "print(\"\\n--- Cell 5: Llama 2 70B Model Loading Complete ---\")\n",
    "\n",
    "print(\"Model max_position_embeddings:\", model.config.max_position_embeddings)\n",
    "print(\"Tokenizer model_max_length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fae66bb-f328-4814-9e23-cbc376eca2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started. Looking for zip file at: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip\n",
      "Zip file found at /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip.\n",
      "Attempting to unzip /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip to /raid/infolab/gaurav/Llama_Spider_A100_Project/...\n",
      "Successfully unzipped files to /raid/infolab/gaurav/Llama_Spider_A100_Project/\n",
      "Contents of /raid/infolab/gaurav/Llama_Spider_A100_Project/:\n",
      "  - experiments_70b_llama\n",
      "  - .gitignore\n",
      "  - backup_to_github.sh\n",
      "  - Miniconda3-latest-Linux-x86_64.sh\n",
      "  - spider_subset_data.zip\n",
      "  - randomQ_allDBs_run1\n",
      "  - .ipynb_checkpoints\n",
      "  - .git\n",
      "  - miniconda3\n",
      "  - 100_queries.txt\n",
      "  - spider_subset_data\n",
      "  - __MACOSX\n",
      "\n",
      "Verifying extracted file paths...\n",
      "SUCCESS: dev.json path is valid: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/dev.json\n",
      "SUCCESS: tables.json path is valid: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json\n",
      "\n",
      "--- Ready to load data ---\n",
      "Path to dev.json: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/dev.json\n",
      "Path to tables.json: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "SERVER_ZIP_FILE_PATH = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip'\n",
    "EXTRACTION_DESTINATION_DIR_ON_SERVER = '/raid/infolab/gaurav/Llama_Spider_A100_Project/'\n",
    "\n",
    "DEV_JSON_PATH = None\n",
    "TABLES_JSON_PATH = None\n",
    "\n",
    "def unzip_data(zip_filepath, dest_dir):\n",
    "    \"\"\"\n",
    "    Unzips a zip file to a specified destination directory.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to unzip {zip_filepath} to {dest_dir}...\")\n",
    "    try:\n",
    "        \n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "        print(f\"Successfully unzipped files to {dest_dir}\")\n",
    "\n",
    "        print(f\"Contents of {dest_dir}:\")\n",
    "        for item in os.listdir(dest_dir):\n",
    "            print(f\"  - {item}\")\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_filepath} is not a valid zip file or is corrupted.\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Zip file not found at {zip_filepath}. Please ensure the path is correct.\")\n",
    "        return False\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied to write to {dest_dir} or read {zip_filepath}.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during unzipping: {e}\")\n",
    "        return False\n",
    "\n",
    "print(f\"Script started. Looking for zip file at: {SERVER_ZIP_FILE_PATH}\")\n",
    "\n",
    "if os.path.exists(SERVER_ZIP_FILE_PATH):\n",
    "    print(f\"Zip file found at {SERVER_ZIP_FILE_PATH}.\")\n",
    "    if unzip_data(SERVER_ZIP_FILE_PATH, EXTRACTION_DESTINATION_DIR_ON_SERVER):\n",
    "        \n",
    "        EXPECTED_EXTRACTED_FOLDER_NAME = 'spider_subset_data' # This is the folder INSIDE the zip\n",
    "\n",
    "        DEV_JSON_PATH = os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME, 'dev.json')\n",
    "        TABLES_JSON_PATH = os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME, 'tables.json')\n",
    "\n",
    "        print(\"\\nVerifying extracted file paths...\")\n",
    "        if os.path.exists(DEV_JSON_PATH):\n",
    "            print(f\"SUCCESS: dev.json path is valid: {DEV_JSON_PATH}\")\n",
    "        else:\n",
    "            print(f\"ERROR: dev.json NOT FOUND at expected path: {DEV_JSON_PATH}\")\n",
    "            print(f\"Please check the contents of {os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME)}\")\n",
    "\n",
    "\n",
    "        if os.path.exists(TABLES_JSON_PATH):\n",
    "            print(f\"SUCCESS: tables.json path is valid: {TABLES_JSON_PATH}\")\n",
    "        else:\n",
    "            print(f\"ERROR: tables.json NOT FOUND at expected path: {TABLES_JSON_PATH}\")\n",
    "            print(f\"Please check the contents of {os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME)}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Unzipping failed on the server. Cannot define data paths.\")\n",
    "else:\n",
    "    print(f\"ERROR: Zip file NOT FOUND at {SERVER_ZIP_FILE_PATH} on the server.\")\n",
    "    print(\"Please ensure the 'scp' command was successful and the path is correct.\")\n",
    "\n",
    "\n",
    "if DEV_JSON_PATH and TABLES_JSON_PATH and os.path.exists(DEV_JSON_PATH) and os.path.exists(TABLES_JSON_PATH):\n",
    "    print(\"\\n--- Ready to load data ---\")\n",
    "    print(f\"Path to dev.json: {DEV_JSON_PATH}\")\n",
    "    print(f\"Path to tables.json: {TABLES_JSON_PATH}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n--- Data paths are not correctly set up. Cannot proceed with data loading. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b93479-df37-4b4b-9f80-44c9222dba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1034 queries from dev.json\n",
      "Loaded 166 database schemas from tables.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"ERROR: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "dev_data = load_json_data(DEV_JSON_PATH)\n",
    "tables_data = load_json_data(TABLES_JSON_PATH)\n",
    "\n",
    "if dev_data and tables_data:\n",
    "    print(f\"Loaded {len(dev_data)} queries from dev.json\")\n",
    "    print(f\"Loaded {len(tables_data)} database schemas from tables.json\")\n",
    "else:\n",
    "    print(\"Failed to load Spider data. Please check paths and upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63c304d6-7712-4138-9d36-cbab1a8e225d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Preparing Database Schema SQL Strings (Dictionary Output) ---\n",
      "Loaded schema data for 166 databases from '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'.\n",
      "Successfully populated `all_db_schemas_sql_strings` dictionary with 166 entries.\n",
      "\n",
      "--- Verification of all_db_schemas_sql_strings ---\n",
      "Type: <class 'dict'>\n",
      "Number of schemas processed: 166\n",
      "Sample - DB ID: college_2\n",
      "Sample - SQL String :\n",
      "CREATE TABLE classroom (\n",
      "  building TEXT PRIMARY KEY,\n",
      "  room_number TEXT,\n",
      "  capacity REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE department (\n",
      "  dept_name TEXT PRIMARY KEY,\n",
      "  building TEXT,\n",
      "  budget REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE course (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  title TEXT,\n",
      "  dept_name TEXT,\n",
      "  credits REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE instructor (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  name TEXT,\n",
      "  dept_name TEXT,\n",
      "  salary REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE section (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year REAL,\n",
      "  building TEXT,\n",
      "  room_number TEXT,\n",
      "  time_slot_id TEXT,\n",
      "  FOREIGN KEY (building) REFERENCES classroom (building),\n",
      "  FOREIGN KEY (room_number) REFERENCES classroom (room_number),\n",
      "  FOREIGN KEY (course_id) REFERENCES course (course_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE teaches (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  course_id TEXT,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year INTEGER,\n",
      "  FOREIGN KEY (ID) REFERENCES instructor (ID),\n",
      "  FOREIGN KEY (course_id) REFERENCES section (course_id),\n",
      "  FOREIGN KEY (sec_id) REFERENCES section (sec_id),\n",
      "  FOREIGN KEY (semester) REFERENCES section (semester),\n",
      "  FOREIGN KEY (year) REFERENCES section (year)\n",
      ");\n",
      "\n",
      "CREATE TABLE student (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  name TEXT,\n",
      "  dept_name TEXT,\n",
      "  tot_cred REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE takes (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  course_id TEXT,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year INTEGER,\n",
      "  grade TEXT,\n",
      "  FOREIGN KEY (ID) REFERENCES student (ID),\n",
      "  FOREIGN KEY (course_id) REFERENCES section (course_id),\n",
      "  FOREIGN KEY (sec_id) REFERENCES section (sec_id),\n",
      "  FOREIGN KEY (semester) REFERENCES section (semester),\n",
      "  FOREIGN KEY (year) REFERENCES section (year)\n",
      ");\n",
      "\n",
      "CREATE TABLE advisor (\n",
      "  s_ID TEXT PRIMARY KEY,\n",
      "  i_ID TEXT,\n",
      "  FOREIGN KEY (s_ID) REFERENCES student (ID),\n",
      "  FOREIGN KEY (i_ID) REFERENCES instructor (ID)\n",
      ");\n",
      "\n",
      "CREATE TABLE time_slot (\n",
      "  time_slot_id TEXT PRIMARY KEY,\n",
      "  day TEXT,\n",
      "  start_hr REAL,\n",
      "  start_min REAL,\n",
      "  end_hr REAL,\n",
      "  end_min REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE prereq (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  prereq_id TEXT,\n",
      "  FOREIGN KEY (prereq_id) REFERENCES course (course_id),\n",
      "  FOREIGN KEY (course_id) REFERENCES course (course_id)\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# earlier\n",
    "import json\n",
    "# import os # Not strictly needed for this dictionary creation unless used in paths\n",
    "# import traceback # Only needed if you keep the full traceback print in except\n",
    "\n",
    "# --- Helper Functions (These are the same as you provided) ---\n",
    "def load_schemas(tables_json_path):\n",
    "    \"\"\"Loads schemas from tables.json into a dictionary keyed by db_id.\"\"\"\n",
    "    with open(tables_json_path, 'r') as f:\n",
    "        schemas_list = json.load(f)\n",
    "    schemas_dict = {db_info['db_id']: db_info for db_info in schemas_list}\n",
    "    return schemas_dict\n",
    "\n",
    "def map_spider_type_to_sql_type(spider_type, is_pk_or_fk=False):\n",
    "    \"\"\"Maps Spider's generic types to SQLite data types.\"\"\"\n",
    "    spider_type = spider_type.lower()\n",
    "    if spider_type == \"text\":\n",
    "        return \"TEXT\"\n",
    "    elif spider_type == \"number\":\n",
    "        return \"INTEGER\" if is_pk_or_fk else \"REAL\"\n",
    "    elif spider_type == \"time\":\n",
    "        return \"DATETIME\"\n",
    "    elif spider_type == \"boolean\":\n",
    "        return \"BOOLEAN\"\n",
    "    elif spider_type == \"others\":\n",
    "        return \"BLOB\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "def escape_sql_identifier(name):\n",
    "    \"\"\"Escapes SQL identifiers (table/column names) if they contain spaces or are keywords.\"\"\"\n",
    "    if \" \" in name or name.lower() in {\"select\", \"from\", \"where\", \"table\", \"primary\", \"key\", \"foreign\", \"index\", \"order\", \"group\"}:\n",
    "        return f'\"{name}\"'\n",
    "    return name\n",
    "\n",
    "def generate_create_table_sql_for_db(db_id, all_schemas_data): # Parameter name changed for consistency\n",
    "    \"\"\"\n",
    "    Generates SQL CREATE TABLE statements for a given db_id from the Spider schema.\n",
    "    'all_schemas_data' is the dictionary produced by load_schemas.\n",
    "    \"\"\"\n",
    "    if db_id not in all_schemas_data:\n",
    "        return f\"-- Database ID '{db_id}' not found in schemas.\"\n",
    "\n",
    "    db_schema = all_schemas_data[db_id] # Get the specific schema info for this db_id\n",
    "    sql_statements = []\n",
    "    column_info_by_index = {}\n",
    "    for i, (table_idx, col_name_original) in enumerate(db_schema['column_names_original']):\n",
    "        if col_name_original == \"*\":\n",
    "            continue\n",
    "        column_info_by_index[i] = {\n",
    "            \"original_name\": col_name_original,\n",
    "            \"table_index\": table_idx,\n",
    "            \"original_table_name\": db_schema['table_names_original'][table_idx],\n",
    "            \"type\": db_schema['column_types'][i]\n",
    "        }\n",
    "    for table_idx, table_name_original in enumerate(db_schema['table_names_original']):\n",
    "        escaped_table_name = escape_sql_identifier(table_name_original)\n",
    "        column_definitions = []\n",
    "        table_constraints = []\n",
    "        current_table_columns = []\n",
    "        for col_global_idx, (tbl_idx_for_col, col_name_orig) in enumerate(db_schema['column_names_original']):\n",
    "            if col_name_orig == \"*\":\n",
    "                continue\n",
    "            if tbl_idx_for_col == table_idx:\n",
    "                current_table_columns.append({\n",
    "                    \"global_idx\": col_global_idx,\n",
    "                    \"name\": col_name_orig,\n",
    "                    \"type\": db_schema['column_types'][col_global_idx]\n",
    "                })\n",
    "        pk_column_indices_for_table = [\n",
    "            pk_idx for pk_idx in db_schema['primary_keys']\n",
    "            if column_info_by_index.get(pk_idx) and column_info_by_index[pk_idx]['table_index'] == table_idx\n",
    "        ]\n",
    "        pk_column_names_for_table = [column_info_by_index[idx]['original_name'] for idx in pk_column_indices_for_table]\n",
    "        for col_data in current_table_columns:\n",
    "            col_name_original = col_data['name']\n",
    "            spider_type = col_data['type']\n",
    "            col_global_idx = col_data['global_idx']\n",
    "            is_pk_col = col_global_idx in pk_column_indices_for_table\n",
    "            is_fk_col = any(fk_pair[0] == col_global_idx for fk_pair in db_schema['foreign_keys'])\n",
    "            sql_type = map_spider_type_to_sql_type(spider_type, is_pk_or_fk=(is_pk_col or is_fk_col))\n",
    "            escaped_col_name = escape_sql_identifier(col_name_original)\n",
    "            col_def_str = f\"{escaped_col_name} {sql_type}\"\n",
    "            if is_pk_col and len(pk_column_names_for_table) == 1:\n",
    "                col_def_str += \" PRIMARY KEY\"\n",
    "            column_definitions.append(col_def_str)\n",
    "        if len(pk_column_names_for_table) > 1:\n",
    "            escaped_pk_cols = [escape_sql_identifier(name) for name in pk_column_names_for_table]\n",
    "            table_constraints.append(f\"PRIMARY KEY ({', '.join(escaped_pk_cols)})\")\n",
    "        for fk_col_idx, referenced_col_idx in db_schema['foreign_keys']:\n",
    "            if column_info_by_index.get(fk_col_idx) and \\\n",
    "               column_info_by_index.get(referenced_col_idx) and \\\n",
    "               column_info_by_index[fk_col_idx]['table_index'] == table_idx:\n",
    "                fk_column_name = column_info_by_index[fk_col_idx]['original_name']\n",
    "                referenced_table_name = column_info_by_index[referenced_col_idx]['original_table_name']\n",
    "                referenced_column_name = column_info_by_index[referenced_col_idx]['original_name']\n",
    "                escaped_fk_col = escape_sql_identifier(fk_column_name)\n",
    "                escaped_ref_table = escape_sql_identifier(referenced_table_name)\n",
    "                escaped_ref_col = escape_sql_identifier(referenced_column_name)\n",
    "                table_constraints.append(\n",
    "                    f\"FOREIGN KEY ({escaped_fk_col}) REFERENCES {escaped_ref_table} ({escaped_ref_col})\"\n",
    "                )\n",
    "        all_parts = column_definitions + table_constraints\n",
    "        create_table_statement = f\"CREATE TABLE {escaped_table_name} (\\n  \"\n",
    "        create_table_statement += \",\\n  \".join(all_parts)\n",
    "        create_table_statement += \"\\n);\"\n",
    "        sql_statements.append(create_table_statement)\n",
    "    return \"\\n\\n\".join(sql_statements)\n",
    "# --- End of Helper Functions ---\n",
    "\n",
    "\n",
    "# --- MODIFIED \"Main Execution\" for \"Cell 1\" to produce the dictionary ---\n",
    "# This code will be run when you execute the Jupyter cell.\n",
    "# The output variable needed by your experiment is `all_db_schemas_sql_strings`.\n",
    "\n",
    "all_db_schemas_sql_strings = {} # This is the dictionary your experiment needs\n",
    "\n",
    "# Define the path to your tables.json\n",
    "spider_tables_json_path = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'\n",
    "\n",
    "print(\"--- Cell 1: Preparing Database Schema SQL Strings (Dictionary Output) ---\")\n",
    "try:\n",
    "    # 1. Load all schema structures from tables.json\n",
    "    # `all_db_schemas_data_loaded` will be a dictionary: {db_id: schema_info_dict, ...}\n",
    "    all_db_schemas_data_loaded = load_schemas(spider_tables_json_path) # Renamed to avoid confusion with function parameter\n",
    "    print(f\"Loaded schema data for {len(all_db_schemas_data_loaded)} databases from '{spider_tables_json_path}'.\")\n",
    "\n",
    "    # 2. Iterate through each loaded schema and generate its SQL string, storing it in the dictionary\n",
    "    if all_db_schemas_data_loaded:\n",
    "        for db_id in all_db_schemas_data_loaded: # Iterate through keys (db_ids)\n",
    "            # Call generate_create_table_sql_for_db, passing the full loaded data\n",
    "            # and the current db_id.\n",
    "            sql_string_for_db = generate_create_table_sql_for_db(db_id, all_db_schemas_data_loaded)\n",
    "\n",
    "            # Store the raw SQL string in the dictionary.\n",
    "            # We only store it if it's a successful generation (doesn't start with the error message)\n",
    "            if sql_string_for_db and not sql_string_for_db.startswith(\"-- Database ID\"):\n",
    "                all_db_schemas_sql_strings[db_id] = sql_string_for_db\n",
    "            elif sql_string_for_db.startswith(\"-- Database ID\"):\n",
    "                print(f\"Warning: Schema for {db_id} reported as not found by generate_create_table_sql_for_db.\")\n",
    "            else:\n",
    "                print(f\"Warning: SQL generation returned empty or unexpected for {db_id} (Result: '{sql_string_for_db[:50]}...')\")\n",
    "\n",
    "        print(f\"Successfully populated `all_db_schemas_sql_strings` dictionary with {len(all_db_schemas_sql_strings)} entries.\")\n",
    "    else:\n",
    "        print(\"No schema data loaded from tables.json, so `all_db_schemas_sql_strings` will be empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The file '{spider_tables_json_path}' was not found.\")\n",
    "    all_db_schemas_sql_strings = {} # Ensure it's defined as empty on error\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"FATAL ERROR: Could not decode JSON from '{spider_tables_json_path}'. Check if it's a valid JSON file.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during schema preparation: {e}\")\n",
    "    # import traceback # Uncomment if you need the full traceback here\n",
    "    # traceback.print_exc()\n",
    "    all_db_schemas_sql_strings = {}\n",
    "\n",
    "# --- Verification (you can add this to your cell to check after it runs) ---\n",
    "print(f\"\\n--- Verification of all_db_schemas_sql_strings ---\")\n",
    "print(f\"Type: {type(all_db_schemas_sql_strings)}\")\n",
    "print(f\"Number of schemas processed: {len(all_db_schemas_sql_strings)}\")\n",
    "if all_db_schemas_sql_strings:\n",
    "    # Print a sample to verify content\n",
    "    sample_db_id = list(all_db_schemas_sql_strings.keys())[1]\n",
    "    print(f\"Sample - DB ID: {sample_db_id}\")\n",
    "    print(f\"Sample - SQL String :\\n{all_db_schemas_sql_strings[sample_db_id]}\")\n",
    "else:\n",
    "    print(\"`all_db_schemas_sql_strings` is empty. Review errors above.\")\n",
    "# --- End of Cell 1 Logic ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2452e5b-6f75-47d1-bc17-276b3afc9b16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Random Sampling\n",
    "\n",
    "# This cell defines parameters for running the experiment.\n",
    "# It will now randomly select queries and always use ALL database schemas as candidates.\n",
    "\n",
    "import random # Ensure random is imported at the top of your notebook or this cell\n",
    "# import os # Ensure os is imported (likely already done for path joining)\n",
    "# import json # Ensure json is imported (likely already done for loading)\n",
    "\n",
    "# --- 2.1. Experiment Parameters ---\n",
    "# Number of NL queries to RANDOMLY select from dev.json to process.\n",
    "# For initial testing in Colab, use a small subset. For a more thorough run, increase this.\n",
    "NUM_RANDOM_QUERIES_TO_TEST = 100 # For example, test 5 random queries\n",
    "\n",
    "# This will now effectively always be True based on your requirement.\n",
    "# The logic will be set up to use all schemas from all_db_schemas_sql_strings.\n",
    "# We can keep the variable for clarity or remove it if it's always all DBs.\n",
    "# For this implementation, let's explicitly aim for all DBs.\n",
    "print(\"INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\")\n",
    "\n",
    "\n",
    "# --- 2.2. Randomly Select NL Queries for the Experiment ---\n",
    "# We will randomly sample NUM_RANDOM_QUERIES_TO_TEST queries from the loaded dev_data.\n",
    "if not dev_data: # dev_data should have been loaded in Cell 1\n",
    "    raise ValueError(\"dev_data is not loaded (from dev.json). Cannot select queries. Please run Cell 1 first.\")\n",
    "\n",
    "if len(dev_data) == 0:\n",
    "    raise ValueError(\"dev_data is empty. No queries to select.\")\n",
    "\n",
    "actual_num_queries_to_select = min(NUM_RANDOM_QUERIES_TO_TEST, len(dev_data))\n",
    "# Using min ensures we don't try to sample more queries than available.\n",
    "\n",
    "if actual_num_queries_to_select < NUM_RANDOM_QUERIES_TO_TEST:\n",
    "    print(f\"Warning: Requested {NUM_RANDOM_QUERIES_TO_TEST} random queries, but only {len(dev_data)} are available. Using all {len(dev_data)} queries.\")\n",
    "\n",
    "# Randomly sample without replacement\n",
    "selected_nl_queries = random.sample(dev_data, actual_num_queries_to_select)\n",
    "\n",
    "print(f\"\\nRandomly selected {len(selected_nl_queries)} NL queries for the experiment:\")\n",
    "for i, q_info in enumerate(selected_nl_queries):\n",
    "    print(f\"  Test Query {i+1}: '{q_info['question']}' (True DB: {q_info['db_id']})\")\n",
    "\n",
    "\n",
    "# --- 2.3. Determine Candidate Database Schemas for Each Query ---\n",
    "# For this experiment design, we ALWAYS use ALL available database schemas.\n",
    "# all_db_schemas_sql_strings should have been populated in Cell 1.\n",
    "if not all_sql_output: # Populated in Cell 1\n",
    "    raise ValueError(\"all_sql_output is empty. Schemas were not converted in Cell 1. Cannot proceed.\")\n",
    "\n",
    "candidate_schemas_for_evaluation = all_db_schemas_sql_strings # Use all converted schemas\n",
    "print(f\"\\nEach of the {len(selected_nl_queries)} selected queries will be evaluated against all {len(candidate_schemas_for_evaluation)} available Spider database schemas.\")\n",
    "\n",
    "if not candidate_schemas_for_evaluation: # Should not happen if all_db_schemas_sql_strings was populated\n",
    "    raise ValueError(\"No candidate schemas available for evaluation. This indicates an issue with schema loading or conversion in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "588ef6c6-6baa-460f-92ac-1f24fa060b05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Preparing Database Schemas with PK/FK Annotations ---\n",
      "Loaded schema data for 166 databases from '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'.\n",
      "Populated `all_db_schemas_sql_strings` with 166 entries.\n",
      "\n",
      "--- Verification of all_db_schemas_sql_strings ---\n",
      "Type: <class 'dict'>\n",
      "Number of schemas processed: 166\n",
      "Sample - DB ID: yelp\n",
      "Sample - Schema Description:\n",
      "business: bid(REAL)[PK], business_id(TEXT), name(TEXT), full_address(TEXT), city(TEXT), latitude(TEXT), longitude(TEXT), review_count(REAL), is_open(REAL), rating(REAL), state(TEXT); category: id(REAL)[PK], business_id(TEXT)[FKâ†’business.business_id], category_name(TEXT); user: uid(REAL)[PK], user_id(TEXT), name(TEXT); checkin: cid(REAL)[PK], business_id(TEXT)[FKâ†’business.business_id], count(REAL), day(TEXT); neighbourhood: id(REAL)[PK], business_id(TEXT)[FKâ†’business.business_id], neighbourhood_name(TEXT); review: rid(REAL)[PK], business_id(TEXT)[FKâ†’business.business_id], user_id(TEXT)[FKâ†’user.user_id], rating(REAL), text(TEXT), year(REAL), month(TEXT); tip: tip_id(REAL)[PK], business_id(TEXT)[FKâ†’business.business_id], text(TEXT), user_id(TEXT)[FKâ†’user.user_id], likes(REAL), year(REAL), month(TEXT)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Helper Functions (modified to output schema with PK and FK relationships) ---\n",
    "\n",
    "def load_schemas(tables_json_path):\n",
    "    \"\"\"Loads schemas from tables.json into a dictionary keyed by db_id.\"\"\"\n",
    "    with open(tables_json_path, 'r') as f:\n",
    "        schemas_list = json.load(f)\n",
    "    schemas_dict = {db_info['db_id']: db_info for db_info in schemas_list}\n",
    "    return schemas_dict\n",
    "\n",
    "def map_spider_type_to_sql_type(spider_type):\n",
    "    \"\"\"Maps Spider's generic types to a concise SQLâ€style type string.\"\"\"\n",
    "    spider_type = spider_type.lower()\n",
    "    if spider_type == \"text\":\n",
    "        return \"TEXT\"\n",
    "    elif spider_type == \"number\":\n",
    "        return \"REAL\"\n",
    "    elif spider_type == \"time\":\n",
    "        return \"DATETIME\"\n",
    "    elif spider_type == \"boolean\":\n",
    "        return \"BOOLEAN\"\n",
    "    elif spider_type == \"others\":\n",
    "        return \"BLOB\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "def escape_sql_identifier(name):\n",
    "    \"\"\"Escapes identifiers if they contain spaces or are SQL keywords.\"\"\"\n",
    "    if \" \" in name or name.lower() in {\n",
    "        \"select\", \"from\", \"where\", \"table\", \"primary\", \"key\",\n",
    "        \"foreign\", \"index\", \"order\", \"group\"\n",
    "    }:\n",
    "        return f'\"{name}\"'\n",
    "    return name\n",
    "\n",
    "def generate_create_table_sql_for_db(db_id, all_schemas_data):\n",
    "    \"\"\"\n",
    "    Instead of producing CREATE TABLE statements, this returns a schema description string:\n",
    "    - For each table: \"table_name: col1(type)[PK], col2(type)[FK->refTable.refCol], col3(type), ...\"\n",
    "    It uses 'primary_keys' and 'foreign_keys' arrays from the Spider JSON.\n",
    "    \"\"\"\n",
    "    if db_id not in all_schemas_data:\n",
    "        return f\"-- Database ID '{db_id}' not found in schemas.\"\n",
    "    \n",
    "    db_schema = all_schemas_data[db_id]\n",
    "    table_names = db_schema['table_names_original']\n",
    "    column_names = db_schema['column_names_original']\n",
    "    column_types = db_schema['column_types']\n",
    "    pk_indices = set(db_schema.get('primary_keys', []))\n",
    "    fk_pairs = db_schema.get('foreign_keys', [])\n",
    "\n",
    "    # Build a lookup: column_index -> (table_idx, column_name, column_type)\n",
    "    col_info = {}\n",
    "    for idx, (tbl_idx, col_name) in enumerate(column_names):\n",
    "        if col_name == \"*\":\n",
    "            continue\n",
    "        col_info[idx] = {\n",
    "            \"table_index\": tbl_idx,\n",
    "            \"column_name\": col_name,\n",
    "            \"column_type\": column_types[idx]\n",
    "        }\n",
    "\n",
    "    # Initialize a structure: table_idx -> list of column descriptors\n",
    "    tables_columns = {i: [] for i in range(len(table_names))}\n",
    "\n",
    "    # For each column, determine if it's PK or FK or normal\n",
    "    for col_idx, info in col_info.items():\n",
    "        tbl_idx = info['table_index']\n",
    "        col_name = escape_sql_identifier(info['column_name'])\n",
    "        col_type = map_spider_type_to_sql_type(info['column_type'])\n",
    "\n",
    "        is_pk = (col_idx in pk_indices)\n",
    "        # Check if this column is a foreign key, and if so, note the referenced table/column\n",
    "        fk_description = \"\"\n",
    "        for (fk_col_idx, ref_col_idx) in fk_pairs:\n",
    "            if fk_col_idx == col_idx:\n",
    "                ref_info = col_info[ref_col_idx]\n",
    "                ref_table = escape_sql_identifier(ref_info['original_table_name'] if 'original_table_name' in ref_info else table_names[ref_info['table_index']])\n",
    "                ref_col_name = escape_sql_identifier(ref_info['column_name'])\n",
    "                fk_description = f\"[FKâ†’{ref_table}.{ref_col_name}]\"\n",
    "                break\n",
    "\n",
    "        pk_tag = \"[PK]\" if is_pk else \"\"\n",
    "        tables_columns[tbl_idx].append(f\"{col_name}({col_type}){pk_tag}{fk_description}\")\n",
    "\n",
    "    # Build the final schema string, one segment per table\n",
    "    segments = []\n",
    "    for tbl_idx, tbl_name in enumerate(table_names):\n",
    "        escaped_tbl = escape_sql_identifier(tbl_name)\n",
    "        col_list = tables_columns.get(tbl_idx, [])\n",
    "        cols_joined = \", \".join(col_list)\n",
    "        segments.append(f\"{escaped_tbl}: {cols_joined}\")\n",
    "\n",
    "    # Join table segments with semicolons\n",
    "    return \"; \".join(segments)\n",
    "\n",
    "# --- Main Execution (Cell 1) producing the dictionary ---\n",
    "all_db_schemas_sql_strings = {}  # Dictionary to hold schema descriptions\n",
    "\n",
    "# Path to tables.json\n",
    "spider_tables_json_path = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'\n",
    "\n",
    "print(\"--- Cell 1: Preparing Database Schemas with PK/FK Annotations ---\")\n",
    "try:\n",
    "    # 1. Load all schema structures\n",
    "    all_db_schemas_data_loaded = load_schemas(spider_tables_json_path)\n",
    "    print(f\"Loaded schema data for {len(all_db_schemas_data_loaded)} databases from '{spider_tables_json_path}'.\")\n",
    "\n",
    "    # 2. Iterate and generate schema descriptions\n",
    "    for db_id in all_db_schemas_data_loaded:\n",
    "        schema_desc = generate_create_table_sql_for_db(db_id, all_db_schemas_data_loaded)\n",
    "        if schema_desc and not schema_desc.startswith(\"-- Database ID\"):\n",
    "            all_db_schemas_sql_strings[db_id] = schema_desc\n",
    "        else:\n",
    "            print(f\"Warning: Could not generate schema for {db_id}\")\n",
    "\n",
    "    print(f\"Populated `all_db_schemas_sql_strings` with {len(all_db_schemas_sql_strings)} entries.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: File '{spider_tables_json_path}' not found.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"FATAL ERROR: Could not decode JSON from '{spider_tables_json_path}'.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during schema preparation: {e}\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\n--- Verification of all_db_schemas_sql_strings ---\")\n",
    "print(f\"Type: {type(all_db_schemas_sql_strings)}\")\n",
    "print(f\"Number of schemas processed: {len(all_db_schemas_sql_strings)}\")\n",
    "if all_db_schemas_sql_strings:\n",
    "    sample_db_id = list(all_db_schemas_sql_strings.keys())[125]\n",
    "    print(f\"Sample - DB ID: {sample_db_id}\")\n",
    "    print(f\"Sample - Schema Description:\\n{all_db_schemas_sql_strings[sample_db_id]}\")\n",
    "else:\n",
    "    print(\"`all_db_schemas_sql_strings` is empty. Check for errors above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d67f720-5263-4bb6-8db6-a4da23c1a95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\n",
      "Loaded 20 queries from '/raid/infolab/gaurav/Llama_Spider_A100_Project/100_queries.txt':\n",
      "  Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
      "  Query 2: 'What are names of countries with the top 3 largest population?' (True DB: world_1)\n",
      "  Query 3: 'What are the names and birth dates of people, ordered by their names in alphabetical order?' (True DB: poker_player)\n",
      "  Query 4: 'How many different store locations are there?' (True DB: employee_hire_evaluation)\n",
      "  Query 5: 'How many different nationalities do conductors have?' (True DB: orchestra)\n",
      "  Query 6: 'How many states are there?' (True DB: voter_1)\n",
      "  Query 7: 'What are the codes of template types that have fewer than 3 templates?' (True DB: cre_Doc_Template_Mgt)\n",
      "  Query 8: 'How many dogs have not gone through any treatment?' (True DB: dog_kennels)\n",
      "  Query 9: 'What are the template ids of any templates used in more than a single document?' (True DB: cre_Doc_Template_Mgt)\n",
      "  Query 10: 'Show name, country, age for all singers ordered by age from the oldest to the youngest.' (True DB: concert_singer)\n",
      "  Query 11: 'Show the student IDs and numbers of friends corresponding to each.' (True DB: network_1)\n",
      "  Query 12: 'What are flight numbers of flights arriving at City \"Aberdeen\"?' (True DB: flight_2)\n",
      "  Query 13: 'How many countries speak both English and Dutch?' (True DB: world_1)\n",
      "  Query 14: 'What are the notes of the death events which has substring 'East'?' (True DB: battle_death)\n",
      "  Query 15: 'What are the names of conductors as well as the corresonding orchestras that they have conducted?' (True DB: orchestra)\n",
      "  Query 16: 'List the earnings of poker players in descending order.' (True DB: poker_player)\n",
      "  Query 17: 'Which owner has paid the largest amount of money in total for their dogs? Show the owner id and zip code.' (True DB: dog_kennels)\n",
      "  Query 18: 'Find the number of flights landing in the city of Aberdeen or Abilene.' (True DB: flight_2)\n",
      "  Query 19: 'How many pets have a greater weight than 10?' (True DB: pets_1)\n",
      "  Query 20: 'Show different citizenships and the maximum net worth of singers of each citizenship.' (True DB: singer)\n",
      "\n",
      "Each of the 20 selected queries will be evaluated against all 166 available Spider database schemas.\n"
     ]
    }
   ],
   "source": [
    "# This cell defines parameters for running the experiment.\n",
    "# It will now randomly select queries and always use ALL database schemas as candidates.\n",
    "\n",
    "import random # Ensure random is imported at the top of your notebook or this cell\n",
    "# import os # Ensure os is imported (likely already done for path joining)\n",
    "# import json # Ensure json is imported (likely already done for loading)\n",
    "\n",
    "# --- 2.1. Experiment Parameters ---\n",
    "# Number of NL queries to RANDOMLY select from dev.json to process.\n",
    "# For initial testing in Colab, use a small subset. For a more thorough run, increase this.\n",
    "NUM_RANDOM_QUERIES_TO_TEST = 100 # For example, test 5 random queries\n",
    "\n",
    "# This will now effectively always be True based on your requirement.\n",
    "# The logic will be set up to use all schemas from all_db_schemas_sql_strings.\n",
    "# We can keep the variable for clarity or remove it if it's always all DBs.\n",
    "# For this implementation, let's explicitly aim for all DBs.\n",
    "print(\"INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\")\n",
    "\n",
    "\n",
    "# --- 2.2. Randomly Select NL Queries for the Experiment ---\n",
    "# We will randomly sample NUM_RANDOM_QUERIES_TO_TEST queries from the loaded dev_data.\n",
    "if not dev_data: # dev_data should have been loaded in Cell 1\n",
    "    raise ValueError(\"dev_data is not loaded (from dev.json). Cannot select queries. Please run Cell 1 first.\")\n",
    "\n",
    "if len(dev_data) == 0:\n",
    "    raise ValueError(\"dev_data is empty. No queries to select.\")\n",
    "\n",
    "actual_num_queries_to_select = min(NUM_RANDOM_QUERIES_TO_TEST, len(dev_data))\n",
    "# Using min ensures we don't try to sample more queries than available.\n",
    "\n",
    "if actual_num_queries_to_select < NUM_RANDOM_QUERIES_TO_TEST:\n",
    "    print(f\"Warning: Requested {NUM_RANDOM_QUERIES_TO_TEST} random queries, but only {len(dev_data)} are available. Using all {len(dev_data)} queries.\")\n",
    "\n",
    "# Randomly sample without replacementselected_nl_queries = random.sample(dev_data, actual_num_queries_to_select)\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Path to your text file containing lines like:\n",
    "#   Test Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
    "TEXT_QUERIES_FILE = \"/raid/infolab/gaurav/Llama_Spider_A100_Project/100_queries.txt\"\n",
    "\n",
    "if not os.path.exists(TEXT_QUERIES_FILE):\n",
    "    raise FileNotFoundError(f\"Cannot find '{TEXT_QUERIES_FILE}' â€“ make sure itâ€™s in your working directory or update the path.\")\n",
    "\n",
    "selected_nl_queries = []\n",
    "pattern = re.compile(r\"Test Query\\s+\\d+:\\s+'(.+)'\\s+\\(True DB:\\s*([^)]+)\\)\")\n",
    "\n",
    "with open(TEXT_QUERIES_FILE, \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        # Skip any header or nonâ€â€œTest Queryâ€ lines\n",
    "        if not line.startswith(\"Test Query\"):\n",
    "            continue\n",
    "\n",
    "        m = pattern.match(line)\n",
    "        if not m:\n",
    "            print(f\"Warning: could not parse line:\\n  {line}\")\n",
    "            continue\n",
    "\n",
    "        question_text = m.group(1)\n",
    "        true_db_id    = m.group(2)\n",
    "\n",
    "        # Build the same dictâ€structure downstream code expects\n",
    "        selected_nl_queries.append({\n",
    "            \"question\": question_text,\n",
    "            \"db_id\":    true_db_id\n",
    "        })\n",
    "\n",
    "if len(selected_nl_queries) == 0:\n",
    "    raise ValueError(f\"No queries were parsed from '{TEXT_QUERIES_FILE}'. Check your fileâ€™s format.\")\n",
    "\n",
    "print(f\"Loaded {len(selected_nl_queries)} queries from '{TEXT_QUERIES_FILE}':\")\n",
    "for i, q in enumerate(selected_nl_queries, 1):\n",
    "    print(f\"  Query {i}: '{q['question']}' (True DB: {q['db_id']})\")\n",
    "\n",
    "# --- 2.3. Determine Candidate Database Schemas for Each Query ---\n",
    "# For this experiment design, we ALWAYS use ALL available database schemas.\n",
    "# all_db_schemas_sql_strings should have been populated in Cell 1.\n",
    "# if not all_sql_output: # Populated in Cell 1\n",
    "#     raise ValueError(\"all_sql_output is empty. Schemas were not converted in Cell 1. Cannot proceed.\")\n",
    "\n",
    "candidate_schemas_for_evaluation = all_db_schemas_sql_strings # Use all converted schemas\n",
    "print(f\"\\nEach of the {len(selected_nl_queries)} selected queries will be evaluated against all {len(candidate_schemas_for_evaluation)} available Spider database schemas.\")\n",
    "\n",
    "if not candidate_schemas_for_evaluation: # Should not happen if all_db_schemas_sql_strings was populated\n",
    "    raise ValueError(\"No candidate schemas available for evaluation. This indicates an issue with schema loading or conversion in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "531bac23-fdaf-4552-a664-e0b0ae3ca27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured experiment project directory exists: '/raid/infolab/gaurav/Llama_Spider_A100_Project/randomQ_allDBs_run1'\n",
      "Experiment results will be saved to: /raid/infolab/gaurav/Llama_Spider_A100_Project/randomQ_allDBs_run1/spider_random_query_all_db_scores_prompt_llama-2.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "LOCAL_EXPERIMENT_BASE_DIR = \"/raid/infolab/gaurav/Llama_Spider_A100_Project/\"\n",
    "\n",
    "\n",
    "EXPERIMENT_RUN_NAME = \"randomQ_allDBs_run1\" \n",
    "EXPERIMENT_PROJECT_DIR = os.path.join(LOCAL_EXPERIMENT_BASE_DIR, EXPERIMENT_RUN_NAME)\n",
    "\n",
    "try:\n",
    "    os.makedirs(EXPERIMENT_PROJECT_DIR, exist_ok=True)\n",
    "    print(f\"Ensured experiment project directory exists: '{EXPERIMENT_PROJECT_DIR}'\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory {EXPERIMENT_PROJECT_DIR}: {e}\")\n",
    "    EXPERIMENT_PROJECT_DIR = \".\" \n",
    "\n",
    "\n",
    "RESULTS_FILENAME = \"spider_random_query_all_db_scores_prompt_llama-2.json\"\n",
    "EXPERIMENT_RESULTS_FILE = os.path.join(EXPERIMENT_PROJECT_DIR, RESULTS_FILENAME)\n",
    "\n",
    "print(f\"Experiment results will be saved to: {EXPERIMENT_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f06c3025-9d75-4217-8071-1418e90aaa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'get_yes_no_token_ids' defined (with actual logic).\n"
     ]
    }
   ],
   "source": [
    "# Cell defining get_yes_no_token_ids (CORRECTED)\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"\n",
    "    Determines the token IDs for 'Yes' and 'No', accounting for potential leading spaces.\n",
    "    Llama-2-chat tends to produce \" Yes\" or \" No\" as single tokens after the prompt.\n",
    "    \"\"\"\n",
    "    # Try with leading space first, as it's common for chat models\n",
    "    yes_token_id_with_space = tokenizer_arg.encode(\" Yes\", add_special_tokens=False)\n",
    "    no_token_id_with_space = tokenizer_arg.encode(\" No\", add_special_tokens=False)\n",
    "\n",
    "    if len(yes_token_id_with_space) == 1 and len(no_token_id_with_space) == 1:\n",
    "        print(\"Using ' Yes' and ' No' (with leading space) for Yes/No token IDs.\")\n",
    "        return yes_token_id_with_space[0], no_token_id_with_space[0] # Explicit return\n",
    "    else:\n",
    "        # Fallback to \"Yes\" and \"No\" without leading space\n",
    "        yes_token_id_no_space = tokenizer_arg.encode(\"Yes\", add_special_tokens=False)\n",
    "        no_token_id_no_space = tokenizer_arg.encode(\"No\", add_special_tokens=False)\n",
    "        if len(yes_token_id_no_space) == 1 and len(no_token_id_no_space) == 1:\n",
    "            print(\"Warning: Using 'Yes' and 'No' (no leading space) for Yes/No token IDs. This might be suboptimal for chat models.\")\n",
    "            return yes_token_id_no_space[0], no_token_id_no_space[0] # Explicit return\n",
    "        else:\n",
    "            # This case is problematic.\n",
    "            print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or ' Yes'/' No'.\")\n",
    "            print(f\"Tokenization of ' Yes': {yes_token_id_with_space} (decoded: {[tokenizer_arg.decode(t) for t in yes_token_id_with_space]})\")\n",
    "            print(f\"Tokenization of ' No': {no_token_id_with_space} (decoded: {[tokenizer_arg.decode(t) for t in no_token_id_with_space]})\")\n",
    "            print(f\"Tokenization of 'Yes': {yes_token_id_no_space} (decoded: {[tokenizer_arg.decode(t) for t in yes_token_id_no_space]})\")\n",
    "            print(f\"Tokenization of 'No': {no_token_id_no_space} (decoded: {[tokenizer_arg.decode(t) for t in no_token_id_no_space]})\")\n",
    "            # It's better to raise an error here so the problem is immediately obvious\n",
    "            # rather than returning None and causing a TypeError later.\n",
    "            raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Review tokenization outputs above. Cannot proceed without reliable Yes/No token IDs.\")\n",
    "\n",
    "print(\"Helper function 'get_yes_no_token_ids' defined (with actual logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "abb8b6cb-d43f-40a9-bf6e-6779c779201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using 'Yes' and 'No' (no leading space) for Yes/No token IDs. This might be suboptimal for chat models.\n",
      "YES_TOKEN_ID: 3869 ('Yes')\n",
      "NO_TOKEN_ID: 1939 ('No')\n"
     ]
    }
   ],
   "source": [
    "if 'tokenizer' in globals() and tokenizer is not None:\n",
    "    try:\n",
    "        YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "        print(f\"YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID])}')\")\n",
    "        print(f\"NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID])}')\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error defining YES/NO token IDs: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'tokenizer' is not defined. Cannot define YES_TOKEN_ID and NO_TOKEN_ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc6a8811-2963-48c5-9f50-0eba843139cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core function 'get_yes_probability' defined.\n"
     ]
    }
   ],
   "source": [
    "def get_yes_probability(model_arg, tokenizer_arg, system_prompt_arg, user_prompt_content_arg, yes_token_id_arg, no_token_id_arg, max_length=model.config.max_position_embeddings):\n",
    "    \"\"\"\n",
    "    Gets the probability of the model answering \"Yes\" to the given query and schema.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_arg},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_content_arg}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template and capture it in a variable\n",
    "    prompt_for_model = tokenizer_arg.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # <-- Insert print here to inspect the fully-formatted prompt\n",
    "    # print(\"=== Prompt After Chat Template ===\")\n",
    "    # print(prompt_for_model)\n",
    "    # print(\"=== End of Prompt ===\\n\")\n",
    "\n",
    "    inputs = tokenizer_arg(\n",
    "        prompt_for_model,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length - 10\n",
    "    )\n",
    "    inputs = {k: v.to(model_arg.device) for k, v in inputs.items()}\n",
    "\n",
    "    if inputs['input_ids'].shape[1] >= max_length - 10:\n",
    "        print(f\"Warning: Prompt for query was truncated. Length: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_arg(**inputs)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        logit_yes = next_token_logits[:, yes_token_id_arg].item()\n",
    "        logit_no = next_token_logits[:, no_token_id_arg].item()\n",
    "\n",
    "    max_logit = max(logit_yes, logit_no)\n",
    "    exp_yes = torch.exp(torch.tensor(logit_yes - max_logit, device=model_arg.device))\n",
    "    exp_no = torch.exp(torch.tensor(logit_no - max_logit, device=model_arg.device))\n",
    "\n",
    "    prob_yes = exp_yes / (exp_yes + exp_no)\n",
    "    return prob_yes.item()\n",
    "\n",
    "\n",
    "print(\"Core function 'get_yes_probability' defined.\")  # Add a print statement to confirm execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3dfe1828-56ee-4994-91e0-9ecc5acfb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Condensed Prompt Configuration (fits in a 2Kâ€token window) ---\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert analyst. Decide if a naturalâ€language question can be answered *only* from the given schema. \n",
    "If all required tables, columns, and joinâ€paths exist, respond with exactly â€œYesâ€. Otherwise, respond with exactly â€œNoâ€.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "# Fewâ€Shot Examples (Spider style)\n",
    "\n",
    "[Schema: student(student_id, student_name); course(course_id, course_name); enrollment(student_idâ†’student, course_idâ†’course)]\n",
    "Q: List the names of all students enrolled in the 'Math' course.\n",
    "Reasoning: enrollment links studentâ†”course; course_name exists; student_name exists â†’ SQL possible.\n",
    "A: Yes\n",
    "\n",
    "[Schema: orders(order_id, customer_id, amount); customer(customer_id, customer_name)]\n",
    "Q: Find the total number of orders placed in 2019.\n",
    "Reasoning: no order_date or year column â†’ cannot filter by 2019.\n",
    "A: No\n",
    "\n",
    "# Now Evaluate\n",
    "\n",
    "[Schema: {schema_string}]\n",
    "Q: {nl_query}\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a534772-a426-4df2-bb02-72aae58c82ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_yes_probability directly...\n",
      "get_yes_probability returned: 0.9724147915840149\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing get_yes_probability directly...\")\n",
    "try:\n",
    "    # Construct a very simple schema and query for testing\n",
    "    test_schema = \"CREATE TABLE TestTable (id INT, name TEXT, salary INT);\"\n",
    "    test_nl_query = \"random prompt score\"\n",
    "    sample_user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "        schema_string=test_schema,\n",
    "        nl_query=test_nl_query\n",
    "    )\n",
    "    # print(f\"Test User Prompt: {sample_user_prompt_content}\")\n",
    "\n",
    "    # Make sure all these variables are defined and loaded:\n",
    "    # model, tokenizer, SYSTEM_PROMPT, YES_TOKEN_ID, NO_TOKEN_ID\n",
    "    prob = get_yes_probability(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        SYSTEM_PROMPT,\n",
    "        sample_user_prompt_content,\n",
    "        YES_TOKEN_ID,\n",
    "        NO_TOKEN_ID\n",
    "    )\n",
    "    print(f\"get_yes_probability returned: {prob}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"Error during direct call to get_yes_probability:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "62edcddb-f870-4d78-ba30-b568fc757b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating with SAMPLING (do_sample=True) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Model Response: 'No, the given schema does not contain any'\n",
      "\n",
      "--- Generating with GREEDY DECODING (do_sample=False) ---\n",
      "Greedy Model Response: 'No, the given schema does not contain any information about iPhones or their launch dates. The schema is focused on employee and project data, with no mention of products or launch dates. Therefore, it is not possible to answer the question \"Name the latest iPhone launched by APPLE\" based solely on the given schema.\n",
      "\n",
      "Answer: No'\n",
      "\n",
      "--- For reference: get_yes_probability output ---\n",
      "P(Yes) from get_yes_probability: 0.9846\n",
      "Based on get_yes_probability, 'Yes' is more likely.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# --- 1. Create a Sample Input (Schema + Query) ---\n",
    "# You can use any example; this is just for demonstration\n",
    "test_schema_string = \"\"\"\n",
    "CREATE TABLE department (did INTEGER, dname TEXT, budget REAL, num_employees INTEGER);\n",
    "CREATE TABLE employee (eid INTEGER, ename TEXT, age INTEGER, department_did INTEGER, FOREIGN KEY(department_did) REFERENCES department(did));\n",
    "CREATE TABLE project (pid INTEGER, pname TEXT, lead_eid INTEGER, FOREIGN KEY(lead_eid) REFERENCES employee(eid));\n",
    "CREATE TABLE works_on (employee_eid INTEGER, project_pid INTEGER, hours INTEGER, FOREIGN KEY(employee_eid) REFERENCES employee(eid), FOREIGN KEY(project_pid) REFERENCES project(pid));\n",
    "\"\"\"\n",
    "test_nl_query = \"Name the latest Iphone launched by APPLE\"\n",
    "\n",
    "# Use the USER_PROMPT_TEMPLATE from cell 3dfe1828-...\n",
    "# This template includes few-shot examples and placeholders for the current schema/query.\n",
    "sample_user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "    schema_string=test_schema_string,\n",
    "    nl_query=test_nl_query\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": sample_user_prompt_content}\n",
    "]\n",
    "\n",
    "# --- 2. Prepare Inputs for the Model ---\n",
    "\n",
    "_messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(schema_string=test_schema_string, nl_query=test_nl_query)}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_input_string = tokenizer.apply_chat_template(\n",
    "    _messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # This is important!\n",
    ")\n",
    "\n",
    "inputs = tokenizer(tokenized_input_string, return_tensors=\"pt\", truncation=True, max_length=2048-10).to(model.device)\n",
    "\n",
    "\n",
    "# --- 3. Generate with SAMPLING (as in your original cell, but fixed) ---\n",
    "print(\"\\n--- Generating with SAMPLING (do_sample=True) ---\")\n",
    "max_new_tokens_sample = 10 # We expect \"Yes\" or \"No\", so few tokens are needed.\n",
    "with torch.no_grad():\n",
    "    outputs_sample = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens_sample,\n",
    "        do_sample=True,\n",
    "        num_beams=1,  # Explicitly state you are not using beam search\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "\n",
    "# Decode and clean the output\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "generated_tokens_sample = outputs_sample[0][input_length:]\n",
    "response_text_sample = tokenizer.decode(generated_tokens_sample, skip_special_tokens=True).strip()\n",
    "print(f\"Sampled Model Response: '{response_text_sample}'\")\n",
    "\n",
    "\n",
    "# --- 4. Generate with GREEDY DECODING ---\n",
    "print(\"\\n--- Generating with GREEDY DECODING (do_sample=False) ---\")\n",
    "max_new_tokens_greedy = 100 # Expecting \"Yes\" or \"No\"\n",
    "with torch.no_grad():\n",
    "    outputs_greedy = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens_greedy,\n",
    "        do_sample=False,  # Key for greedy decoding\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and clean the output\n",
    "generated_tokens_greedy = outputs_greedy[0][input_length:] # input_length is the same\n",
    "response_text_greedy = tokenizer.decode(generated_tokens_greedy, skip_special_tokens=True).strip()\n",
    "print(f\"Greedy Model Response: '{response_text_greedy}'\")\n",
    "\n",
    "# --- 5. (Optional) Compare with get_yes_probability ---\n",
    "print(\"\\n--- For reference: get_yes_probability output ---\")\n",
    "# Use the 'user' part of the _messages, which is the fully formatted user prompt\n",
    "user_prompt_content_for_prob = _messages[1]['content']\n",
    "try:\n",
    "    prob_yes = get_yes_probability(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        SYSTEM_PROMPT, # from cell 3dfe1828-...\n",
    "        user_prompt_content_for_prob, # This is already formatted with schema, query, and \"A:\"\n",
    "        YES_TOKEN_ID, # from cell abb8b6cb-...\n",
    "        NO_TOKEN_ID   # from cell abb8b6cb-...\n",
    "    )\n",
    "    print(f\"P(Yes) from get_yes_probability: {prob_yes:.4f}\")\n",
    "    if prob_yes > 0.5:\n",
    "        print(\"Based on get_yes_probability, 'Yes' is more likely.\")\n",
    "    else:\n",
    "        print(\"Based on get_yes_probability, 'No' is more likely.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running get_yes_probability for comparison: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75247d87-a7f5-4ca4-8770-78e43e0c7e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment: 20 Random Queries vs. 166 Total DB Schemas ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96708fa8734a24b9bc73cdfcadf833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing NL Queries:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Query 1/20 (ID: spider_dev_q0_idx0): 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2551dfc50ac747b19941b2fb913b3392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  DBs for Q:spider_dev_q0_idx0:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m p_yes_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     p_yes_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_yes_probability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYES_TOKEN_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNO_TOKEN_ID\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Use tqdm.write for error messages occurring inside the inner loop\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    ERROR: Exception in get_yes_probability for Query ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_query_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with DB \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidate_db_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[81], line 37\u001b[0m, in \u001b[0;36mget_yes_probability\u001b[0;34m(model_arg, tokenizer_arg, system_prompt_arg, user_prompt_content_arg, yes_token_id_arg, no_token_id_arg, max_length)\u001b[0m\n\u001b[1;32m     35\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     36\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m---> 37\u001b[0m     logit_yes \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myes_token_id_arg\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     logit_no \u001b[38;5;241m=\u001b[39m next_token_logits[:, no_token_id_arg]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m max_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(logit_yes, logit_no)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Ensure these imports are at the top of your script/notebook ---\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from tqdm.auto import tqdm # Use .auto or .notebook for Jupyter\n",
    "\n",
    "# --- Prerequisites (must be defined and populated from Cell 1 and Cell 2): ---\n",
    "# model, tokenizer, SYSTEM_PROMPT, USER_PROMPT_TEMPLATE,\n",
    "# YES_TOKEN_ID, NO_TOKEN_ID, get_yes_probability,\n",
    "# selected_nl_queries, candidate_schemas_for_evaluation, EXPERIMENT_RESULTS_FILE\n",
    "# --- (Assume these are correctly defined above this cell) ---\n",
    "\n",
    "# --- 3.1. Initialize Results Storage ---\n",
    "experiment_all_query_results = []\n",
    "\n",
    "# --- 3.2. Start the Loop ---\n",
    "# This initial print is fine as it's before any tqdm loops start for this cell's main logic\n",
    "print(f\"\\n--- Starting Experiment: {len(selected_nl_queries)} Random Queries vs. {len(candidate_schemas_for_evaluation)} Total DB Schemas ---\")\n",
    "\n",
    "# Outer loop: Iterate through each randomly selected NL query\n",
    "for query_idx, nl_query_info in enumerate(tqdm(selected_nl_queries, desc=\"Processing NL Queries\")):\n",
    "    current_nl_query_text = nl_query_info['question']\n",
    "    true_db_id_for_query = nl_query_info['db_id']\n",
    "    experiment_query_id = f\"spider_dev_q{query_idx}_{nl_query_info.get('query_id', 'idx'+str(query_idx))}\"\n",
    "\n",
    "    # Use tqdm.write for status updates related to the outer loop's progress\n",
    "    # The '\\n' at the beginning helps separate entries for each query visually.\n",
    "    tqdm.write(f\"\\nProcessing Query {query_idx + 1}/{len(selected_nl_queries)} (ID: {experiment_query_id}): '{current_nl_query_text}' (True DB: {true_db_id_for_query})\")\n",
    "\n",
    "    scores_for_current_query = []\n",
    "\n",
    "    # --- Optional: For debugging, print scores for the VERY FIRST query only ---\n",
    "    # print_debug_scores_for_first_query_only = True\n",
    "    # if print_debug_scores_for_first_query_only and query_idx == 0:\n",
    "    #     tqdm.write(f\"  --- Incremental Scores for First Query: '{current_nl_query_text}' ---\")\n",
    "    # --- End Optional Debug Print Setup ---\n",
    "\n",
    "    # Inner loop: Iterate through each candidate database schema\n",
    "    for candidate_db_id, candidate_schema_sql in tqdm(\n",
    "        candidate_schemas_for_evaluation.items(),\n",
    "        desc=f\"  DBs for Q:{experiment_query_id[:20]}\", # Description for the inner bar\n",
    "        leave=False  # Inner bar will be removed upon completion of its loop\n",
    "    ):\n",
    "        user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "            schema_string=candidate_schema_sql,\n",
    "            nl_query=current_nl_query_text\n",
    "        )\n",
    "        p_yes_score = -1.0\n",
    "\n",
    "        try:\n",
    "            p_yes_score = get_yes_probability(\n",
    "                model, tokenizer, SYSTEM_PROMPT, user_prompt_content, YES_TOKEN_ID, NO_TOKEN_ID\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Use tqdm.write for error messages occurring inside the inner loop\n",
    "            tqdm.write(f\"    ERROR: Exception in get_yes_probability for Query ID '{experiment_query_id}' with DB '{candidate_db_id}'.\")\n",
    "            tqdm.write(f\"    Exception type: {type(e).__name__}, Message: {e}\")\n",
    "            # if you need full traceback for debugging, tqdm.write(traceback.format_exc()) might work,\n",
    "            # but it can be very verbose. Printing to a log file is better for extensive tracebacks.\n",
    "            # traceback.print_exc() # This will print to stderr and might still mess with tqdm display\n",
    "\n",
    "        scores_for_current_query.append({\n",
    "            'candidate_db_id': candidate_db_id,\n",
    "            'p_yes_score': p_yes_score\n",
    "        })\n",
    "\n",
    "        # --- Optional: For debugging, print scores for the VERY FIRST query only ---\n",
    "        # if print_debug_scores_for_first_query_only and query_idx == 0:\n",
    "        #     tqdm.write(f\"    DB: {candidate_db_id}, Score: {p_yes_score:.4f}\") # Incremental print with tqdm.write\n",
    "        # --- End Optional Debug Print ---\n",
    "\n",
    "    ranked_databases_for_query = sorted(scores_for_current_query, key=lambda x: x['p_yes_score'], reverse=True)\n",
    "\n",
    "    # --- Optional: For debugging, print sorted scores for the VERY FIRST query only ---\n",
    "    # if print_debug_scores_for_first_query_only and query_idx == 0:\n",
    "    #     tqdm.write(f\"  --- Sorted Ranked Databases for First Query: '{current_nl_query_text}' (Top 10) ---\")\n",
    "    #     for rank_info in ranked_databases_for_query[:10]:\n",
    "    #         tqdm.write(f\"    Ranked DB: {rank_info['candidate_db_id']}, Score: {rank_info['p_yes_score']:.4f}\")\n",
    "    # --- End Optional Debug Print ---\n",
    "\n",
    "    experiment_all_query_results.append({\n",
    "        'experiment_query_id': experiment_query_id,\n",
    "        'nl_query_text': current_nl_query_text,\n",
    "        'true_db_id': true_db_id_for_query,\n",
    "        'ranked_databases_with_scores': ranked_databases_for_query\n",
    "    })\n",
    "\n",
    "    # --- 3.3. Periodic Saving of Results ---\n",
    "    if (query_idx + 1) % 1 == 0 or (query_idx + 1) == len(selected_nl_queries):\n",
    "        try:\n",
    "            with open(EXPERIMENT_RESULTS_FILE, 'w') as f_out:\n",
    "                json.dump(experiment_all_query_results, f_out, indent=2)\n",
    "            # Use tqdm.write for save messages that occur between outer loop iterations\n",
    "            tqdm.write(f\"  Successfully saved intermediate results for {len(experiment_all_query_results)} queries to {EXPERIMENT_RESULTS_FILE}\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  ERROR: Could not save intermediate results: {e}\")\n",
    "\n",
    "# --- 3.4. Experiment Loop Completion ---\n",
    "# These final prints are after all tqdm loops are done, so standard print is fine.\n",
    "print(\"\\n--- Experiment Loop Finished ---\")\n",
    "if experiment_all_query_results:\n",
    "    print(f\"Processed {len(experiment_all_query_results)} queries in total.\")\n",
    "    try:\n",
    "        with open(EXPERIMENT_RESULTS_FILE, 'w') as f_out:\n",
    "            json.dump(experiment_all_query_results, f_out, indent=2)\n",
    "        print(f\"Final results comprehensively saved to {EXPERIMENT_RESULTS_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save final results: {e}\")\n",
    "else:\n",
    "    print(\"No results were generated from the experiment. Check logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87ed3a14-354d-4762-a702-1fce12986b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory experiment_all_query_results for evaluation.\n",
      "\n",
      "--- Evaluation: Recall@K ---\n",
      "Evaluated on 20 queries.\n",
      "Recall@1: 0.00%\n",
      "Recall@3: 0.00%\n",
      "Recall@5: 0.00%\n",
      "Recall@10: 0.00%\n",
      "Saved evaluation results to 'recall_k_results_context_4096_prompt_changed.json'\n",
      "\n",
      "--- Sample Detailed Query Results (Top 5 Queries) ---\n",
      "\n",
      "Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. real_estate_properties  (Score: 0.9931)\n",
      "    2. student_assessment  (Score: 0.9909)\n",
      "    3. driving_school  (Score: 0.9909)\n",
      "    4. college_1  (Score: 0.9909)\n",
      "    5. academic  (Score: 0.9900)\n",
      "\n",
      "Query 2: 'What are names of countries with the top 3 largest population?' (True DB: world_1)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. real_estate_properties  (Score: 0.9933)\n",
      "    2. insurance_and_eClaims  (Score: 0.9897)\n",
      "    3. student_assessment  (Score: 0.9893)\n",
      "    4. solvency_ii  (Score: 0.9883)\n",
      "    5. academic  (Score: 0.9879)\n",
      "\n",
      "Query 3: 'What are the names and birth dates of people, ordered by their names in alphabetical order?' (True DB: poker_player)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. driving_school  (Score: 0.9912)\n",
      "    2. college_1  (Score: 0.9909)\n",
      "    3. real_estate_properties  (Score: 0.9909)\n",
      "    4. solvency_ii  (Score: 0.9906)\n",
      "    5. e_government  (Score: 0.9903)\n",
      "\n",
      "Query 4: 'How many different store locations are there?' (True DB: employee_hire_evaluation)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. real_estate_properties  (Score: 0.9929)\n",
      "    2. student_assessment  (Score: 0.9912)\n",
      "    3. academic  (Score: 0.9900)\n",
      "    4. driving_school  (Score: 0.9883)\n",
      "    5. e_learning  (Score: 0.9879)\n",
      "\n",
      "Query 5: 'How many different nationalities do conductors have?' (True DB: orchestra)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. real_estate_properties  (Score: 0.9944)\n",
      "    2. student_assessment  (Score: 0.9900)\n",
      "    3. hr_1  (Score: 0.9876)\n",
      "    4. driving_school  (Score: 0.9868)\n",
      "    5. solvency_ii  (Score: 0.9864)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path where the evaluation summary (Recall@K results) will be saved\n",
    "EVAL_RESULTS_SAVE_PATH = \"recall_k_results_context_4096_prompt_changed.json\"\n",
    "\n",
    "# --- 4.1. Define Recall@K Calculation Function ---\n",
    "def calculate_recall_at_k_metric(all_query_results_list, k_values_list):\n",
    "    \"\"\"\n",
    "    Calculates Recall@K for a list of K values.\n",
    "    Each item in all_query_results_list should be a dictionary with:\n",
    "        'true_db_id': The ground truth database ID for the query.\n",
    "        'ranked_databases_with_scores': A list of {'candidate_db_id': id, 'p_yes_score': score},\n",
    "                                         sorted by score in descending order.\n",
    "    \"\"\"\n",
    "    recall_counts = {k: 0 for k in k_values_list}  # Stores how many times true_db was in top K\n",
    "    total_valid_queries = 0  # Queries for which we have a true_db_id\n",
    "\n",
    "    if not all_query_results_list:\n",
    "        return {k: 0.0 for k in k_values_list}, 0\n",
    "\n",
    "    for query_result in all_query_results_list:\n",
    "        true_db = query_result.get('true_db_id')\n",
    "        ranked_dbs_info = query_result.get('ranked_databases_with_scores')\n",
    "\n",
    "        if true_db is None or ranked_dbs_info is None:\n",
    "            print(f\"Warning: Skipping query result due to missing 'true_db_id' or 'ranked_databases_with_scores': \"\n",
    "                  f\"{query_result.get('experiment_query_id', 'Unknown Query')}\")\n",
    "            continue  # Skip if essential information is missing\n",
    "\n",
    "        total_valid_queries += 1\n",
    "        # Extract just the DB IDs from the ranked list\n",
    "        ranked_db_ids_only = [item['candidate_db_id'] for item in ranked_dbs_info]\n",
    "\n",
    "        for k in k_values_list:\n",
    "            # Get the top K predicted database IDs\n",
    "            top_k_predicted_dbs = ranked_db_ids_only[:k]\n",
    "            if true_db in top_k_predicted_dbs:\n",
    "                recall_counts[k] += 1\n",
    "\n",
    "    # Calculate final recall percentages\n",
    "    recall_percentages = {}\n",
    "    if total_valid_queries > 0:\n",
    "        for k in k_values_list:\n",
    "            recall_percentages[k] = (recall_counts[k] / total_valid_queries) * 100.0  # As percentage\n",
    "    else:\n",
    "        recall_percentages = {k: 0.0 for k in k_values_list}\n",
    "\n",
    "    return recall_percentages, total_valid_queries\n",
    "\n",
    "\n",
    "# --- 4.2. Perform Evaluation ---\n",
    "# Load results if this cell is run in a new session and experiment_all_query_results isn't in memory\n",
    "# (assuming results were saved to EXPERIMENT_RESULTS_FILE)\n",
    "loaded_results_for_eval = None\n",
    "if 'experiment_all_query_results' in globals() and experiment_all_query_results:\n",
    "    print(\"Using in-memory experiment_all_query_results for evaluation.\")\n",
    "    loaded_results_for_eval = experiment_all_query_results\n",
    "elif os.path.exists(EXPERIMENT_RESULTS_FILE):\n",
    "    print(f\"Loading results from {EXPERIMENT_RESULTS_FILE} for evaluation...\")\n",
    "    try:\n",
    "        with open(EXPERIMENT_RESULTS_FILE, 'r') as f_in:\n",
    "            loaded_results_for_eval = json.load(f_in)\n",
    "        print(f\"Successfully loaded {len(loaded_results_for_eval)} results from file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results from file for evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No results available in memory or in the specified results file for evaluation.\")\n",
    "\n",
    "if loaded_results_for_eval:\n",
    "    K_VALUES_TO_EVALUATE = [1, 3, 5, 10]  # Define the K values you care about\n",
    "    recall_scores_map, num_queries_evaluated = calculate_recall_at_k_metric(\n",
    "        loaded_results_for_eval, K_VALUES_TO_EVALUATE\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Evaluation: Recall@K ---\")\n",
    "    print(f\"Evaluated on {num_queries_evaluated} queries.\")\n",
    "    for k_val, recall_val in recall_scores_map.items():\n",
    "        print(f\"Recall@{k_val}: {recall_val:.2f}%\")\n",
    "\n",
    "    # --- 4.2.1. Save evaluation results to a JSON file ---\n",
    "    try:\n",
    "        eval_summary = {\n",
    "            \"num_queries_evaluated\": num_queries_evaluated,\n",
    "            \"recall_scores\": recall_scores_map\n",
    "        }\n",
    "        with open(EVAL_RESULTS_SAVE_PATH, 'w') as fout:\n",
    "            json.dump(eval_summary, fout, indent=2)\n",
    "        print(f\"Saved evaluation results to '{EVAL_RESULTS_SAVE_PATH}'\")\n",
    "    except Exception as save_err:\n",
    "        print(f\"Error saving evaluation results: {save_err}\")\n",
    "\n",
    "    # --- 4.3. Optional: Print Detailed Results for a Few Queries ---\n",
    "    print(\"\\n--- Sample Detailed Query Results (Top 5 Queries) ---\")\n",
    "    for i, res in enumerate(loaded_results_for_eval[:5]):  # Show for first 5 queries\n",
    "        print(f\"\\nQuery {i+1}: '{res.get('nl_query_text', '<no text>')}' (True DB: {res.get('true_db_id')})\")\n",
    "        print(\"  Top Ranked Databases (with P(Yes) scores):\")\n",
    "        for rank, db_info in enumerate(res.get('ranked_databases_with_scores', [])[:5]):  # Show top 5 ranked DBs\n",
    "            is_true_db_char = \"*\" if db_info['candidate_db_id'] == res['true_db_id'] else \" \"\n",
    "            print(f\"    {rank+1}. {db_info['candidate_db_id']}{is_true_db_char} \"\n",
    "                  f\"(Score: {db_info['p_yes_score']:.4f})\")\n",
    "else:\n",
    "    print(\"Cannot perform evaluation as no results were loaded or generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama_spider_env)",
   "language": "python",
   "name": "llama_spider_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
