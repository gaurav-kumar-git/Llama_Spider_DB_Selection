{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e11db41-baca-4438-b743-120a663f7321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.32.3)\n",
      "Requirement already satisfied: tqdm in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.67.1)\n",
<<<<<<< HEAD
      "Requirement already satisfied: filelock in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
=======
      "Requirement already satisfied: filelock in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
      "Requirement already satisfied: packaging>=20.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: psutil in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
<<<<<<< HEAD
      "Requirement already satisfied: torch>=2.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: sympy in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
=======
      "Requirement already satisfied: torch>=2.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (78.1.1)\n",
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
<<<<<<< HEAD
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
=======
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate bitsandbytes sentencepiece pandas datasets huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 10,
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
   "id": "4cbfcb4e-129c-4b07-8feb-7798c2e1b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
<<<<<<< HEAD
      "PyTorch Version: 2.2.0\n",
=======
      "PyTorch Version: 2.7.0+cu126\n",
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "af36680e-8256-4dfb-b473-54e7964d9ed0",
   "metadata": {},
   "outputs": [
=======
   "execution_count": 5,
   "id": "eb48cfef-6ee8-4e34-b2bc-62c82e923e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5362d48efa468cbea7237242d500e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful or already authenticated.\n",
      "\n",
      "--- Cell 2: Hugging Face Login Attempt Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Hugging Face Hub Authentication ---\n",
    "# You MUST have requested access to Llama 2 models via Meta's form on Hugging Face\n",
    "# AND have your request approved.\n",
    "\n",
    "# Option 1: If you've stored your token as an environment variable on the server\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "# if HF_TOKEN:\n",
    "#     print(\"Logging into Hugging Face Hub using token from environment variable...\")\n",
    "#     login(token=HF_TOKEN)\n",
    "# else:\n",
    "#     print(\"HF_TOKEN environment variable not set. Attempting widget login if in interactive environment, or manual CLI login might be needed.\")\n",
    "#     login() # Will prompt if in an environment that supports it\n",
    "\n",
    "# Option 2: Paste token directly (less secure, use with caution)\n",
    "# HF_TOKEN = \"YOUR_HF_READ_TOKEN_HERE\"\n",
    "# login(token=HF_TOKEN)\n",
    "\n",
    "# Option 3: Use huggingface-cli login in a server terminal beforehand (Recommended)\n",
    "# If already logged in via CLI, this cell might not be strictly necessary,\n",
    "# but running login() can confirm status or refresh credentials.\n",
    "try:\n",
    "    login() # Will use cached token or prompt if needed\n",
    "    print(\"Hugging Face login successful or already authenticated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}. Ensure you are authenticated to download Llama 2.\")\n",
    "\n",
    "print(\"\\n--- Cell 2: Hugging Face Login Attempt Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13940fd1-94c2-4f75-be23-926306045142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model: meta-llama/Llama-2-70b-chat-hf\n",
      "BitsAndBytesConfig: load_in_4bit=True, compute_dtype=torch.bfloat16\n",
      "System and User prompt templates defined.\n",
      "Hugging Face model cache directory set to: /raid/infolab/gaurav/Llama_Spider_A100_Project/experiments_70b_llama/.hf_model_cache_70b\n",
      "\n",
      "--- Cell 3: Model and Prompt Configuration Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Tokenizer Configuration ---\n",
    "import os\n",
    "\n",
    "# 3.1. Specify the Llama 2 70B Chat Model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "print(f\"Target Model: {MODEL_NAME}\")\n",
    "\n",
    "# 3.2. Configure 4-bit Quantization (essential for 70B, even on A100s for single/few GPU use)\n",
    "# A100s support bfloat16, which is excellent for mixed-precision.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # nf4 is a good default\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation on A100s\n",
    "    bnb_4bit_use_double_quant=True,   # Can save a bit more memory\n",
    ")\n",
    "print(f\"BitsAndBytesConfig: load_in_4bit={bnb_config.load_in_4bit}, compute_dtype={bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# 3.3. Define Prompt Templates\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert data analyst. Your task is to determine if a given natural language query \"\n",
    "    \"can be answered *solely* based on the provided database schema. \"\n",
    "    \"Do not attempt to answer the query itself. Your entire response must be only the word 'Yes' or the word 'No'.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Database Schema:\n",
    "---\n",
    "{schema_string}\n",
    "---\n",
    "Natural Language Query: \"{nl_query}\"\n",
    "---\n",
    "Can the query be answered using *only* the provided schema and its potential contents? Answer with either \"Yes\" or \"No\".\n",
    "\"\"\"\n",
    "print(\"System and User prompt templates defined.\")\n",
    "\n",
    "# 3.4. Define Cache Directory for Hugging Face downloads (optional, but good for managing large models)\n",
    "# Create it within your project directory on the A100 server.\n",
    "HF_MODEL_CACHE_DIR = os.path.join(os.getcwd(), \".hf_model_cache_70b\") # Assumes current dir is project root\n",
    "os.makedirs(HF_MODEL_CACHE_DIR, exist_ok=True)\n",
    "print(f\"Hugging Face model cache directory set to: {HF_MODEL_CACHE_DIR}\")\n",
    "\n",
    "print(\"\\n--- Cell 3: Model and Prompt Configuration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f22b9fb-9618-4db1-8216-6e2140fde35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-2-70b-chat-hf...\n"
     ]
    },
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31adfc6e4734a01b8b88f9e7d616234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee32a573fa8418ea9aaaff994d63463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ffe3aecdc14e13bcedc457b7565825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63543a5e6828492f957bd258bd5fbff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "PyTorch version: 2.2.0\n",
      "CUDA available: True\n",
      "CUDA version PyTorch compiled with: 11.8\n",
      "Number of GPUs available to PyTorch: 8\n",
      "  GPU 0: NVIDIA A100-SXM4-80GB\n",
      "  GPU 1: NVIDIA A100-SXM4-80GB\n",
      "  GPU 2: NVIDIA A100-SXM4-80GB\n",
      "  GPU 3: NVIDIA A100-SXM4-80GB\n",
      "  GPU 4: NVIDIA A100-SXM4-80GB\n",
      "  GPU 5: NVIDIA A100-SXM4-80GB\n",
      "  GPU 6: NVIDIA A100-SXM4-80GB\n",
      "  GPU 7: NVIDIA A100-SXM4-80GB\n"
=======
      "Set tokenizer.pad_token to tokenizer.eos_token ('</s>')\n",
      "Tokenizer loaded successfully.\n",
      "Found single token for 'Yes': ID 3869\n",
      "Found single token for 'No': ID 1939\n",
      "GLOBAL YES_TOKEN_ID: 3869 ('Yes')\n",
      "GLOBAL NO_TOKEN_ID: 1939 ('No')\n",
      "\n",
      "--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\n"
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch compiled with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs available to PyTorch: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"ERROR: PyTorch cannot see the GPUs! Check installation and CUDA compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf50af66-84d7-405d-a5db-781178af57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
      "PyTorch Version: 2.2.0\n",
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb48cfef-6ee8-4e34-b2bc-62c82e923e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7735c9ab864d4b9da5fbcce1a2b4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful or already authenticated.\n",
      "\n",
      "--- Cell 2: Hugging Face Login Attempt Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Hugging Face Hub Authentication ---\n",
    "# You MUST have requested access to Llama 2 models via Meta's form on Hugging Face\n",
    "# AND have your request approved.\n",
    "\n",
    "# Option 1: If you've stored your token as an environment variable on the server\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "# if HF_TOKEN:\n",
    "#     print(\"Logging into Hugging Face Hub using token from environment variable...\")\n",
    "#     login(token=HF_TOKEN)\n",
    "# else:\n",
    "#     print(\"HF_TOKEN environment variable not set. Attempting widget login if in interactive environment, or manual CLI login might be needed.\")\n",
    "#     login() # Will prompt if in an environment that supports it\n",
    "\n",
    "# Option 2: Paste token directly (less secure, use with caution)\n",
    "# HF_TOKEN = \"YOUR_HF_READ_TOKEN_HERE\"\n",
    "# login(token=HF_TOKEN)\n",
    "\n",
    "# Option 3: Use huggingface-cli login in a server terminal beforehand (Recommended)\n",
    "# If already logged in via CLI, this cell might not be strictly necessary,\n",
    "# but running login() can confirm status or refresh credentials.\n",
    "try:\n",
    "    login() # Will use cached token or prompt if needed\n",
    "    print(\"Hugging Face login successful or already authenticated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}. Ensure you are authenticated to download Llama 2.\")\n",
    "\n",
    "print(\"\\n--- Cell 2: Hugging Face Login Attempt Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13940fd1-94c2-4f75-be23-926306045142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model: meta-llama/Llama-2-70b-chat-hf\n",
      "BitsAndBytesConfig: load_in_4bit=True, compute_dtype=torch.bfloat16\n",
      "System and User prompt templates defined.\n",
      "Hugging Face model cache directory set to: /raid/infolab/gaurav/Llama_Spider_A100_Project/experiments_70b_llama/.hf_model_cache_70b\n",
      "\n",
      "--- Cell 3: Model and Prompt Configuration Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Tokenizer Configuration ---\n",
    "import os\n",
    "\n",
    "# 3.1. Specify the Llama 2 70B Chat Model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "print(f\"Target Model: {MODEL_NAME}\")\n",
    "\n",
    "# 3.2. Configure 4-bit Quantization (essential for 70B, even on A100s for single/few GPU use)\n",
    "# A100s support bfloat16, which is excellent for mixed-precision.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # nf4 is a good default\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation on A100s\n",
    "    bnb_4bit_use_double_quant=True,   # Can save a bit more memory\n",
    ")\n",
    "print(f\"BitsAndBytesConfig: load_in_4bit={bnb_config.load_in_4bit}, compute_dtype={bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# 3.3. Define Prompt Templates\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert data analyst. Your task is to determine if a given natural language query \"\n",
    "    \"can be answered *solely* based on the provided database schema. \"\n",
    "    \"Do not attempt to answer the query itself. Your entire response must be only the word 'Yes' or the word 'No'.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Database Schema:\n",
    "---\n",
    "{schema_string}\n",
    "---\n",
    "Natural Language Query: \"{nl_query}\"\n",
    "---\n",
    "Can the query be answered using *only* the provided schema and its potential contents? Answer with either \"Yes\" or \"No\".\n",
    "\"\"\"\n",
    "print(\"System and User prompt templates defined.\")\n",
    "\n",
    "# 3.4. Define Cache Directory for Hugging Face downloads (optional, but good for managing large models)\n",
    "# Create it within your project directory on the A100 server.\n",
    "HF_MODEL_CACHE_DIR = os.path.join(os.getcwd(), \".hf_model_cache_70b\") # Assumes current dir is project root\n",
    "os.makedirs(HF_MODEL_CACHE_DIR, exist_ok=True)\n",
    "print(f\"Hugging Face model cache directory set to: {HF_MODEL_CACHE_DIR}\")\n",
    "\n",
    "print(\"\\n--- Cell 3: Model and Prompt Configuration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f22b9fb-9618-4db1-8216-6e2140fde35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-2-70b-chat-hf...\n",
      "Set tokenizer.pad_token to tokenizer.eos_token ('</s>')\n",
      "Tokenizer loaded successfully.\n",
      "Found single token for 'Yes': ID 3869\n",
      "Found single token for 'No': ID 1939\n",
      "GLOBAL YES_TOKEN_ID: 3869 ('Yes')\n",
      "GLOBAL NO_TOKEN_ID: 1939 ('No')\n",
      "\n",
      "--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\n"
     ]
    }
   ],
   "source": [
=======
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
    "# --- Load Tokenizer and Define Yes/No Token Logic ---\n",
    "\n",
    "# 4.1. Load Tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HF_MODEL_CACHE_DIR)\n",
    "    # Set pad token if not already set (Llama tokenizers often don't have one)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set tokenizer.pad_token to tokenizer.eos_token ('{tokenizer.eos_token}')\")\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load tokenizer for {MODEL_NAME}: {e}\")\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "# 4.2. Define Helper Function to get Yes/No Token IDs\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"Determines token IDs for 'Yes'/'No', preferring those with a leading space.\"\"\"\n",
    "    # Try with leading space first for chat models\n",
    "    yes_variants = [\" Yes\", \"Yes\"]\n",
    "    no_variants = [\" No\", \"No\"]\n",
    "    \n",
    "    final_yes_id = None\n",
    "    final_no_id = None\n",
    "\n",
    "    for variant in yes_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_yes_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_yes_id}\")\n",
    "            break\n",
    "            \n",
    "    for variant in no_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_no_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_no_id}\")\n",
    "            break\n",
    "\n",
    "    if final_yes_id is None or final_no_id is None:\n",
    "        print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or variants.\")\n",
    "        # You might want to print detailed tokenization attempts here if this error occurs\n",
    "        raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Cannot proceed.\")\n",
    "    \n",
    "    return final_yes_id, final_no_id\n",
    "\n",
    "# 4.3. Define Global YES_TOKEN_ID and NO_TOKEN_ID\n",
    "try:\n",
    "    YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "    print(f\"GLOBAL YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID]).strip()}')\")\n",
    "    print(f\"GLOBAL NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID]).strip()}')\")\n",
    "except ValueError as e:\n",
    "    raise RuntimeError(f\"Failed to set YES/NO token IDs: {e}\")\n",
    "\n",
    "print(\"\\n--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30caff72-a187-4cf2-be07-8bdf3beea0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-70b-chat-hf with 4-bit quantization. This will take significant time and memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e82c6526734d20a9125df794dfeb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c04c9a5426a4e0cb59302a1e5b5c7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00015.safetensors:   0%|          | 0.00/9.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dffb0df2dbf42e585fda75873c6366d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5464f8fc634d0c96ef56cae60f87e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n",
      "Time taken to load model: 618.92 seconds.\n",
      "Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 4, 'model.layers.46': 4, 'model.layers.47': 4, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 5, 'model.layers.55': 5, 'model.layers.56': 5, 'model.layers.57': 5, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 6, 'model.layers.64': 6, 'model.layers.65': 6, 'model.layers.66': 6, 'model.layers.67': 6, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.layers.72': 7, 'model.layers.73': 7, 'model.layers.74': 7, 'model.layers.75': 7, 'model.layers.76': 7, 'model.layers.77': 7, 'model.layers.78': 7, 'model.layers.79': 7, 'model.norm': 7, 'model.rotary_emb': 7, 'lm_head': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_594729/2920798812.py\", line 25, in <module>\n",
      "    gc.collect()\n",
      "NameError: name 'gc' is not defined\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load model meta-llama/Llama-2-70b-chat-hf: name 'gc' is not defined. Check VRAM, CUDA setup, and Hugging Face authentication.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformed memory cleanup (torch.cuda.empty_cache(), gc.collect())\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Check VRAM, CUDA setup, and Hugging Face authentication.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Cell 5: Llama 2 70B Model Loading Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load model meta-llama/Llama-2-70b-chat-hf: name 'gc' is not defined. Check VRAM, CUDA setup, and Hugging Face authentication."
     ]
    }
   ],
   "source": [
    "# --- Load the Llama 2 70B Model ---\n",
    "# This is a memory-intensive step. `device_map=\"auto\"` will attempt to distribute\n",
    "# the model across available GPUs if one is insufficient.\n",
    "# Ensure CUDA_VISIBLE_DEVICES is set in your shell if you want to restrict which GPUs are used.\n",
    "import gc\n",
=======
    "\n",
    "\n",
    "# 4.2. Define Helper Function to get Yes/No Token IDs\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"Determines token IDs for 'Yes'/'No', preferring those with a leading space.\"\"\"\n",
    "    # Try with leading space first for chat models\n",
    "    yes_variants = [\" Yes\", \"Yes\"]\n",
    "    no_variants = [\" No\", \"No\"]\n",
    "    \n",
    "    final_yes_id = None\n",
    "    final_no_id = None\n",
    "\n",
    "    for variant in yes_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_yes_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_yes_id}\")\n",
    "            break\n",
    "            \n",
    "    for variant in no_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_no_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_no_id}\")\n",
    "            break\n",
    "\n",
    "    if final_yes_id is None or final_no_id is None:\n",
    "        print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or variants.\")\n",
    "        # You might want to print detailed tokenization attempts here if this error occurs\n",
    "        raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Cannot proceed.\")\n",
    "    \n",
    "    return final_yes_id, final_no_id\n",
    "\n",
    "# 4.3. Define Global YES_TOKEN_ID and NO_TOKEN_ID\n",
    "try:\n",
    "    YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "    print(f\"GLOBAL YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID]).strip()}')\")\n",
    "    print(f\"GLOBAL NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID]).strip()}')\")\n",
    "except ValueError as e:\n",
    "    raise RuntimeError(f\"Failed to set YES/NO token IDs: {e}\")\n",
    "\n",
    "print(\"\\n--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f72066e7-3f59-4788-9c15-54937a2456a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-70b-chat-hf with 4-bit quantization. This will take significant time and memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad23808a87a4d1bb79b33b7c2ab86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu', 'cuda', 'hpu', 'mps', 'xpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1304908/2920798812.py\", line 9, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4390, in from_pretrained\n",
      "    hf_quantizer.validate_environment(\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 84, in validate_environment\n",
      "    validate_bnb_backend_availability(raise_exception=True)\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py\", line 560, in validate_bnb_backend_availability\n",
      "    return _validate_bnb_multi_backend_availability(raise_exception)\n",
      "  File \"/raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py\", line 517, in _validate_bnb_multi_backend_availability\n",
      "    raise RuntimeError(err_msg)\n",
      "RuntimeError: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu', 'cuda', 'hpu', 'mps', 'xpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load model meta-llama/Llama-2-70b-chat-hf: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu', 'cuda', 'hpu', 'mps', 'xpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend. Check VRAM, CUDA setup, and Hugging Face authentication.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Apply 4-bit quantization\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 on A100s\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Distribute model across available GPUs automatically\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Often needed for newer models\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_MODEL_CACHE_DIR\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     model_load_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/modeling_utils.py:4390\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4390\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4397\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:84\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[0;32m---> 84\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:560\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_multi_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_cuda_backend_availability(raise_exception)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:517\u001b[0m, in \u001b[0;36m_validate_bnb_multi_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    516\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(err_msg)\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    519\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported devices found for bitsandbytes multi-backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu', 'cuda', 'hpu', 'mps', 'xpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Check VRAM, CUDA setup, and Hugging Face authentication.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Cell 5: Llama 2 70B Model Loading Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load model meta-llama/Llama-2-70b-chat-hf: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu', 'cuda', 'hpu', 'mps', 'xpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend. Check VRAM, CUDA setup, and Hugging Face authentication."
     ]
    }
   ],
   "source": [
    "# --- Load the Llama 2 70B Model ---\n",
    "# This is a memory-intensive step. `device_map=\"auto\"` will attempt to distribute\n",
    "# the model across available GPUs if one is insufficient.\n",
    "# Ensure CUDA_VISIBLE_DEVICES is set in your shell if you want to restrict which GPUs are used.\n",
    "\n",
>>>>>>> 98c51a95935f383852f5a12a6b30c15c7102eb8a
    "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization. This will take significant time and memory...\")\n",
    "model_load_start_time = time.time()\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,    # Apply 4-bit quantization\n",
    "        torch_dtype=torch.bfloat16,        # Use bfloat16 on A100s\n",
    "        device_map=\"auto\",                 # Distribute model across available GPUs automatically\n",
    "        trust_remote_code=True,            # Often needed for newer models\n",
    "        cache_dir=HF_MODEL_CACHE_DIR\n",
    "    )\n",
    "    model_load_end_time = time.time()\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Time taken to load model: {model_load_end_time - model_load_start_time:.2f} seconds.\")\n",
    "    print(f\"Model device map: {model.hf_device_map}\") # Shows how layers are distributed\n",
    "    # For a 70B model, this should show parts on different GPUs if more than one is used.\n",
    "    \n",
    "    # Perform a quick memory cleanup after loading large model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Performed memory cleanup (torch.cuda.empty_cache(), gc.collect())\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(f\"Failed to load model {MODEL_NAME}: {e}. Check VRAM, CUDA setup, and Hugging Face authentication.\")\n",
    "\n",
    "print(\"\\n--- Cell 5: Llama 2 70B Model Loading Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama_spider_env)",
   "language": "python",
   "name": "llama_spider_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
