{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c999f2fd-56f0-4ae9-949c-4803f663a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (0.32.3)\n",
      "Requirement already satisfied: tqdm in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: psutil in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: sympy in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate bitsandbytes sentencepiece pandas datasets huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc106017-bfb9-40d0-8409-ff72140b945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets version: 8.1.5\n",
      "ipywidgets location: /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/ipywidgets/__init__.py\n",
      "tqdm version: 4.67.1\n",
      "tqdm location: /raid/infolab/gaurav/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/tqdm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "  import ipywidgets\n",
    "  print(f\"ipywidgets version: {ipywidgets.__version__}\")\n",
    "  print(f\"ipywidgets location: {ipywidgets.__file__}\")\n",
    "\n",
    "  import tqdm\n",
    "  print(f\"tqdm version: {tqdm.__version__}\")\n",
    "  print(f\"tqdm location: {tqdm.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48f920e-9675-4ae6-ac4a-e221b3e0175a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm imported successfully from .auto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543280c6633d4f0d8f1926f7f577ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minimal Auto Test:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tqdm .auto loop completed\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"tqdm imported successfully from .auto\")\n",
    "my_list = list(range(3))\n",
    "for i in tqdm(my_list, desc=\"Minimal Auto Test\"):\n",
    "    time.sleep(0.2)\n",
    "print(\"Simple tqdm .auto loop completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51bd609-5bef-4bae-9977-8909136a2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
      "PyTorch Version: 2.2.0\n",
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8aed063-c3c2-49b9-ba1c-3ec07ec68adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0\n",
      "CUDA available: True\n",
      "CUDA version PyTorch compiled with: 11.8\n",
      "Number of GPUs available to PyTorch: 8\n",
      "  GPU 0: NVIDIA A100-SXM4-80GB\n",
      "  GPU 1: NVIDIA A100-SXM4-80GB\n",
      "  GPU 2: NVIDIA A100-SXM4-80GB\n",
      "  GPU 3: NVIDIA A100-SXM4-80GB\n",
      "  GPU 4: NVIDIA A100-SXM4-80GB\n",
      "  GPU 5: NVIDIA A100-SXM4-80GB\n",
      "  GPU 6: NVIDIA A100-SXM4-80GB\n",
      "  GPU 7: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch compiled with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs available to PyTorch: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"ERROR: PyTorch cannot see the GPUs! Check installation and CUDA compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba6d845-eeb6-4dc3-8936-00a0686991c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Imports and Initial Configuration Complete ---\n",
      "PyTorch Version: 2.2.0\n",
      "Transformers Version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "import transformers # <--- ADD THIS LINE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Third-party Library Imports ---\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from huggingface_hub import login # For Hugging Face Hub authentication\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"--- Cell 1: Imports and Initial Configuration Complete ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac563a7e-b468-4b6e-9b89-ea76a46e229b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88cce2c462c449b841bf6155e232984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful or already authenticated.\n",
      "\n",
      "--- Cell 2: Hugging Face Login Attempt Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Hugging Face Hub Authentication ---\n",
    "# You MUST have requested access to Llama 2 models via Meta's form on Hugging Face\n",
    "# AND have your request approved.\n",
    "\n",
    "# Option 1: If you've stored your token as an environment variable on the server\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "# if HF_TOKEN:\n",
    "#     print(\"Logging into Hugging Face Hub using token from environment variable...\")\n",
    "#     login(token=HF_TOKEN)\n",
    "# else:\n",
    "#     print(\"HF_TOKEN environment variable not set. Attempting widget login if in interactive environment, or manual CLI login might be needed.\")\n",
    "#     login() # Will prompt if in an environment that supports it\n",
    "\n",
    "# Option 2: Paste token directly (less secure, use with caution)\n",
    "# HF_TOKEN = \"YOUR_HF_READ_TOKEN_HERE\"\n",
    "# login(token=HF_TOKEN)\n",
    "\n",
    "# Option 3: Use huggingface-cli login in a server terminal beforehand (Recommended)\n",
    "# If already logged in via CLI, this cell might not be strictly necessary,\n",
    "# but running login() can confirm status or refresh credentials.\n",
    "try:\n",
    "    login() # Will use cached token or prompt if needed\n",
    "    print(\"Hugging Face login successful or already authenticated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}. Ensure you are authenticated to download Llama 2.\")\n",
    "\n",
    "print(\"\\n--- Cell 2: Hugging Face Login Attempt Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f0694-4129-4a0a-9bec-150838594c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model: meta-llama/Llama-2-70b-chat-hf\n",
      "BitsAndBytesConfig: load_in_4bit=True, compute_dtype=torch.bfloat16\n",
      "System and User prompt templates defined.\n",
      "Hugging Face model cache directory set to: /raid/infolab/gaurav/Llama_Spider_A100_Project/experiments_70b_llama/.hf_model_cache_70b\n",
      "\n",
      "--- Cell 3: Model and Prompt Configuration Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Tokenizer Configuration ---\n",
    "import os\n",
    "\n",
    "# 3.1. Specify the Llama 2 70B Chat Model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "print(f\"Target Model: {MODEL_NAME}\")\n",
    "\n",
    "# 3.2. Configure 4-bit Quantization (essential for 70B, even on A100s for single/few GPU use)\n",
    "# A100s support bfloat16, which is excellent for mixed-precision.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # nf4 is a good default\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation on A100s\n",
    "    bnb_4bit_use_double_quant=True,   # Can save a bit more memory\n",
    ")\n",
    "print(f\"BitsAndBytesConfig: load_in_4bit={bnb_config.load_in_4bit}, compute_dtype={bnb_config.bnb_4bit_compute_dtype}\")\n",
    "\n",
    "# 3.3. Define Prompt Templates\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert data analyst. Your task is to determine if a given natural language query \"\n",
    "    \"can be answered *solely* based on the provided database schema. \"\n",
    "    \"Do not attempt to answer the query itself. Your entire response must be only the word 'Yes' or the word 'No'.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Database Schema:\n",
    "---\n",
    "{schema_string}\n",
    "---\n",
    "Natural Language Query: \"{nl_query}\"\n",
    "---\n",
    "Can the query be answered using *only* the provided schema and its potential contents? Answer with either \"Yes\" or \"No\".\n",
    "\"\"\"\n",
    "print(\"System and User prompt templates defined.\")\n",
    "\n",
    "# 3.4. Define Cache Directory for Hugging Face downloads (optional, but good for managing large models)\n",
    "# Create it within your project directory on the A100 server.\n",
    "HF_MODEL_CACHE_DIR = os.path.join(os.getcwd(), \".hf_model_cache_70b\") # Assumes current dir is project root\n",
    "os.makedirs(HF_MODEL_CACHE_DIR, exist_ok=True)\n",
    "print(f\"Hugging Face model cache directory set to: {HF_MODEL_CACHE_DIR}\")\n",
    "\n",
    "print(\"\\n--- Cell 3: Model and Prompt Configuration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd45122-14da-4134-8b16-2732bbe33a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-2-70b-chat-hf...\n",
      "Set tokenizer.pad_token to tokenizer.eos_token ('</s>')\n",
      "Tokenizer loaded successfully.\n",
      "Found single token for 'Yes': ID 3869\n",
      "Found single token for 'No': ID 1939\n",
      "GLOBAL YES_TOKEN_ID: 3869 ('Yes')\n",
      "GLOBAL NO_TOKEN_ID: 1939 ('No')\n",
      "\n",
      "--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer and Define Yes/No Token Logic ---\n",
    "\n",
    "# 4.1. Load Tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HF_MODEL_CACHE_DIR)\n",
    "    # Set pad token if not already set (Llama tokenizers often don't have one)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set tokenizer.pad_token to tokenizer.eos_token ('{tokenizer.eos_token}')\")\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load tokenizer for {MODEL_NAME}: {e}\")\n",
    "\n",
    "\n",
    "# 4.2. Define Helper Function to get Yes/No Token IDs\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"Determines token IDs for 'Yes'/'No', preferring those with a leading space.\"\"\"\n",
    "    # Try with leading space first for chat models\n",
    "    yes_variants = [\" Yes\", \"Yes\"]\n",
    "    no_variants = [\" No\", \"No\"]\n",
    "    \n",
    "    final_yes_id = None\n",
    "    final_no_id = None\n",
    "\n",
    "    for variant in yes_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_yes_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_yes_id}\")\n",
    "            break\n",
    "            \n",
    "    for variant in no_variants:\n",
    "        token_ids = tokenizer_arg.encode(variant, add_special_tokens=False)\n",
    "        if len(token_ids) == 1:\n",
    "            final_no_id = token_ids[0]\n",
    "            print(f\"Found single token for '{variant}': ID {final_no_id}\")\n",
    "            break\n",
    "\n",
    "    if final_yes_id is None or final_no_id is None:\n",
    "        print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or variants.\")\n",
    "        # You might want to print detailed tokenization attempts here if this error occurs\n",
    "        raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Cannot proceed.\")\n",
    "    \n",
    "    return final_yes_id, final_no_id\n",
    "\n",
    "# 4.3. Define Global YES_TOKEN_ID and NO_TOKEN_ID\n",
    "try:\n",
    "    YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "    print(f\"GLOBAL YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID]).strip()}')\")\n",
    "    print(f\"GLOBAL NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID]).strip()}')\")\n",
    "except ValueError as e:\n",
    "    raise RuntimeError(f\"Failed to set YES/NO token IDs: {e}\")\n",
    "\n",
    "print(\"\\n--- Cell 4: Tokenizer Loading and Yes/No Token ID Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546dc473-9b23-4b0d-aff2-a1064c893deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-70b-chat-hf with 4-bit quantization. This will take significant time and memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eff96983414d918af9c31f1818f266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n",
      "Time taken to load model: 47.87 seconds.\n",
      "Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 4, 'model.layers.46': 4, 'model.layers.47': 4, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 5, 'model.layers.55': 5, 'model.layers.56': 5, 'model.layers.57': 5, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 6, 'model.layers.64': 6, 'model.layers.65': 6, 'model.layers.66': 6, 'model.layers.67': 6, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.layers.72': 7, 'model.layers.73': 7, 'model.layers.74': 7, 'model.layers.75': 7, 'model.layers.76': 7, 'model.layers.77': 7, 'model.layers.78': 7, 'model.layers.79': 7, 'model.norm': 7, 'model.rotary_emb': 7, 'lm_head': 7}\n",
      "Performed memory cleanup (torch.cuda.empty_cache(), gc.collect())\n",
      "\n",
      "--- Cell 5: Llama 2 70B Model Loading Complete ---\n",
      "Model max_position_embeddings: 4096\n",
      "Tokenizer model_max_length: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Llama 2 70B Model ---\n",
    "# This is a memory-intensive step. `device_map=\"auto\"` will attempt to distribute\n",
    "# the model across available GPUs if one is insufficient.\n",
    "# Ensure CUDA_VISIBLE_DEVICES is set in your shell if you want to restrict which GPUs are used.\n",
    "import gc\n",
    "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization. This will take significant time and memory...\")\n",
    "model_load_start_time = time.time()\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,    # Apply 4-bit quantization\n",
    "        torch_dtype=torch.bfloat16,        # Use bfloat16 on A100s\n",
    "        device_map=\"auto\",                 # Distribute model across available GPUs automatically\n",
    "        trust_remote_code=True,            # Often needed for newer models\n",
    "        cache_dir=HF_MODEL_CACHE_DIR\n",
    "    )\n",
    "    model_load_end_time = time.time()\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Time taken to load model: {model_load_end_time - model_load_start_time:.2f} seconds.\")\n",
    "    print(f\"Model device map: {model.hf_device_map}\") # Shows how layers are distributed\n",
    "    # For a 70B model, this should show parts on different GPUs if more than one is used.\n",
    "    \n",
    "    # Perform a quick memory cleanup after loading large model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Performed memory cleanup (torch.cuda.empty_cache(), gc.collect())\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(f\"Failed to load model {MODEL_NAME}: {e}. Check VRAM, CUDA setup, and Hugging Face authentication.\")\n",
    "\n",
    "print(\"\\n--- Cell 5: Llama 2 70B Model Loading Complete ---\")\n",
    "\n",
    "print(\"Model max_position_embeddings:\", model.config.max_position_embeddings)\n",
    "print(\"Tokenizer model_max_length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fae66bb-f328-4814-9e23-cbc376eca2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started. Looking for zip file at: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip\n",
      "Zip file found at /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip.\n",
      "Attempting to unzip /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip to /raid/infolab/gaurav/Llama_Spider_A100_Project/...\n",
      "Successfully unzipped files to /raid/infolab/gaurav/Llama_Spider_A100_Project/\n",
      "Contents of /raid/infolab/gaurav/Llama_Spider_A100_Project/:\n",
      "  - experiments_70b_llama\n",
      "  - .gitignore\n",
      "  - backup_to_github.sh\n",
      "  - Miniconda3-latest-Linux-x86_64.sh\n",
      "  - spider_subset_data.zip\n",
      "  - randomQ_allDBs_run1\n",
      "  - .ipynb_checkpoints\n",
      "  - .git\n",
      "  - miniconda3\n",
      "  - 100_queries.txt\n",
      "  - spider_subset_data\n",
      "  - __MACOSX\n",
      "\n",
      "Verifying extracted file paths...\n",
      "SUCCESS: dev.json path is valid: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/dev.json\n",
      "SUCCESS: tables.json path is valid: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json\n",
      "\n",
      "--- Ready to load data ---\n",
      "Path to dev.json: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/dev.json\n",
      "Path to tables.json: /raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "SERVER_ZIP_FILE_PATH = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data.zip'\n",
    "EXTRACTION_DESTINATION_DIR_ON_SERVER = '/raid/infolab/gaurav/Llama_Spider_A100_Project/'\n",
    "\n",
    "DEV_JSON_PATH = None\n",
    "TABLES_JSON_PATH = None\n",
    "\n",
    "def unzip_data(zip_filepath, dest_dir):\n",
    "    \"\"\"\n",
    "    Unzips a zip file to a specified destination directory.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to unzip {zip_filepath} to {dest_dir}...\")\n",
    "    try:\n",
    "        \n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "        print(f\"Successfully unzipped files to {dest_dir}\")\n",
    "\n",
    "        print(f\"Contents of {dest_dir}:\")\n",
    "        for item in os.listdir(dest_dir):\n",
    "            print(f\"  - {item}\")\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_filepath} is not a valid zip file or is corrupted.\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Zip file not found at {zip_filepath}. Please ensure the path is correct.\")\n",
    "        return False\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied to write to {dest_dir} or read {zip_filepath}.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during unzipping: {e}\")\n",
    "        return False\n",
    "\n",
    "print(f\"Script started. Looking for zip file at: {SERVER_ZIP_FILE_PATH}\")\n",
    "\n",
    "if os.path.exists(SERVER_ZIP_FILE_PATH):\n",
    "    print(f\"Zip file found at {SERVER_ZIP_FILE_PATH}.\")\n",
    "    if unzip_data(SERVER_ZIP_FILE_PATH, EXTRACTION_DESTINATION_DIR_ON_SERVER):\n",
    "        \n",
    "        EXPECTED_EXTRACTED_FOLDER_NAME = 'spider_subset_data' # This is the folder INSIDE the zip\n",
    "\n",
    "        DEV_JSON_PATH = os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME, 'dev.json')\n",
    "        TABLES_JSON_PATH = os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME, 'tables.json')\n",
    "\n",
    "        print(\"\\nVerifying extracted file paths...\")\n",
    "        if os.path.exists(DEV_JSON_PATH):\n",
    "            print(f\"SUCCESS: dev.json path is valid: {DEV_JSON_PATH}\")\n",
    "        else:\n",
    "            print(f\"ERROR: dev.json NOT FOUND at expected path: {DEV_JSON_PATH}\")\n",
    "            print(f\"Please check the contents of {os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME)}\")\n",
    "\n",
    "\n",
    "        if os.path.exists(TABLES_JSON_PATH):\n",
    "            print(f\"SUCCESS: tables.json path is valid: {TABLES_JSON_PATH}\")\n",
    "        else:\n",
    "            print(f\"ERROR: tables.json NOT FOUND at expected path: {TABLES_JSON_PATH}\")\n",
    "            print(f\"Please check the contents of {os.path.join(EXTRACTION_DESTINATION_DIR_ON_SERVER, EXPECTED_EXTRACTED_FOLDER_NAME)}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Unzipping failed on the server. Cannot define data paths.\")\n",
    "else:\n",
    "    print(f\"ERROR: Zip file NOT FOUND at {SERVER_ZIP_FILE_PATH} on the server.\")\n",
    "    print(\"Please ensure the 'scp' command was successful and the path is correct.\")\n",
    "\n",
    "\n",
    "if DEV_JSON_PATH and TABLES_JSON_PATH and os.path.exists(DEV_JSON_PATH) and os.path.exists(TABLES_JSON_PATH):\n",
    "    print(\"\\n--- Ready to load data ---\")\n",
    "    print(f\"Path to dev.json: {DEV_JSON_PATH}\")\n",
    "    print(f\"Path to tables.json: {TABLES_JSON_PATH}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n--- Data paths are not correctly set up. Cannot proceed with data loading. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b93479-df37-4b4b-9f80-44c9222dba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1034 queries from dev.json\n",
      "Loaded 166 database schemas from tables.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"ERROR: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "dev_data = load_json_data(DEV_JSON_PATH)\n",
    "tables_data = load_json_data(TABLES_JSON_PATH)\n",
    "\n",
    "if dev_data and tables_data:\n",
    "    print(f\"Loaded {len(dev_data)} queries from dev.json\")\n",
    "    print(f\"Loaded {len(tables_data)} database schemas from tables.json\")\n",
    "else:\n",
    "    print(\"Failed to load Spider data. Please check paths and upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c304d6-7712-4138-9d36-cbab1a8e225d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Preparing Database Schema SQL Strings (Dictionary Output) ---\n",
      "Loaded schema data for 166 databases from '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'.\n",
      "Successfully populated `all_db_schemas_sql_strings` dictionary with 166 entries.\n",
      "\n",
      "--- Verification of all_db_schemas_sql_strings ---\n",
      "Type: <class 'dict'>\n",
      "Number of schemas processed: 166\n",
      "Sample - DB ID: college_2\n",
      "Sample - SQL String :\n",
      "CREATE TABLE classroom (\n",
      "  building TEXT PRIMARY KEY,\n",
      "  room_number TEXT,\n",
      "  capacity REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE department (\n",
      "  dept_name TEXT PRIMARY KEY,\n",
      "  building TEXT,\n",
      "  budget REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE course (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  title TEXT,\n",
      "  dept_name TEXT,\n",
      "  credits REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE instructor (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  name TEXT,\n",
      "  dept_name TEXT,\n",
      "  salary REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE section (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year REAL,\n",
      "  building TEXT,\n",
      "  room_number TEXT,\n",
      "  time_slot_id TEXT,\n",
      "  FOREIGN KEY (building) REFERENCES classroom (building),\n",
      "  FOREIGN KEY (room_number) REFERENCES classroom (room_number),\n",
      "  FOREIGN KEY (course_id) REFERENCES course (course_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE teaches (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  course_id TEXT,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year INTEGER,\n",
      "  FOREIGN KEY (ID) REFERENCES instructor (ID),\n",
      "  FOREIGN KEY (course_id) REFERENCES section (course_id),\n",
      "  FOREIGN KEY (sec_id) REFERENCES section (sec_id),\n",
      "  FOREIGN KEY (semester) REFERENCES section (semester),\n",
      "  FOREIGN KEY (year) REFERENCES section (year)\n",
      ");\n",
      "\n",
      "CREATE TABLE student (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  name TEXT,\n",
      "  dept_name TEXT,\n",
      "  tot_cred REAL,\n",
      "  FOREIGN KEY (dept_name) REFERENCES department (dept_name)\n",
      ");\n",
      "\n",
      "CREATE TABLE takes (\n",
      "  ID TEXT PRIMARY KEY,\n",
      "  course_id TEXT,\n",
      "  sec_id TEXT,\n",
      "  semester TEXT,\n",
      "  year INTEGER,\n",
      "  grade TEXT,\n",
      "  FOREIGN KEY (ID) REFERENCES student (ID),\n",
      "  FOREIGN KEY (course_id) REFERENCES section (course_id),\n",
      "  FOREIGN KEY (sec_id) REFERENCES section (sec_id),\n",
      "  FOREIGN KEY (semester) REFERENCES section (semester),\n",
      "  FOREIGN KEY (year) REFERENCES section (year)\n",
      ");\n",
      "\n",
      "CREATE TABLE advisor (\n",
      "  s_ID TEXT PRIMARY KEY,\n",
      "  i_ID TEXT,\n",
      "  FOREIGN KEY (s_ID) REFERENCES student (ID),\n",
      "  FOREIGN KEY (i_ID) REFERENCES instructor (ID)\n",
      ");\n",
      "\n",
      "CREATE TABLE time_slot (\n",
      "  time_slot_id TEXT PRIMARY KEY,\n",
      "  day TEXT,\n",
      "  start_hr REAL,\n",
      "  start_min REAL,\n",
      "  end_hr REAL,\n",
      "  end_min REAL\n",
      ");\n",
      "\n",
      "CREATE TABLE prereq (\n",
      "  course_id TEXT PRIMARY KEY,\n",
      "  prereq_id TEXT,\n",
      "  FOREIGN KEY (prereq_id) REFERENCES course (course_id),\n",
      "  FOREIGN KEY (course_id) REFERENCES course (course_id)\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# earlier\n",
    "import json\n",
    "# import os # Not strictly needed for this dictionary creation unless used in paths\n",
    "# import traceback # Only needed if you keep the full traceback print in except\n",
    "\n",
    "# --- Helper Functions (These are the same as you provided) ---\n",
    "def load_schemas(tables_json_path):\n",
    "    \"\"\"Loads schemas from tables.json into a dictionary keyed by db_id.\"\"\"\n",
    "    with open(tables_json_path, 'r') as f:\n",
    "        schemas_list = json.load(f)\n",
    "    schemas_dict = {db_info['db_id']: db_info for db_info in schemas_list}\n",
    "    return schemas_dict\n",
    "\n",
    "def map_spider_type_to_sql_type(spider_type, is_pk_or_fk=False):\n",
    "    \"\"\"Maps Spider's generic types to SQLite data types.\"\"\"\n",
    "    spider_type = spider_type.lower()\n",
    "    if spider_type == \"text\":\n",
    "        return \"TEXT\"\n",
    "    elif spider_type == \"number\":\n",
    "        return \"INTEGER\" if is_pk_or_fk else \"REAL\"\n",
    "    elif spider_type == \"time\":\n",
    "        return \"DATETIME\"\n",
    "    elif spider_type == \"boolean\":\n",
    "        return \"BOOLEAN\"\n",
    "    elif spider_type == \"others\":\n",
    "        return \"BLOB\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "def escape_sql_identifier(name):\n",
    "    \"\"\"Escapes SQL identifiers (table/column names) if they contain spaces or are keywords.\"\"\"\n",
    "    if \" \" in name or name.lower() in {\"select\", \"from\", \"where\", \"table\", \"primary\", \"key\", \"foreign\", \"index\", \"order\", \"group\"}:\n",
    "        return f'\"{name}\"'\n",
    "    return name\n",
    "\n",
    "def generate_create_table_sql_for_db(db_id, all_schemas_data): # Parameter name changed for consistency\n",
    "    \"\"\"\n",
    "    Generates SQL CREATE TABLE statements for a given db_id from the Spider schema.\n",
    "    'all_schemas_data' is the dictionary produced by load_schemas.\n",
    "    \"\"\"\n",
    "    if db_id not in all_schemas_data:\n",
    "        return f\"-- Database ID '{db_id}' not found in schemas.\"\n",
    "\n",
    "    db_schema = all_schemas_data[db_id] # Get the specific schema info for this db_id\n",
    "    sql_statements = []\n",
    "    column_info_by_index = {}\n",
    "    for i, (table_idx, col_name_original) in enumerate(db_schema['column_names_original']):\n",
    "        if col_name_original == \"*\":\n",
    "            continue\n",
    "        column_info_by_index[i] = {\n",
    "            \"original_name\": col_name_original,\n",
    "            \"table_index\": table_idx,\n",
    "            \"original_table_name\": db_schema['table_names_original'][table_idx],\n",
    "            \"type\": db_schema['column_types'][i]\n",
    "        }\n",
    "    for table_idx, table_name_original in enumerate(db_schema['table_names_original']):\n",
    "        escaped_table_name = escape_sql_identifier(table_name_original)\n",
    "        column_definitions = []\n",
    "        table_constraints = []\n",
    "        current_table_columns = []\n",
    "        for col_global_idx, (tbl_idx_for_col, col_name_orig) in enumerate(db_schema['column_names_original']):\n",
    "            if col_name_orig == \"*\":\n",
    "                continue\n",
    "            if tbl_idx_for_col == table_idx:\n",
    "                current_table_columns.append({\n",
    "                    \"global_idx\": col_global_idx,\n",
    "                    \"name\": col_name_orig,\n",
    "                    \"type\": db_schema['column_types'][col_global_idx]\n",
    "                })\n",
    "        pk_column_indices_for_table = [\n",
    "            pk_idx for pk_idx in db_schema['primary_keys']\n",
    "            if column_info_by_index.get(pk_idx) and column_info_by_index[pk_idx]['table_index'] == table_idx\n",
    "        ]\n",
    "        pk_column_names_for_table = [column_info_by_index[idx]['original_name'] for idx in pk_column_indices_for_table]\n",
    "        for col_data in current_table_columns:\n",
    "            col_name_original = col_data['name']\n",
    "            spider_type = col_data['type']\n",
    "            col_global_idx = col_data['global_idx']\n",
    "            is_pk_col = col_global_idx in pk_column_indices_for_table\n",
    "            is_fk_col = any(fk_pair[0] == col_global_idx for fk_pair in db_schema['foreign_keys'])\n",
    "            sql_type = map_spider_type_to_sql_type(spider_type, is_pk_or_fk=(is_pk_col or is_fk_col))\n",
    "            escaped_col_name = escape_sql_identifier(col_name_original)\n",
    "            col_def_str = f\"{escaped_col_name} {sql_type}\"\n",
    "            if is_pk_col and len(pk_column_names_for_table) == 1:\n",
    "                col_def_str += \" PRIMARY KEY\"\n",
    "            column_definitions.append(col_def_str)\n",
    "        if len(pk_column_names_for_table) > 1:\n",
    "            escaped_pk_cols = [escape_sql_identifier(name) for name in pk_column_names_for_table]\n",
    "            table_constraints.append(f\"PRIMARY KEY ({', '.join(escaped_pk_cols)})\")\n",
    "        for fk_col_idx, referenced_col_idx in db_schema['foreign_keys']:\n",
    "            if column_info_by_index.get(fk_col_idx) and \\\n",
    "               column_info_by_index.get(referenced_col_idx) and \\\n",
    "               column_info_by_index[fk_col_idx]['table_index'] == table_idx:\n",
    "                fk_column_name = column_info_by_index[fk_col_idx]['original_name']\n",
    "                referenced_table_name = column_info_by_index[referenced_col_idx]['original_table_name']\n",
    "                referenced_column_name = column_info_by_index[referenced_col_idx]['original_name']\n",
    "                escaped_fk_col = escape_sql_identifier(fk_column_name)\n",
    "                escaped_ref_table = escape_sql_identifier(referenced_table_name)\n",
    "                escaped_ref_col = escape_sql_identifier(referenced_column_name)\n",
    "                table_constraints.append(\n",
    "                    f\"FOREIGN KEY ({escaped_fk_col}) REFERENCES {escaped_ref_table} ({escaped_ref_col})\"\n",
    "                )\n",
    "        all_parts = column_definitions + table_constraints\n",
    "        create_table_statement = f\"CREATE TABLE {escaped_table_name} (\\n  \"\n",
    "        create_table_statement += \",\\n  \".join(all_parts)\n",
    "        create_table_statement += \"\\n);\"\n",
    "        sql_statements.append(create_table_statement)\n",
    "    return \"\\n\\n\".join(sql_statements)\n",
    "# --- End of Helper Functions ---\n",
    "\n",
    "\n",
    "# --- MODIFIED \"Main Execution\" for \"Cell 1\" to produce the dictionary ---\n",
    "# This code will be run when you execute the Jupyter cell.\n",
    "# The output variable needed by your experiment is `all_db_schemas_sql_strings`.\n",
    "\n",
    "all_db_schemas_sql_strings = {} # This is the dictionary your experiment needs\n",
    "\n",
    "# Define the path to your tables.json\n",
    "spider_tables_json_path = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'\n",
    "\n",
    "print(\"--- Cell 1: Preparing Database Schema SQL Strings (Dictionary Output) ---\")\n",
    "try:\n",
    "    # 1. Load all schema structures from tables.json\n",
    "    # `all_db_schemas_data_loaded` will be a dictionary: {db_id: schema_info_dict, ...}\n",
    "    all_db_schemas_data_loaded = load_schemas(spider_tables_json_path) # Renamed to avoid confusion with function parameter\n",
    "    print(f\"Loaded schema data for {len(all_db_schemas_data_loaded)} databases from '{spider_tables_json_path}'.\")\n",
    "\n",
    "    # 2. Iterate through each loaded schema and generate its SQL string, storing it in the dictionary\n",
    "    if all_db_schemas_data_loaded:\n",
    "        for db_id in all_db_schemas_data_loaded: # Iterate through keys (db_ids)\n",
    "            # Call generate_create_table_sql_for_db, passing the full loaded data\n",
    "            # and the current db_id.\n",
    "            sql_string_for_db = generate_create_table_sql_for_db(db_id, all_db_schemas_data_loaded)\n",
    "\n",
    "            # Store the raw SQL string in the dictionary.\n",
    "            # We only store it if it's a successful generation (doesn't start with the error message)\n",
    "            if sql_string_for_db and not sql_string_for_db.startswith(\"-- Database ID\"):\n",
    "                all_db_schemas_sql_strings[db_id] = sql_string_for_db\n",
    "            elif sql_string_for_db.startswith(\"-- Database ID\"):\n",
    "                print(f\"Warning: Schema for {db_id} reported as not found by generate_create_table_sql_for_db.\")\n",
    "            else:\n",
    "                print(f\"Warning: SQL generation returned empty or unexpected for {db_id} (Result: '{sql_string_for_db[:50]}...')\")\n",
    "\n",
    "        print(f\"Successfully populated `all_db_schemas_sql_strings` dictionary with {len(all_db_schemas_sql_strings)} entries.\")\n",
    "    else:\n",
    "        print(\"No schema data loaded from tables.json, so `all_db_schemas_sql_strings` will be empty.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The file '{spider_tables_json_path}' was not found.\")\n",
    "    all_db_schemas_sql_strings = {} # Ensure it's defined as empty on error\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"FATAL ERROR: Could not decode JSON from '{spider_tables_json_path}'. Check if it's a valid JSON file.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during schema preparation: {e}\")\n",
    "    # import traceback # Uncomment if you need the full traceback here\n",
    "    # traceback.print_exc()\n",
    "    all_db_schemas_sql_strings = {}\n",
    "\n",
    "# --- Verification (you can add this to your cell to check after it runs) ---\n",
    "print(f\"\\n--- Verification of all_db_schemas_sql_strings ---\")\n",
    "print(f\"Type: {type(all_db_schemas_sql_strings)}\")\n",
    "print(f\"Number of schemas processed: {len(all_db_schemas_sql_strings)}\")\n",
    "if all_db_schemas_sql_strings:\n",
    "    # Print a sample to verify content\n",
    "    sample_db_id = list(all_db_schemas_sql_strings.keys())[1]\n",
    "    print(f\"Sample - DB ID: {sample_db_id}\")\n",
    "    print(f\"Sample - SQL String :\\n{all_db_schemas_sql_strings[sample_db_id]}\")\n",
    "else:\n",
    "    print(\"`all_db_schemas_sql_strings` is empty. Review errors above.\")\n",
    "# --- End of Cell 1 Logic ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2452e5b-6f75-47d1-bc17-276b3afc9b16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Random Sampling\n",
    "\n",
    "# This cell defines parameters for running the experiment.\n",
    "# It will now randomly select queries and always use ALL database schemas as candidates.\n",
    "\n",
    "import random # Ensure random is imported at the top of your notebook or this cell\n",
    "# import os # Ensure os is imported (likely already done for path joining)\n",
    "# import json # Ensure json is imported (likely already done for loading)\n",
    "\n",
    "# --- 2.1. Experiment Parameters ---\n",
    "# Number of NL queries to RANDOMLY select from dev.json to process.\n",
    "# For initial testing in Colab, use a small subset. For a more thorough run, increase this.\n",
    "NUM_RANDOM_QUERIES_TO_TEST = 100 # For example, test 5 random queries\n",
    "\n",
    "# This will now effectively always be True based on your requirement.\n",
    "# The logic will be set up to use all schemas from all_db_schemas_sql_strings.\n",
    "# We can keep the variable for clarity or remove it if it's always all DBs.\n",
    "# For this implementation, let's explicitly aim for all DBs.\n",
    "print(\"INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\")\n",
    "\n",
    "\n",
    "# --- 2.2. Randomly Select NL Queries for the Experiment ---\n",
    "# We will randomly sample NUM_RANDOM_QUERIES_TO_TEST queries from the loaded dev_data.\n",
    "if not dev_data: # dev_data should have been loaded in Cell 1\n",
    "    raise ValueError(\"dev_data is not loaded (from dev.json). Cannot select queries. Please run Cell 1 first.\")\n",
    "\n",
    "if len(dev_data) == 0:\n",
    "    raise ValueError(\"dev_data is empty. No queries to select.\")\n",
    "\n",
    "actual_num_queries_to_select = min(NUM_RANDOM_QUERIES_TO_TEST, len(dev_data))\n",
    "# Using min ensures we don't try to sample more queries than available.\n",
    "\n",
    "if actual_num_queries_to_select < NUM_RANDOM_QUERIES_TO_TEST:\n",
    "    print(f\"Warning: Requested {NUM_RANDOM_QUERIES_TO_TEST} random queries, but only {len(dev_data)} are available. Using all {len(dev_data)} queries.\")\n",
    "\n",
    "# Randomly sample without replacement\n",
    "selected_nl_queries = random.sample(dev_data, actual_num_queries_to_select)\n",
    "\n",
    "print(f\"\\nRandomly selected {len(selected_nl_queries)} NL queries for the experiment:\")\n",
    "for i, q_info in enumerate(selected_nl_queries):\n",
    "    print(f\"  Test Query {i+1}: '{q_info['question']}' (True DB: {q_info['db_id']})\")\n",
    "\n",
    "\n",
    "# --- 2.3. Determine Candidate Database Schemas for Each Query ---\n",
    "# For this experiment design, we ALWAYS use ALL available database schemas.\n",
    "# all_db_schemas_sql_strings should have been populated in Cell 1.\n",
    "if not all_sql_output: # Populated in Cell 1\n",
    "    raise ValueError(\"all_sql_output is empty. Schemas were not converted in Cell 1. Cannot proceed.\")\n",
    "\n",
    "candidate_schemas_for_evaluation = all_db_schemas_sql_strings # Use all converted schemas\n",
    "print(f\"\\nEach of the {len(selected_nl_queries)} selected queries will be evaluated against all {len(candidate_schemas_for_evaluation)} available Spider database schemas.\")\n",
    "\n",
    "if not candidate_schemas_for_evaluation: # Should not happen if all_db_schemas_sql_strings was populated\n",
    "    raise ValueError(\"No candidate schemas available for evaluation. This indicates an issue with schema loading or conversion in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "588ef6c6-6baa-460f-92ac-1f24fa060b05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Preparing Database Schemas with PK/FK Annotations ---\n",
      "Loaded schema data for 166 databases from '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'.\n",
      "Populated `all_db_schemas_sql_strings` with 166 entries.\n",
      "\n",
      "--- Verification of all_db_schemas_sql_strings ---\n",
      "Type: <class 'dict'>\n",
      "Number of schemas processed: 166\n",
      "Sample - DB ID: yelp\n",
      "Sample - Schema Description:\n",
      "business: bid(REAL)[PK], business_id(TEXT), name(TEXT), full_address(TEXT), city(TEXT), latitude(TEXT), longitude(TEXT), review_count(REAL), is_open(REAL), rating(REAL), state(TEXT); category: id(REAL)[PK], business_id(TEXT)[FK→business.business_id], category_name(TEXT); user: uid(REAL)[PK], user_id(TEXT), name(TEXT); checkin: cid(REAL)[PK], business_id(TEXT)[FK→business.business_id], count(REAL), day(TEXT); neighbourhood: id(REAL)[PK], business_id(TEXT)[FK→business.business_id], neighbourhood_name(TEXT); review: rid(REAL)[PK], business_id(TEXT)[FK→business.business_id], user_id(TEXT)[FK→user.user_id], rating(REAL), text(TEXT), year(REAL), month(TEXT); tip: tip_id(REAL)[PK], business_id(TEXT)[FK→business.business_id], text(TEXT), user_id(TEXT)[FK→user.user_id], likes(REAL), year(REAL), month(TEXT)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Helper Functions (modified to output schema with PK and FK relationships) ---\n",
    "\n",
    "def load_schemas(tables_json_path):\n",
    "    \"\"\"Loads schemas from tables.json into a dictionary keyed by db_id.\"\"\"\n",
    "    with open(tables_json_path, 'r') as f:\n",
    "        schemas_list = json.load(f)\n",
    "    schemas_dict = {db_info['db_id']: db_info for db_info in schemas_list}\n",
    "    return schemas_dict\n",
    "\n",
    "def map_spider_type_to_sql_type(spider_type):\n",
    "    \"\"\"Maps Spider's generic types to a concise SQL‐style type string.\"\"\"\n",
    "    spider_type = spider_type.lower()\n",
    "    if spider_type == \"text\":\n",
    "        return \"TEXT\"\n",
    "    elif spider_type == \"number\":\n",
    "        return \"REAL\"\n",
    "    elif spider_type == \"time\":\n",
    "        return \"DATETIME\"\n",
    "    elif spider_type == \"boolean\":\n",
    "        return \"BOOLEAN\"\n",
    "    elif spider_type == \"others\":\n",
    "        return \"BLOB\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "def escape_sql_identifier(name):\n",
    "    \"\"\"Escapes identifiers if they contain spaces or are SQL keywords.\"\"\"\n",
    "    if \" \" in name or name.lower() in {\n",
    "        \"select\", \"from\", \"where\", \"table\", \"primary\", \"key\",\n",
    "        \"foreign\", \"index\", \"order\", \"group\"\n",
    "    }:\n",
    "        return f'\"{name}\"'\n",
    "    return name\n",
    "\n",
    "def generate_create_table_sql_for_db(db_id, all_schemas_data):\n",
    "    \"\"\"\n",
    "    Instead of producing CREATE TABLE statements, this returns a schema description string:\n",
    "    - For each table: \"table_name: col1(type)[PK], col2(type)[FK->refTable.refCol], col3(type), ...\"\n",
    "    It uses 'primary_keys' and 'foreign_keys' arrays from the Spider JSON.\n",
    "    \"\"\"\n",
    "    if db_id not in all_schemas_data:\n",
    "        return f\"-- Database ID '{db_id}' not found in schemas.\"\n",
    "    \n",
    "    db_schema = all_schemas_data[db_id]\n",
    "    table_names = db_schema['table_names_original']\n",
    "    column_names = db_schema['column_names_original']\n",
    "    column_types = db_schema['column_types']\n",
    "    pk_indices = set(db_schema.get('primary_keys', []))\n",
    "    fk_pairs = db_schema.get('foreign_keys', [])\n",
    "\n",
    "    # Build a lookup: column_index -> (table_idx, column_name, column_type)\n",
    "    col_info = {}\n",
    "    for idx, (tbl_idx, col_name) in enumerate(column_names):\n",
    "        if col_name == \"*\":\n",
    "            continue\n",
    "        col_info[idx] = {\n",
    "            \"table_index\": tbl_idx,\n",
    "            \"column_name\": col_name,\n",
    "            \"column_type\": column_types[idx]\n",
    "        }\n",
    "\n",
    "    # Initialize a structure: table_idx -> list of column descriptors\n",
    "    tables_columns = {i: [] for i in range(len(table_names))}\n",
    "\n",
    "    # For each column, determine if it's PK or FK or normal\n",
    "    for col_idx, info in col_info.items():\n",
    "        tbl_idx = info['table_index']\n",
    "        col_name = escape_sql_identifier(info['column_name'])\n",
    "        col_type = map_spider_type_to_sql_type(info['column_type'])\n",
    "\n",
    "        is_pk = (col_idx in pk_indices)\n",
    "        # Check if this column is a foreign key, and if so, note the referenced table/column\n",
    "        fk_description = \"\"\n",
    "        for (fk_col_idx, ref_col_idx) in fk_pairs:\n",
    "            if fk_col_idx == col_idx:\n",
    "                ref_info = col_info[ref_col_idx]\n",
    "                ref_table = escape_sql_identifier(ref_info['original_table_name'] if 'original_table_name' in ref_info else table_names[ref_info['table_index']])\n",
    "                ref_col_name = escape_sql_identifier(ref_info['column_name'])\n",
    "                fk_description = f\"[FK→{ref_table}.{ref_col_name}]\"\n",
    "                break\n",
    "\n",
    "        pk_tag = \"[PK]\" if is_pk else \"\"\n",
    "        tables_columns[tbl_idx].append(f\"{col_name}({col_type}){pk_tag}{fk_description}\")\n",
    "\n",
    "    # Build the final schema string, one segment per table\n",
    "    segments = []\n",
    "    for tbl_idx, tbl_name in enumerate(table_names):\n",
    "        escaped_tbl = escape_sql_identifier(tbl_name)\n",
    "        col_list = tables_columns.get(tbl_idx, [])\n",
    "        cols_joined = \", \".join(col_list)\n",
    "        segments.append(f\"{escaped_tbl}: {cols_joined}\")\n",
    "\n",
    "    # Join table segments with semicolons\n",
    "    return \"; \".join(segments)\n",
    "\n",
    "# --- Main Execution (Cell 1) producing the dictionary ---\n",
    "all_db_schemas_sql_strings = {}  # Dictionary to hold schema descriptions\n",
    "\n",
    "# Path to tables.json\n",
    "spider_tables_json_path = '/raid/infolab/gaurav/Llama_Spider_A100_Project/spider_subset_data/tables.json'\n",
    "\n",
    "print(\"--- Cell 1: Preparing Database Schemas with PK/FK Annotations ---\")\n",
    "try:\n",
    "    # 1. Load all schema structures\n",
    "    all_db_schemas_data_loaded = load_schemas(spider_tables_json_path)\n",
    "    print(f\"Loaded schema data for {len(all_db_schemas_data_loaded)} databases from '{spider_tables_json_path}'.\")\n",
    "\n",
    "    # 2. Iterate and generate schema descriptions\n",
    "    for db_id in all_db_schemas_data_loaded:\n",
    "        schema_desc = generate_create_table_sql_for_db(db_id, all_db_schemas_data_loaded)\n",
    "        if schema_desc and not schema_desc.startswith(\"-- Database ID\"):\n",
    "            all_db_schemas_sql_strings[db_id] = schema_desc\n",
    "        else:\n",
    "            print(f\"Warning: Could not generate schema for {db_id}\")\n",
    "\n",
    "    print(f\"Populated `all_db_schemas_sql_strings` with {len(all_db_schemas_sql_strings)} entries.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: File '{spider_tables_json_path}' not found.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"FATAL ERROR: Could not decode JSON from '{spider_tables_json_path}'.\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during schema preparation: {e}\")\n",
    "    all_db_schemas_sql_strings = {}\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\n--- Verification of all_db_schemas_sql_strings ---\")\n",
    "print(f\"Type: {type(all_db_schemas_sql_strings)}\")\n",
    "print(f\"Number of schemas processed: {len(all_db_schemas_sql_strings)}\")\n",
    "if all_db_schemas_sql_strings:\n",
    "    sample_db_id = list(all_db_schemas_sql_strings.keys())[125]\n",
    "    print(f\"Sample - DB ID: {sample_db_id}\")\n",
    "    print(f\"Sample - Schema Description:\\n{all_db_schemas_sql_strings[sample_db_id]}\")\n",
    "else:\n",
    "    print(\"`all_db_schemas_sql_strings` is empty. Check for errors above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d67f720-5263-4bb6-8db6-a4da23c1a95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\n",
      "Loaded 20 queries from '/raid/infolab/gaurav/Llama_Spider_A100_Project/100_queries.txt':\n",
      "  Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
      "  Query 2: 'What are names of countries with the top 3 largest population?' (True DB: world_1)\n",
      "  Query 3: 'What are the names and birth dates of people, ordered by their names in alphabetical order?' (True DB: poker_player)\n",
      "  Query 4: 'How many different store locations are there?' (True DB: employee_hire_evaluation)\n",
      "  Query 5: 'How many different nationalities do conductors have?' (True DB: orchestra)\n",
      "  Query 6: 'How many states are there?' (True DB: voter_1)\n",
      "  Query 7: 'What are the codes of template types that have fewer than 3 templates?' (True DB: cre_Doc_Template_Mgt)\n",
      "  Query 8: 'How many dogs have not gone through any treatment?' (True DB: dog_kennels)\n",
      "  Query 9: 'What are the template ids of any templates used in more than a single document?' (True DB: cre_Doc_Template_Mgt)\n",
      "  Query 10: 'Show name, country, age for all singers ordered by age from the oldest to the youngest.' (True DB: concert_singer)\n",
      "  Query 11: 'Show the student IDs and numbers of friends corresponding to each.' (True DB: network_1)\n",
      "  Query 12: 'What are flight numbers of flights arriving at City \"Aberdeen\"?' (True DB: flight_2)\n",
      "  Query 13: 'How many countries speak both English and Dutch?' (True DB: world_1)\n",
      "  Query 14: 'What are the notes of the death events which has substring 'East'?' (True DB: battle_death)\n",
      "  Query 15: 'What are the names of conductors as well as the corresonding orchestras that they have conducted?' (True DB: orchestra)\n",
      "  Query 16: 'List the earnings of poker players in descending order.' (True DB: poker_player)\n",
      "  Query 17: 'Which owner has paid the largest amount of money in total for their dogs? Show the owner id and zip code.' (True DB: dog_kennels)\n",
      "  Query 18: 'Find the number of flights landing in the city of Aberdeen or Abilene.' (True DB: flight_2)\n",
      "  Query 19: 'How many pets have a greater weight than 10?' (True DB: pets_1)\n",
      "  Query 20: 'Show different citizenships and the maximum net worth of singers of each citizenship.' (True DB: singer)\n",
      "\n",
      "Each of the 20 selected queries will be evaluated against all 166 available Spider database schemas.\n"
     ]
    }
   ],
   "source": [
    "# This cell defines parameters for running the experiment.\n",
    "# It will now randomly select queries and always use ALL database schemas as candidates.\n",
    "\n",
    "import random # Ensure random is imported at the top of your notebook or this cell\n",
    "# import os # Ensure os is imported (likely already done for path joining)\n",
    "# import json # Ensure json is imported (likely already done for loading)\n",
    "\n",
    "# --- 2.1. Experiment Parameters ---\n",
    "# Number of NL queries to RANDOMLY select from dev.json to process.\n",
    "# For initial testing in Colab, use a small subset. For a more thorough run, increase this.\n",
    "NUM_RANDOM_QUERIES_TO_TEST = 100 # For example, test 5 random queries\n",
    "\n",
    "# This will now effectively always be True based on your requirement.\n",
    "# The logic will be set up to use all schemas from all_db_schemas_sql_strings.\n",
    "# We can keep the variable for clarity or remove it if it's always all DBs.\n",
    "# For this implementation, let's explicitly aim for all DBs.\n",
    "print(\"INFO: This experiment configuration will test each randomly selected query against ALL available Spider database schemas.\")\n",
    "\n",
    "\n",
    "# --- 2.2. Randomly Select NL Queries for the Experiment ---\n",
    "# We will randomly sample NUM_RANDOM_QUERIES_TO_TEST queries from the loaded dev_data.\n",
    "if not dev_data: # dev_data should have been loaded in Cell 1\n",
    "    raise ValueError(\"dev_data is not loaded (from dev.json). Cannot select queries. Please run Cell 1 first.\")\n",
    "\n",
    "if len(dev_data) == 0:\n",
    "    raise ValueError(\"dev_data is empty. No queries to select.\")\n",
    "\n",
    "actual_num_queries_to_select = min(NUM_RANDOM_QUERIES_TO_TEST, len(dev_data))\n",
    "# Using min ensures we don't try to sample more queries than available.\n",
    "\n",
    "if actual_num_queries_to_select < NUM_RANDOM_QUERIES_TO_TEST:\n",
    "    print(f\"Warning: Requested {NUM_RANDOM_QUERIES_TO_TEST} random queries, but only {len(dev_data)} are available. Using all {len(dev_data)} queries.\")\n",
    "\n",
    "# Randomly sample without replacementselected_nl_queries = random.sample(dev_data, actual_num_queries_to_select)\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Path to your text file containing lines like:\n",
    "#   Test Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
    "TEXT_QUERIES_FILE = \"/raid/infolab/gaurav/Llama_Spider_A100_Project/100_queries.txt\"\n",
    "\n",
    "if not os.path.exists(TEXT_QUERIES_FILE):\n",
    "    raise FileNotFoundError(f\"Cannot find '{TEXT_QUERIES_FILE}' – make sure it’s in your working directory or update the path.\")\n",
    "\n",
    "selected_nl_queries = []\n",
    "pattern = re.compile(r\"Test Query\\s+\\d+:\\s+'(.+)'\\s+\\(True DB:\\s*([^)]+)\\)\")\n",
    "\n",
    "with open(TEXT_QUERIES_FILE, \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        # Skip any header or non‐“Test Query” lines\n",
    "        if not line.startswith(\"Test Query\"):\n",
    "            continue\n",
    "\n",
    "        m = pattern.match(line)\n",
    "        if not m:\n",
    "            print(f\"Warning: could not parse line:\\n  {line}\")\n",
    "            continue\n",
    "\n",
    "        question_text = m.group(1)\n",
    "        true_db_id    = m.group(2)\n",
    "\n",
    "        # Build the same dict‐structure downstream code expects\n",
    "        selected_nl_queries.append({\n",
    "            \"question\": question_text,\n",
    "            \"db_id\":    true_db_id\n",
    "        })\n",
    "\n",
    "if len(selected_nl_queries) == 0:\n",
    "    raise ValueError(f\"No queries were parsed from '{TEXT_QUERIES_FILE}'. Check your file’s format.\")\n",
    "\n",
    "print(f\"Loaded {len(selected_nl_queries)} queries from '{TEXT_QUERIES_FILE}':\")\n",
    "for i, q in enumerate(selected_nl_queries, 1):\n",
    "    print(f\"  Query {i}: '{q['question']}' (True DB: {q['db_id']})\")\n",
    "\n",
    "# --- 2.3. Determine Candidate Database Schemas for Each Query ---\n",
    "# For this experiment design, we ALWAYS use ALL available database schemas.\n",
    "# all_db_schemas_sql_strings should have been populated in Cell 1.\n",
    "# if not all_sql_output: # Populated in Cell 1\n",
    "#     raise ValueError(\"all_sql_output is empty. Schemas were not converted in Cell 1. Cannot proceed.\")\n",
    "\n",
    "candidate_schemas_for_evaluation = all_db_schemas_sql_strings # Use all converted schemas\n",
    "print(f\"\\nEach of the {len(selected_nl_queries)} selected queries will be evaluated against all {len(candidate_schemas_for_evaluation)} available Spider database schemas.\")\n",
    "\n",
    "if not candidate_schemas_for_evaluation: # Should not happen if all_db_schemas_sql_strings was populated\n",
    "    raise ValueError(\"No candidate schemas available for evaluation. This indicates an issue with schema loading or conversion in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531bac23-fdaf-4552-a664-e0b0ae3ca27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured experiment project directory exists: '/raid/infolab/gaurav/Llama_Spider_A100_Project/randomQ_allDBs_run1'\n",
      "Experiment results will be saved to: /raid/infolab/gaurav/Llama_Spider_A100_Project/randomQ_allDBs_run1/spider_random_query_all_db_scores_prompt_llama-2.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "LOCAL_EXPERIMENT_BASE_DIR = \"/raid/infolab/gaurav/Llama_Spider_A100_Project/\"\n",
    "\n",
    "\n",
    "EXPERIMENT_RUN_NAME = \"randomQ_allDBs_run1\" \n",
    "EXPERIMENT_PROJECT_DIR = os.path.join(LOCAL_EXPERIMENT_BASE_DIR, EXPERIMENT_RUN_NAME)\n",
    "\n",
    "try:\n",
    "    os.makedirs(EXPERIMENT_PROJECT_DIR, exist_ok=True)\n",
    "    print(f\"Ensured experiment project directory exists: '{EXPERIMENT_PROJECT_DIR}'\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory {EXPERIMENT_PROJECT_DIR}: {e}\")\n",
    "    EXPERIMENT_PROJECT_DIR = \".\" \n",
    "\n",
    "\n",
    "RESULTS_FILENAME = \"spider_random_query_all_db_scores_prompt_llama-2.json\"\n",
    "EXPERIMENT_RESULTS_FILE = os.path.join(EXPERIMENT_PROJECT_DIR, RESULTS_FILENAME)\n",
    "\n",
    "print(f\"Experiment results will be saved to: {EXPERIMENT_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06c3025-9d75-4217-8071-1418e90aaa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'get_yes_no_token_ids' defined (with actual logic).\n"
     ]
    }
   ],
   "source": [
    "# Cell defining get_yes_no_token_ids (CORRECTED)\n",
    "def get_yes_no_token_ids(tokenizer_arg):\n",
    "    \"\"\"\n",
    "    Determines the token IDs for 'Yes' and 'No', accounting for potential leading spaces.\n",
    "    Llama-2-chat tends to produce \" Yes\" or \" No\" as single tokens after the prompt.\n",
    "    \"\"\"\n",
    "    # Try with leading space first, as it's common for chat models\n",
    "    yes_token_id_with_space = tokenizer_arg.encode(\" Yes\", add_special_tokens=False)\n",
    "    no_token_id_with_space = tokenizer_arg.encode(\" No\", add_special_tokens=False)\n",
    "\n",
    "    if len(yes_token_id_with_space) == 1 and len(no_token_id_with_space) == 1:\n",
    "        print(\"Using ' Yes' and ' No' (with leading space) for Yes/No token IDs.\")\n",
    "        return yes_token_id_with_space[0], no_token_id_with_space[0] # Explicit return\n",
    "    else:\n",
    "        # Fallback to \"Yes\" and \"No\" without leading space\n",
    "        yes_token_id_no_space = tokenizer_arg.encode(\"Yes\", add_special_tokens=False)\n",
    "        no_token_id_no_space = tokenizer_arg.encode(\"No\", add_special_tokens=False)\n",
    "        if len(yes_token_id_no_space) == 1 and len(no_token_id_no_space) == 1:\n",
    "            print(\"Warning: Using 'Yes' and 'No' (no leading space) for Yes/No token IDs. This might be suboptimal for chat models.\")\n",
    "            return yes_token_id_no_space[0], no_token_id_no_space[0] # Explicit return\n",
    "        else:\n",
    "            # This case is problematic.\n",
    "            print(f\"ERROR: Could not determine reliable single token IDs for 'Yes'/'No' or ' Yes'/' No'.\")\n",
    "            print(f\"Tokenization of ' Yes': {yes_token_id_with_space} (decoded: {[tokenizer_arg.decode(t) for t in yes_token_id_with_space]})\")\n",
    "            print(f\"Tokenization of ' No': {no_token_id_with_space} (decoded: {[tokenizer_arg.decode(t) for t in no_token_id_with_space]})\")\n",
    "            print(f\"Tokenization of 'Yes': {yes_token_id_no_space} (decoded: {[tokenizer_arg.decode(t) for t in yes_token_id_no_space]})\")\n",
    "            print(f\"Tokenization of 'No': {no_token_id_no_space} (decoded: {[tokenizer_arg.decode(t) for t in no_token_id_no_space]})\")\n",
    "            # It's better to raise an error here so the problem is immediately obvious\n",
    "            # rather than returning None and causing a TypeError later.\n",
    "            raise ValueError(\"Unstable tokenization for 'Yes'/'No'. Review tokenization outputs above. Cannot proceed without reliable Yes/No token IDs.\")\n",
    "\n",
    "print(\"Helper function 'get_yes_no_token_ids' defined (with actual logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abb8b6cb-d43f-40a9-bf6e-6779c779201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using 'Yes' and 'No' (no leading space) for Yes/No token IDs. This might be suboptimal for chat models.\n",
      "YES_TOKEN_ID: 3869 ('Yes')\n",
      "NO_TOKEN_ID: 1939 ('No')\n"
     ]
    }
   ],
   "source": [
    "if 'tokenizer' in globals() and tokenizer is not None:\n",
    "    try:\n",
    "        YES_TOKEN_ID, NO_TOKEN_ID = get_yes_no_token_ids(tokenizer)\n",
    "        print(f\"YES_TOKEN_ID: {YES_TOKEN_ID} ('{tokenizer.decode([YES_TOKEN_ID])}')\")\n",
    "        print(f\"NO_TOKEN_ID: {NO_TOKEN_ID} ('{tokenizer.decode([NO_TOKEN_ID])}')\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error defining YES/NO token IDs: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'tokenizer' is not defined. Cannot define YES_TOKEN_ID and NO_TOKEN_ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc6a8811-2963-48c5-9f50-0eba843139cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_yes_prob_and_print_greedy_text(\n",
    "    model_arg,\n",
    "    tokenizer_arg,\n",
    "    system_prompt_arg,\n",
    "    user_prompt_content_arg,\n",
    "    yes_token_id_arg,\n",
    "    no_token_id_arg,\n",
    "    max_prompt_length=4096, # Max length for the input prompt tokenization\n",
    "    max_new_tokens_for_greedy_text=1 # Max new tokens for the printed greedy response\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates P(Yes) based on the logits of the first greedily generated token,\n",
    "    and also prints a longer greedy text generation.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated P(Yes) score.\n",
    "    \"\"\"\n",
    "    prob_yes = 0.0 # Initialized\n",
    "    response_text_greedy_full = \"Error: Generation failed or no scores.\" # Initialized\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_arg},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_content_arg}\n",
    "    ]\n",
    "    prompt_for_model = tokenizer_arg.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer_arg(\n",
    "        prompt_for_model,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_length - max_new_tokens_for_greedy_text - 10 # Ensure space for prompt and new tokens\n",
    "    )\n",
    "    inputs = {k: v.to(model_arg.device) for k, v in inputs.items()}\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    if inputs['input_ids'].shape[1] >= max_prompt_length - max_new_tokens_for_greedy_text - 10:\n",
    "        print(f\"Warning: Prompt for query might have been truncated. Input length: {input_length}\")\n",
    "\n",
    "    prob_yes = 0.0  # Default value\n",
    "    response_text_greedy = \"Error during generation.\" # Default error message\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate a sequence of tokens (up to max_new_tokens_for_greedy_text)\n",
    "        # Get scores for each generated token to calculate P(Yes) from the first one.\n",
    "        generated_outputs = model_arg.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens_for_greedy_text,\n",
    "            do_sample=False,        # Greedy choice\n",
    "            output_scores=True,     # Crucial: returns token scores/logits\n",
    "            return_dict_in_generate=True, # Makes accessing scores easier\n",
    "            pad_token_id=tokenizer_arg.pad_token_id if tokenizer_arg.pad_token_id is not None else tokenizer_arg.eos_token_id\n",
    "        )\n",
    "\n",
    "        # --- Calculate P(Yes) from the scores of the FIRST generated token ---\n",
    "        if generated_outputs.scores and len(generated_outputs.scores) > 0:\n",
    "            first_token_logits = generated_outputs.scores[0] # Logits for the first generated token\n",
    "            \n",
    "            logit_yes = first_token_logits[:, yes_token_id_arg].item()\n",
    "            logit_no = first_token_logits[:, no_token_id_arg].item()\n",
    "\n",
    "            max_logit = max(logit_yes, logit_no)\n",
    "            # Use torch.tensor for device placement if model is on GPU\n",
    "            exp_yes_tensor = torch.exp(torch.tensor(logit_yes - max_logit, device=model_arg.device if hasattr(model_arg, 'device') else 'cpu'))\n",
    "            exp_no_tensor = torch.exp(torch.tensor(logit_no - max_logit, device=model_arg.device if hasattr(model_arg, 'device') else 'cpu'))\n",
    "            prob_yes_tensor = exp_yes_tensor / (exp_yes_tensor + exp_no_tensor)\n",
    "            prob_yes = prob_yes_tensor.item()\n",
    "        else:\n",
    "            print(\"Error: model.generate did not return scores for P(Yes) calculation.\")\n",
    "\n",
    "        # --- Decode the full generated greedy text for printing ---\n",
    "        # generated_outputs.sequences includes the input prompt tokens\n",
    "        all_tokens = generated_outputs.sequences[0]\n",
    "        generated_tokens_only = all_tokens[input_length:] # Slice to get only newly generated tokens\n",
    "        response_text_greedy_full = tokenizer_arg.decode(generated_tokens_only, skip_special_tokens=True).strip()\n",
    "\n",
    "    # --- Print the longer greedy decoding output ---\n",
    "    # print(\"\\n--- Generating with GREEDY DECODING (do_sample=False) ---\")\n",
    "    # print(f\"Greedy Model Response (up to {max_new_tokens_for_greedy_text} new tokens): '{response_text_greedy}'\")\n",
    "\n",
    "    return prob_yes, response_text_greedy_full\n",
    "\n",
    "# --- Example Usage (assuming model, tokenizer, prompts, and token IDs are defined) ---\n",
    "# Make sure these are defined from your notebook:\n",
    "# model, tokenizer, SYSTEM_PROMPT, USER_PROMPT_TEMPLATE (from cell 3dfe1828-...)\n",
    "# YES_TOKEN_ID, NO_TOKEN_ID (from cell abb8b6cb-...)\n",
    "\n",
    "# if 'model' in globals() and 'tokenizer' in globals(): # Check if they exist\n",
    "#     # Sample data for testing\n",
    "#     test_schema_string = \"CREATE TABLE student (name TEXT, age INTEGER)\"\n",
    "#     test_nl_query = \"What is the age of John?\"\n",
    "#     sample_user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "#         schema_string=test_schema_string,\n",
    "#         nl_query=test_nl_query\n",
    "#     )\n",
    "#\n",
    "#     # Ensure chat template is set if necessary (e.g., from cell bc6a8811-...)\n",
    "#     if tokenizer.chat_template is None:\n",
    "#         tokenizer.chat_template = (\n",
    "#             \"{% if messages[0]['role'] == 'system' %}\"\n",
    "#             \"<s>[INST] <<SYS>>\\\\n{{ messages[0]['content'] }}\\\\n<</SYS>>\\\\n\\\\n\"\n",
    "#             \"{% endif %}\"\n",
    "#             \"{{ messages[1]['content'] }} [/INST]\"\n",
    "#             \"{% if add_generation_prompt %} {% endif %}\"\n",
    "#         )\n",
    "#         print(\"Manually set a Llama 2 chat template on tokenizer for test.\")\n",
    "#\n",
    "#\n",
    "#     p_yes_score = get_yes_prob_and_print_greedy_text(\n",
    "#         model,\n",
    "#         tokenizer,\n",
    "#         SYSTEM_PROMPT,\n",
    "#         sample_user_prompt_content,\n",
    "#         YES_TOKEN_ID,\n",
    "#         NO_TOKEN_ID,\n",
    "#         max_prompt_length=4096, # Or your model's context window\n",
    "#         max_new_tokens_for_greedy_text=100 # How long you want the printed greedy text to be\n",
    "#     )\n",
    "#     print(f\"\\nCalculated P(Yes) score: {p_yes_score:.4f}\")\n",
    "#\n",
    "# else:\n",
    "#     print(\"Model and/or tokenizer not loaded. Cannot run example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dfe1828-56ee-4994-91e0-9ecc5acfb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Condensed Prompt Configuration (fits in a 2K‐token window) ---\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert analyst. Decide if a natural‐language question can be answered *only* from the given schema. \n",
    "If all required tables, columns, and join‐paths exist, respond with exactly “Yes”. Otherwise, respond with exactly “No”.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "# Few‐Shot Examples (Spider style)\n",
    "\n",
    "[Schema: student(student_id, student_name); course(course_id, course_name); enrollment(student_id→student, course_id→course)]\n",
    "Q: List the names of all students enrolled in the 'Math' course.\n",
    "Reasoning: enrollment links student↔course; course_name exists; student_name exists → SQL possible.\n",
    "A: Yes\n",
    "\n",
    "[Schema: orders(order_id, customer_id, amount); customer(customer_id, customer_name)]\n",
    "Q: Find the total number of orders placed in 2019.\n",
    "Reasoning: no order_date or year column → cannot filter by 2019.\n",
    "A: No\n",
    "\n",
    "# Now Evaluate\n",
    "\n",
    "[Schema: {schema_string}]\n",
    "Q: {nl_query}\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62edcddb-f870-4d78-ba30-b568fc757b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating with SAMPLING (do_sample=True) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Model Response: 'Yes, I can answer that question.'\n",
      "\n",
      "--- Generating with GREEDY DECODING (do_sample=False) (Standalone Test) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone Greedy Model Response: 'Yes, I can answer that question.\n",
      "\n",
      "The schema provides the necessary tables and columns to answer the question. The employee table has a column for salary, and the foreign key in the works_on table allows us to link an employee's salary to their work on a project.\n",
      "\n",
      "Here's a possible SQL query to retrieve John's salary:\n",
      "```sql\n",
      "SELECT salary\n",
      "FROM employee\n",
      "WHERE eid = (\n",
      "  SELECT lead_e'\n",
      "\n",
      "--- Calling get_yes_prob_and_print_greedy_text ---\n",
      "\n",
      "--- Analysis of results from get_yes_prob_and_print_greedy_text ---\n",
      "Returned P(Yes) score (based on first token logits): 0.9996\n",
      "Returned Generated Text (first 70 chars): 'Yes, I can answer that question.\n",
      "\n",
      "The schema provides the...'\n",
      "Based on the P(Yes) score, 'Yes' is more likely.\n",
      "The returned generated text starts with 'Yes'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import traceback # Ensure traceback is imported for error handling\n",
    "\n",
    "# --- Prerequisites (Ensure these are defined in your notebook from previous cells) ---\n",
    "# model, tokenizer, SYSTEM_PROMPT, USER_PROMPT_TEMPLATE (from cell 3dfe1828-...)\n",
    "# YES_TOKEN_ID, NO_TOKEN_ID (from cell abb8b6cb-...)\n",
    "# tokenizer.pad_token should be set\n",
    "# The function get_yes_prob_and_print_greedy_text (the version that returns two values\n",
    "# and prints internally) must be defined.\n",
    "# ---\n",
    "\n",
    "# --- 1. Create a Sample Input (Schema + Query) ---\n",
    "test_schema_string = \"\"\"\n",
    "CREATE TABLE TestTable (id INT, name TEXT, salary INT);\n",
    "CREATE TABLE department (did INTEGER, dname TEXT, budget REAL, num_employees INTEGER);\n",
    "CREATE TABLE employee (eid INTEGER, ename TEXT, age INTEGER, department_did INTEGER, FOREIGN KEY(department_did) REFERENCES department(did));\n",
    "CREATE TABLE project (pid INTEGER, pname TEXT, lead_eid INTEGER, FOREIGN KEY(lead_eid) REFERENCES employee(eid));\n",
    "CREATE TABLE works_on (employee_eid INTEGER, project_pid INTEGER, hours INTEGER, FOREIGN KEY(employee_eid) REFERENCES employee(eid), FOREIGN KEY(project_pid) REFERENCES project(pid));\n",
    "\"\"\"\n",
    "test_nl_query = \"Can you search me John's Salary\"\n",
    "\n",
    "sample_user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "    schema_string=test_schema_string,\n",
    "    nl_query=test_nl_query\n",
    ")\n",
    "\n",
    "# --- 2. Prepare Inputs for the Model ---\n",
    "_messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": sample_user_prompt_content} # Use the already formatted content\n",
    "]\n",
    "\n",
    "tokenized_input_string = tokenizer.apply_chat_template(\n",
    "    _messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Adjust max_length for input to leave space for generated tokens if needed by generate\n",
    "# For this test script, it's less critical than in the main loop, but good practice.\n",
    "# Assuming model.generate in sections 3 and 4 won't exceed 100 new tokens.\n",
    "input_max_length = 2048 - 100 - 10 # Reserve space for 100 new tokens + buffer\n",
    "if input_max_length < 50: input_max_length = 50\n",
    "\n",
    "inputs = tokenizer(tokenized_input_string, return_tensors=\"pt\", truncation=True, max_length=input_max_length).to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "\n",
    "# --- 3. Generate with SAMPLING ---\n",
    "print(\"\\n--- Generating with SAMPLING (do_sample=True) ---\")\n",
    "max_new_tokens_sample = 10\n",
    "with torch.no_grad():\n",
    "    outputs_sample = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens_sample,\n",
    "        do_sample=True,\n",
    "        num_beams=1,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_tokens_sample = outputs_sample[0][input_length:]\n",
    "response_text_sample = tokenizer.decode(generated_tokens_sample, skip_special_tokens=True).strip()\n",
    "print(f\"Sampled Model Response: '{response_text_sample}'\")\n",
    "\n",
    "\n",
    "# --- 4. Generate with GREEDY DECODING (Standalone) ---\n",
    "# This block is fine as a separate demonstration of greedy decoding,\n",
    "# independent of the get_yes_prob_and_print_greedy_text function's internal greedy generation.\n",
    "print(\"\\n--- Generating with GREEDY DECODING (do_sample=False) (Standalone Test) ---\")\n",
    "max_new_tokens_greedy_standalone = 100 # Length for this specific test\n",
    "with torch.no_grad():\n",
    "    outputs_greedy_standalone = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens_greedy_standalone,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_tokens_greedy_standalone = outputs_greedy_standalone[0][input_length:]\n",
    "response_text_greedy_standalone = tokenizer.decode(generated_tokens_greedy_standalone, skip_special_tokens=True).strip()\n",
    "print(f\"Standalone Greedy Model Response: '{response_text_greedy_standalone}'\")\n",
    "\n",
    "\n",
    "# --- 5. Using get_yes_prob_and_print_greedy_text ---\n",
    "# This function will internally print its own greedy decoding output and return P(Yes) + text.\n",
    "print(\"\\n--- Calling get_yes_prob_and_print_greedy_text ---\")\n",
    "# The 'user_prompt_content_arg' for the function is the fully formatted user prompt content\n",
    "user_prompt_content_for_function = sample_user_prompt_content\n",
    "\n",
    "try:\n",
    "    # Make sure get_yes_prob_and_print_greedy_text is defined and returns two values.\n",
    "    # The function itself will print:\n",
    "    # \"--- Greedy Model Response (up to X new tokens) ---\"\n",
    "    # \"Text: '...'\"\n",
    "    # \"P(Yes) score (from first token logits): Y.YYYY\"\n",
    "    # \"--------------------------------------------------\"\n",
    "\n",
    "    # MODIFICATION: Expect two return values\n",
    "    returned_prob_yes, returned_generated_text = get_yes_prob_and_print_greedy_text(\n",
    "        model_arg=model,\n",
    "        tokenizer_arg=tokenizer,\n",
    "        system_prompt_arg=SYSTEM_PROMPT,\n",
    "        user_prompt_content_arg=user_prompt_content_for_function,\n",
    "        yes_token_id_arg=YES_TOKEN_ID,\n",
    "        no_token_id_arg=NO_TOKEN_ID,\n",
    "        max_prompt_length=4096, \n",
    "        max_new_tokens_for_greedy_text=15 # How long you want the printed greedy text to be\n",
    "    )\n",
    "\n",
    "    # Now, use the returned_prob_yes for your logic here\n",
    "    print(f\"\\n--- Analysis of results from get_yes_prob_and_print_greedy_text ---\")\n",
    "    print(f\"Returned P(Yes) score (based on first token logits): {returned_prob_yes:.4f}\")\n",
    "    print(f\"Returned Generated Text (first 70 chars): '{returned_generated_text[:70]}...'\")\n",
    "\n",
    "    if returned_prob_yes > 0.5:\n",
    "        print(\"Based on the P(Yes) score, 'Yes' is more likely.\")\n",
    "    else:\n",
    "        print(\"Based on the P(Yes) score, 'No' is more likely.\")\n",
    "\n",
    "    # Optional: Parse the returned_generated_text here if you need to make decisions based on it\n",
    "    cleaned_text_start = returned_generated_text.strip().lower()\n",
    "    if re.match(r\"^\\W*yes\", cleaned_text_start): # Assuming re is imported\n",
    "        print(\"The returned generated text starts with 'Yes'.\")\n",
    "    elif re.match(r\"^\\W*no\", cleaned_text_start):\n",
    "        print(\"The returned generated text starts with 'No'.\")\n",
    "    else:\n",
    "        print(\"The returned generated text does not clearly start with 'Yes' or 'No'.\")\n",
    "\n",
    "except NameError as ne:\n",
    "    if 'get_yes_prob_and_print_greedy_text' in str(ne):\n",
    "        print(\"ERROR: The function 'get_yes_prob_and_print_greedy_text' is not defined.\")\n",
    "        print(\"Please ensure you have run the cell where it's defined.\")\n",
    "    elif 're' in str(ne) and \"re.match\" in traceback.format_exc():\n",
    "        print(\"ERROR: The 're' module (for regular expressions) is not imported. Please add 'import re'.\")\n",
    "    else:\n",
    "        print(f\"A NameError occurred: {ne}\")\n",
    "        traceback.print_exc()\n",
    "except TypeError as te:\n",
    "    # This catches if get_yes_prob_and_print_greedy_text doesn't return two values\n",
    "    if \"cannot unpack non-iterable\" in str(te):\n",
    "        print(f\"TypeError during unpacking: {te}\")\n",
    "        print(\"This usually means 'get_yes_prob_and_print_greedy_text' is not returning two values (prob_yes, generated_text).\")\n",
    "        print(\"Please RE-RUN THE DEFINITION CELL for 'get_yes_prob_and_print_greedy_text'.\")\n",
    "    else:\n",
    "        print(f\"A TypeError occurred: {te}\")\n",
    "        traceback.print_exc()\n",
    "except Exception as e:\n",
    "    print(f\"Error running get_yes_prob_and_print_greedy_text section: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75247d87-a7f5-4ca4-8770-78e43e0c7e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment: 20 Random Queries vs. 166 Total DB Schemas ---\n",
      "!!! WARNING: Using function that prints greedy output for EACH schema. This will be VERY VERBOSE and SLOW. !!!\n",
      "--- Filtering based on TEXT output starting with 'Yes', then ranking by P(Yes) score. ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50cbb7f473c41bdac7cd5bfc2e5f64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing NL Queries:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Query 1/20 (ID: spider_dev_q0_idx0): 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95945ff248c41739dd4ad5daae880db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  DBs for Q:spider_dev_q0_idx0:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: No text generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# --- MODIFICATION: Call the function that returns P(Yes) AND text ---\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     p_yes_score_from_logits, generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mget_yes_prob_and_print_greedy_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#expects 2 return values\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_prompt_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_prompt_content_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_prompt_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43myes_token_id_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYES_TOKEN_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_token_id_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNO_TOKEN_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Max length for the input prompt tokenization\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens_for_greedy_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Max new tokens for the printed greedy response\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;66;43;03m# for the main loop, can be longer for debugging.\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;66;43;03m# This text will be parsed.\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ne:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_yes_prob_and_print_greedy_text\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ne):\n",
      "Cell \u001b[0;32mIn[54], line 47\u001b[0m, in \u001b[0;36mget_yes_prob_and_print_greedy_text\u001b[0;34m(model_arg, tokenizer_arg, system_prompt_arg, user_prompt_content_arg, yes_token_id_arg, no_token_id_arg, max_prompt_length, max_new_tokens_for_greedy_text)\u001b[0m\n\u001b[1;32m     42\u001b[0m response_text_greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Default error message\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Generate a sequence of tokens (up to max_new_tokens_for_greedy_text)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Get scores for each generated token to calculate P(Yes) from the first one.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens_for_greedy_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Greedy choice\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Crucial: returns token scores/logits\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Makes accessing scores easier\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# --- Calculate P(Yes) from the scores of the FIRST generated token ---\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generated_outputs\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generated_outputs\u001b[38;5;241m.\u001b[39mscores) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/generation/utils.py:3557\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3554\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3557\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:496\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    492\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    494\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:393\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    325\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/bitsandbytes/functional.py:1120\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m quant_state\u001b[38;5;241m.\u001b[39mabsmax\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quant_state\u001b[38;5;241m.\u001b[39mnested:\n\u001b[0;32m-> 1120\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m \u001b[43mdequantize_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     absmax \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m quant_state\u001b[38;5;241m.\u001b[39moffset\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m absmax\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n",
      "File \u001b[0;32m~/Llama_Spider_A100_Project/miniconda3/envs/llama_spider_env/lib/python3.10/site-packages/bitsandbytes/functional.py:864\u001b[0m, in \u001b[0;36mdequantize_blockwise\u001b[0;34m(A, quant_state, absmax, code, out, blocksize, nested)\u001b[0m\n\u001b[1;32m    851\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbitsandbytes\u001b[38;5;241m.\u001b[39mdequantize_blockwise\u001b[38;5;241m.\u001b[39mout(\n\u001b[1;32m    852\u001b[0m         A,\n\u001b[1;32m    853\u001b[0m         absmax,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         out\u001b[38;5;241m=\u001b[39mout,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbitsandbytes\u001b[38;5;241m.\u001b[39mdequantize_blockwise\u001b[38;5;241m.\u001b[39mdefault(\n\u001b[1;32m    862\u001b[0m     A,\n\u001b[1;32m    863\u001b[0m     absmax,\n\u001b[0;32m--> 864\u001b[0m     \u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    865\u001b[0m     quant_state\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[1;32m    866\u001b[0m     quant_state\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    867\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Ensure these imports are at the top of your script/notebook ---\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "import re # For parsing \"Yes\" / \"No\"\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# model, tokenizer, SYSTEM_PROMPT, USER_PROMPT_TEMPLATE,\n",
    "# YES_TOKEN_ID, NO_TOKEN_ID,\n",
    "# selected_nl_queries, candidate_schemas_for_evaluation, EXPERIMENT_RESULTS_FILE\n",
    "# AND the MODIFIED function get_yes_prob_and_print_greedy_text must be defined\n",
    "# --- (Assume these are correctly defined above this cell) ---\n",
    "\n",
    "# --- 3.1. Initialize Results Storage ---\n",
    "experiment_all_query_results = []\n",
    "\n",
    "# --- 3.2. Start the Loop ---\n",
    "print(f\"\\n--- Starting Experiment: {len(selected_nl_queries)} Random Queries vs. {len(candidate_schemas_for_evaluation)} Total DB Schemas ---\")\n",
    "print(\"!!! WARNING: Using function that prints greedy output for EACH schema. This will be VERY VERBOSE and SLOW. !!!\")\n",
    "print(\"--- Filtering based on TEXT output starting with 'Yes', then ranking by P(Yes) score. ---\")\n",
    "\n",
    "\n",
    "# Outer loop\n",
    "for query_idx, nl_query_info in enumerate(tqdm(selected_nl_queries, desc=\"Processing NL Queries\")):\n",
    "    current_nl_query_text = nl_query_info['question']\n",
    "    true_db_id_for_query = nl_query_info['db_id']\n",
    "    experiment_query_id = f\"spider_dev_q{query_idx}_{nl_query_info.get('query_id', 'idx'+str(query_idx))}\"\n",
    "\n",
    "    tqdm.write(f\"\\nProcessing Query {query_idx + 1}/{len(selected_nl_queries)} (ID: {experiment_query_id}): '{current_nl_query_text}' (True DB: {true_db_id_for_query})\")\n",
    "\n",
    "    # Stores schemas for which the *parsed text* starts with \"Yes\"\n",
    "    textually_yes_schemas_with_scores = []\n",
    "    \n",
    "    # Stores all raw outputs for this query if needed for later full dump (optional)\n",
    "    # all_outputs_for_this_query = []\n",
    "\n",
    "\n",
    "    # Inner loop\n",
    "    for candidate_db_id, candidate_schema_sql in tqdm(\n",
    "        candidate_schemas_for_evaluation.items(),\n",
    "        desc=f\"  DBs for Q:{experiment_query_id[:20]}\",\n",
    "        leave=False\n",
    "    ):\n",
    "        user_prompt_content = USER_PROMPT_TEMPLATE.format(\n",
    "            schema_string=candidate_schema_sql,\n",
    "            nl_query=current_nl_query_text\n",
    "        )\n",
    "        p_yes_score_from_logits = -1.0\n",
    "        generated_text = \"Error: No text generated.\"\n",
    "\n",
    "        try:\n",
    "            # --- MODIFICATION: Call the function that returns P(Yes) AND text ---\n",
    "            p_yes_score_from_logits, generated_text = get_yes_prob_and_print_greedy_text( #expects 2 return values\n",
    "                model_arg=model,\n",
    "                tokenizer_arg=tokenizer,\n",
    "                system_prompt_arg=SYSTEM_PROMPT,\n",
    "                user_prompt_content_arg=user_prompt_content,\n",
    "                yes_token_id_arg=YES_TOKEN_ID,\n",
    "                no_token_id_arg=NO_TOKEN_ID,\n",
    "                max_prompt_length=4096, # Max length for the input prompt tokenization\n",
    "                max_new_tokens_for_greedy_text=15 # Max new tokens for the printed greedy response\n",
    "                                                      # for the main loop, can be longer for debugging.\n",
    "                                                      # This text will be parsed.\n",
    "            )\n",
    "        except NameError as ne:\n",
    "            if 'get_yes_prob_and_print_greedy_text' in str(ne):\n",
    "                tqdm.write(\"FATAL ERROR: 'get_yes_prob_and_print_greedy_text' is not defined. Stopping experiment.\")\n",
    "                raise\n",
    "            else:\n",
    "                tqdm.write(f\"    ERROR: NameError for QID '{experiment_query_id}', DB '{candidate_db_id}': {ne}\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    ERROR: Exception in func for QID '{experiment_query_id}', DB '{candidate_db_id}': {type(e).__name__} - {e}\")\n",
    "\n",
    "        # Optional: store all raw outputs\n",
    "        # all_outputs_for_this_query.append({\n",
    "        #     'candidate_db_id': candidate_db_id,\n",
    "        #     'p_yes_score_logits': p_yes_score_from_logits,\n",
    "        #     'generated_text': generated_text\n",
    "        # })\n",
    "\n",
    "        # --- MODIFICATION: Parse generated_text for \"Yes\" ---\n",
    "        # Make it case-insensitive and robust to leading/trailing spaces or simple punctuation\n",
    "        # We are looking for the text starting with \"Yes\"\n",
    "        # A simple check:\n",
    "        cleaned_text_start = generated_text.strip().lower()\n",
    "        # More robust: remove common punctuation at the start of \"Yes\" if any\n",
    "        # e.g., \" Yes.\" or \"Yes,\"\n",
    "        if re.match(r\"^\\W*yes\", cleaned_text_start): # \\W* matches zero or more non-alphanumeric characters\n",
    "            text_is_affirmative = True\n",
    "        else:\n",
    "            text_is_affirmative = False\n",
    "\n",
    "        if text_is_affirmative:\n",
    "            textually_yes_schemas_with_scores.append({\n",
    "                'candidate_db_id': candidate_db_id,\n",
    "                'p_yes_score': p_yes_score_from_logits, # Store the P(Yes) from logits for ranking\n",
    "                'generated_text_preview': generated_text[:70] + \"...\" if len(generated_text) > 70 else generated_text # Store a preview\n",
    "            })\n",
    "            # tqdm.write(f\"    DB: {candidate_db_id} - Text implies YES. P(Yes from logits): {p_yes_score_from_logits:.4f}\")\n",
    "        # else:\n",
    "            # tqdm.write(f\"    DB: {candidate_db_id} - Text implies NO or unclear. Text: '{generated_text[:30]}...'\")\n",
    "\n",
    "\n",
    "    # --- MODIFICATION: Rank only the \"textually Yes\" schemas by their P(Yes) logit score ---\n",
    "    ranked_textually_yes_databases = sorted(\n",
    "        textually_yes_schemas_with_scores,\n",
    "        key=lambda x: x['p_yes_score'], # Rank by the P(Yes) score from logits\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    experiment_all_query_results.append({\n",
    "        'experiment_query_id': experiment_query_id,\n",
    "        'nl_query_text': current_nl_query_text,\n",
    "        'true_db_id': true_db_id_for_query,\n",
    "        'ranked_databases_with_scores': ranked_textually_yes_databases # Save this filtered & ranked list\n",
    "        # 'all_raw_outputs_for_query': all_outputs_for_this_query # Optionally save all raw outputs\n",
    "    })\n",
    "\n",
    "    # Periodic Saving\n",
    "    if (query_idx + 1) % 1 == 0 or (query_idx + 1) == len(selected_nl_queries):\n",
    "        try:\n",
    "            with open(EXPERIMENT_RESULTS_FILE, 'w') as f_out:\n",
    "                json.dump(experiment_all_query_results, f_out, indent=2)\n",
    "            tqdm.write(f\"  Successfully saved intermediate results ({len(experiment_all_query_results)} queries) to {EXPERIMENT_RESULTS_FILE}\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  ERROR: Could not save intermediate results: {e}\")\n",
    "\n",
    "# --- 3.4. Experiment Loop Completion ---\n",
    "print(\"\\n--- Experiment Loop Finished ---\")\n",
    "if experiment_all_query_results:\n",
    "    print(f\"Processed {len(experiment_all_query_results)} queries in total.\")\n",
    "    try:\n",
    "        with open(EXPERIMENT_RESULTS_FILE, 'w') as f_out:\n",
    "            json.dump(experiment_all_query_results, f_out, indent=2)\n",
    "        print(f\"Final results (filtered by text starting with 'Yes', ranked by P(Yes) score) saved to {EXPERIMENT_RESULTS_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save final results: {e}\")\n",
    "else:\n",
    "    print(\"No results (or no textually 'Yes' schemas) generated. Check logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87ed3a14-354d-4762-a702-1fce12986b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from /raid/infolab/gaurav/Llama_Spider_A100_Project/randomQ_allDBs_run1/spider_random_query_all_db_scores_prompt_llama-2.json for evaluation...\n",
      "Successfully loaded 20 results from file.\n",
      "\n",
      "--- Evaluation: Recall@K ---\n",
      "Evaluated on 20 queries.\n",
      "Recall@1: 25.00%\n",
      "Recall@3: 45.00%\n",
      "Recall@5: 75.00%\n",
      "Recall@10: 85.00%\n",
      "Saved evaluation results to 'recall_k_results_context_4096_prompt_changed.json'\n",
      "\n",
      "--- Sample Detailed Query Results (Top 5 Queries) ---\n",
      "\n",
      "Query 1: 'What are the names and release years for all the songs of the youngest singer?' (True DB: concert_singer)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. music_2  (Score: 0.9992)\n",
      "    2. music_4  (Score: 0.9989)\n",
      "    3. music_1  (Score: 0.9987)\n",
      "    4. singer  (Score: 0.9986)\n",
      "    5. concert_singer* (Score: 0.9979)\n",
      "\n",
      "Query 2: 'What are names of countries with the top 3 largest population?' (True DB: world_1)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. geo  (Score: 0.9996)\n",
      "    2. roller_coaster  (Score: 0.9994)\n",
      "    3. match_season  (Score: 0.9993)\n",
      "    4. world_1* (Score: 0.9992)\n",
      "    5. city_record  (Score: 0.9989)\n",
      "\n",
      "Query 3: 'What are the names and birth dates of people, ordered by their names in alphabetical order?' (True DB: poker_player)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. company_employee  (Score: 0.9994)\n",
      "    2. music_2  (Score: 0.9993)\n",
      "    3. body_builder  (Score: 0.9993)\n",
      "    4. gymnast  (Score: 0.9993)\n",
      "    5. poker_player* (Score: 0.9992)\n",
      "\n",
      "Query 4: 'How many different store locations are there?' (True DB: employee_hire_evaluation)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. coffee_shop  (Score: 0.9996)\n",
      "    2. loan_1  (Score: 0.9996)\n",
      "    3. restaurants  (Score: 0.9996)\n",
      "    4. company_1  (Score: 0.9995)\n",
      "    5. company_office  (Score: 0.9995)\n",
      "\n",
      "Query 5: 'How many different nationalities do conductors have?' (True DB: orchestra)\n",
      "  Top Ranked Databases (with P(Yes) scores):\n",
      "    1. match_season  (Score: 0.9994)\n",
      "    2. icfp_1  (Score: 0.9994)\n",
      "    3. company_employee  (Score: 0.9994)\n",
      "    4. orchestra* (Score: 0.9993)\n",
      "    5. world_1  (Score: 0.9992)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path where the evaluation summary (Recall@K results) will be saved\n",
    "EVAL_RESULTS_SAVE_PATH = \"recall_k_results_context_4096_prompt_changed.json\"\n",
    "\n",
    "# --- 4.1. Define Recall@K Calculation Function ---\n",
    "def calculate_recall_at_k_metric(all_query_results_list, k_values_list):\n",
    "    \"\"\"\n",
    "    Calculates Recall@K for a list of K values.\n",
    "    Each item in all_query_results_list should be a dictionary with:\n",
    "        'true_db_id': The ground truth database ID for the query.\n",
    "        'ranked_databases_with_scores': A list of {'candidate_db_id': id, 'p_yes_score': score},\n",
    "                                         sorted by score in descending order.\n",
    "    \"\"\"\n",
    "    recall_counts = {k: 0 for k in k_values_list}  # Stores how many times true_db was in top K\n",
    "    total_valid_queries = 0  # Queries for which we have a true_db_id\n",
    "\n",
    "    if not all_query_results_list:\n",
    "        return {k: 0.0 for k in k_values_list}, 0\n",
    "\n",
    "    for query_result in all_query_results_list:\n",
    "        true_db = query_result.get('true_db_id')\n",
    "        ranked_dbs_info = query_result.get('ranked_databases_with_scores')\n",
    "\n",
    "        if true_db is None or ranked_dbs_info is None:\n",
    "            print(f\"Warning: Skipping query result due to missing 'true_db_id' or 'ranked_databases_with_scores': \"\n",
    "                  f\"{query_result.get('experiment_query_id', 'Unknown Query')}\")\n",
    "            continue  # Skip if essential information is missing\n",
    "\n",
    "        total_valid_queries += 1\n",
    "        # Extract just the DB IDs from the ranked list\n",
    "        ranked_db_ids_only = [item['candidate_db_id'] for item in ranked_dbs_info]\n",
    "\n",
    "        for k in k_values_list:\n",
    "            # Get the top K predicted database IDs\n",
    "            top_k_predicted_dbs = ranked_db_ids_only[:k]\n",
    "            if true_db in top_k_predicted_dbs:\n",
    "                recall_counts[k] += 1\n",
    "\n",
    "    # Calculate final recall percentages\n",
    "    recall_percentages = {}\n",
    "    if total_valid_queries > 0:\n",
    "        for k in k_values_list:\n",
    "            recall_percentages[k] = (recall_counts[k] / total_valid_queries) * 100.0  # As percentage\n",
    "    else:\n",
    "        recall_percentages = {k: 0.0 for k in k_values_list}\n",
    "\n",
    "    return recall_percentages, total_valid_queries\n",
    "\n",
    "\n",
    "# --- 4.2. Perform Evaluation ---\n",
    "# Load results if this cell is run in a new session and experiment_all_query_results isn't in memory\n",
    "# (assuming results were saved to EXPERIMENT_RESULTS_FILE)\n",
    "loaded_results_for_eval = None\n",
    "if 'experiment_all_query_results' in globals() and experiment_all_query_results:\n",
    "    print(\"Using in-memory experiment_all_query_results for evaluation.\")\n",
    "    loaded_results_for_eval = experiment_all_query_results\n",
    "elif os.path.exists(EXPERIMENT_RESULTS_FILE):\n",
    "    print(f\"Loading results from {EXPERIMENT_RESULTS_FILE} for evaluation...\")\n",
    "    try:\n",
    "        with open(EXPERIMENT_RESULTS_FILE, 'r') as f_in:\n",
    "            loaded_results_for_eval = json.load(f_in)\n",
    "        print(f\"Successfully loaded {len(loaded_results_for_eval)} results from file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results from file for evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No results available in memory or in the specified results file for evaluation.\")\n",
    "\n",
    "if loaded_results_for_eval:\n",
    "    K_VALUES_TO_EVALUATE = [1, 3, 5, 10]  # Define the K values you care about\n",
    "    recall_scores_map, num_queries_evaluated = calculate_recall_at_k_metric(\n",
    "        loaded_results_for_eval, K_VALUES_TO_EVALUATE\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Evaluation: Recall@K ---\")\n",
    "    print(f\"Evaluated on {num_queries_evaluated} queries.\")\n",
    "    for k_val, recall_val in recall_scores_map.items():\n",
    "        print(f\"Recall@{k_val}: {recall_val:.2f}%\")\n",
    "\n",
    "    # --- 4.2.1. Save evaluation results to a JSON file ---\n",
    "    try:\n",
    "        eval_summary = {\n",
    "            \"num_queries_evaluated\": num_queries_evaluated,\n",
    "            \"recall_scores\": recall_scores_map\n",
    "        }\n",
    "        with open(EVAL_RESULTS_SAVE_PATH, 'w') as fout:\n",
    "            json.dump(eval_summary, fout, indent=2)\n",
    "        print(f\"Saved evaluation results to '{EVAL_RESULTS_SAVE_PATH}'\")\n",
    "    except Exception as save_err:\n",
    "        print(f\"Error saving evaluation results: {save_err}\")\n",
    "\n",
    "    # --- 4.3. Optional: Print Detailed Results for a Few Queries ---\n",
    "    print(\"\\n--- Sample Detailed Query Results (Top 5 Queries) ---\")\n",
    "    for i, res in enumerate(loaded_results_for_eval[:5]):  # Show for first 5 queries\n",
    "        print(f\"\\nQuery {i+1}: '{res.get('nl_query_text', '<no text>')}' (True DB: {res.get('true_db_id')})\")\n",
    "        print(\"  Top Ranked Databases (with P(Yes) scores):\")\n",
    "        for rank, db_info in enumerate(res.get('ranked_databases_with_scores', [])[:5]):  # Show top 5 ranked DBs\n",
    "            is_true_db_char = \"*\" if db_info['candidate_db_id'] == res['true_db_id'] else \" \"\n",
    "            print(f\"    {rank+1}. {db_info['candidate_db_id']}{is_true_db_char} \"\n",
    "                  f\"(Score: {db_info['p_yes_score']:.4f})\")\n",
    "else:\n",
    "    print(\"Cannot perform evaluation as no results were loaded or generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama_spider_env)",
   "language": "python",
   "name": "llama_spider_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
