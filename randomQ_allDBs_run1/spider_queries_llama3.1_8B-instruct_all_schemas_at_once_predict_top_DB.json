[
  {
    "experiment_query_id": "spider_dev_q0_idx0",
    "nl_query_text": "How many available features are there in total?",
    "true_db_id": "real_estate_properties",
    "predicted_db_id": "real_estate_properties",
    "raw_model_output": "The most relevant database_id is: real_estate_properties"
  },
  {
    "experiment_query_id": "spider_dev_q1_idx1",
    "nl_query_text": "What are the name of the countries where there is not a single car maker?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q2_idx2",
    "nl_query_text": "What are the date and the operating professional's first name of each treatment?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 10"
  },
  {
    "experiment_query_id": "spider_dev_q3_idx3",
    "nl_query_text": "List each owner's first name, last name, and the size of his for her dog.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"List each owner's first name"
  },
  {
    "experiment_query_id": "spider_dev_q4_idx4",
    "nl_query_text": "Find the first name and age of students who have a dog but do not have a cat as a pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine which database is most relevant for answering the question. The question asks about"
  },
  {
    "experiment_query_id": "spider_dev_q5_idx5",
    "nl_query_text": "What is the number of cars with a greater accelerate than the one with the most horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q6_idx6",
    "nl_query_text": "What are the names of the teachers who are aged either 32 or 33?",
    "true_db_id": "course_teach",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q7_idx7",
    "nl_query_text": "Return the maximum final tables made across all poker players who have earnings below 200000.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q8_idx8",
    "nl_query_text": "Show template ids, version numbers, and template type codes for all templates.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q9_idx9",
    "nl_query_text": "What is the most common nationality of people?",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no database schema that contains information about the most common nationality of people"
  },
  {
    "experiment_query_id": "spider_dev_q10_idx10",
    "nl_query_text": "find the pixel aspect ratio and nation of the tv channels that do not use English.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q11_idx11",
    "nl_query_text": "What is the template type code of the template used by document with the name \"Data base\"?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't help you with that. The provided text does not contain any information about a database named"
  },
  {
    "experiment_query_id": "spider_dev_q12_idx12",
    "nl_query_text": "What is the maker of the carr produced in the earliest year and what year was it?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q13_idx13",
    "nl_query_text": "Count the number of high schoolers.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q14_idx14",
    "nl_query_text": "Find the average life expectancy and total population for each continent where the average life expectancy is shorter than 72?",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q15_idx15",
    "nl_query_text": "List the date of each treatment, together with the first name of the professional who operated it.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q16_idx16",
    "nl_query_text": "Show all template ids and number of documents using each template.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q17_idx17",
    "nl_query_text": "Who is the first student to register? List the first name, middle name and last name.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q18_idx18",
    "nl_query_text": "Show the name of the teacher for the math course.",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no question that matches the instruction to show the name of the teacher"
  },
  {
    "experiment_query_id": "spider_dev_q19_idx19",
    "nl_query_text": "Find the id and name of the museum that has the most staff members?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: museum"
  },
  {
    "experiment_query_id": "spider_dev_q20_idx20",
    "nl_query_text": "What are flight numbers of flights departing from Airport \"APG\"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q21_idx21",
    "nl_query_text": "What are the names of poker players in descending order of earnings?",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q22_idx22",
    "nl_query_text": "What is the average earnings of poker players?",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q23_idx23",
    "nl_query_text": "Which year had the most matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "database_id: game_1"
  },
  {
    "experiment_query_id": "spider_dev_q24_idx24",
    "nl_query_text": "Find the names of stores whose number products is more than the average number of products.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q25_idx25",
    "nl_query_text": "What is the money rank of the tallest poker player?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q26_idx26",
    "nl_query_text": "What is the document id with least number of paragraphs?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q27_idx27",
    "nl_query_text": "How many cars have a larger accelerate than the car with the largest horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"How many cars have a larger accelerate than the car with the largest horsepower?\","
  },
  {
    "experiment_query_id": "spider_dev_q28_idx28",
    "nl_query_text": "What is the program id and the summary of the degree that has the most students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q29_idx29",
    "nl_query_text": "What is the number of nations that use English and Dutch?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q30_idx30",
    "nl_query_text": "What is the document id, template id and description for document named \"Robbin CV\"?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q31_idx31",
    "nl_query_text": "What are the codes of countries where Spanish is spoken by the largest percentage of people?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or determine the most relevant database for the question."
  },
  {
    "experiment_query_id": "spider_dev_q32_idx32",
    "nl_query_text": "What is the last name of the student who has a cat that is 3 years old?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you determine which database is most relevant to the"
  },
  {
    "experiment_query_id": "spider_dev_q33_idx33",
    "nl_query_text": "What are the first name and last name of the professionals who have done treatment with cost below average?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you identify the most relevant database for the given"
  },
  {
    "experiment_query_id": "spider_dev_q34_idx34",
    "nl_query_text": "What is average life expectancy in the countries where English is not the official language?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no database schema that directly answers the question about average life expectancy in"
  },
  {
    "experiment_query_id": "spider_dev_q35_idx35",
    "nl_query_text": "What is the average weight of cars each year?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't help with that."
  },
  {
    "experiment_query_id": "spider_dev_q36_idx36",
    "nl_query_text": "Find the first name and age of students who have a pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"Find the first name and age"
  },
  {
    "experiment_query_id": "spider_dev_q37_idx37",
    "nl_query_text": "List all singer names in concerts in year 2014.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q38_idx38",
    "nl_query_text": "Return the names of cities that have a population between 160000 and 900000 .",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "city_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q39_idx39",
    "nl_query_text": "Which city and country is the Alton airport at?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q40_idx40",
    "nl_query_text": "find the names of museums which have more staff than the minimum staff number of all museums opened after 2010.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you with the task."
  },
  {
    "experiment_query_id": "spider_dev_q41_idx41",
    "nl_query_text": "Tell me the age of the oldest dog.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q42_idx42",
    "nl_query_text": "How many orchestras does each record company manage?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q43_idx43",
    "nl_query_text": "Which distinctive models are produced by maker with the full name General Motors or weighing more than 3500?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q44_idx44",
    "nl_query_text": "What are the names of players who won in both 2013 and 2016?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q45_idx45",
    "nl_query_text": "Give the flight numbers of flights leaving from APG.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q46_idx46",
    "nl_query_text": "What is the average edispl for all volvos?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q47_idx47",
    "nl_query_text": "Show the names of conductors and the orchestras they have conducted.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q48_idx48",
    "nl_query_text": "What is the average and maximum age for each pet type?",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q49_idx49",
    "nl_query_text": "What are the death and injury situations caused by the ship with tonnage 't'?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "database_id: battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q50_idx50",
    "nl_query_text": "List the arrival date and the departure date for all the dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "The most relevant database_id is: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q51_idx51",
    "nl_query_text": "What is the average age of all the dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "None of the given databases have a table or schema that directly relates to the question \"What is the"
  },
  {
    "experiment_query_id": "spider_dev_q52_idx52",
    "nl_query_text": "What are years of founding for orchestras that have had more than a single performance?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q53_idx53",
    "nl_query_text": "Find the number of pets whose weight is heavier than 10.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q54_idx54",
    "nl_query_text": "List the email, cell phone and home phone of all the professionals.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 7"
  },
  {
    "experiment_query_id": "spider_dev_q55_idx55",
    "nl_query_text": "What are all different template ids used for documents, and how many times were each of them used?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "error",
    "raw_model_output": ""
  },
  {
    "experiment_query_id": "spider_dev_q56_idx56",
    "nl_query_text": "What are the regions that use English or Dutch?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas, I found that the database with the most relevant information for the question \""
  },
  {
    "experiment_query_id": "spider_dev_q57_idx57",
    "nl_query_text": "Return the different descriptions for templates that have been used in a document.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q58_idx58",
    "nl_query_text": "List pairs of the owner's first name and the dogs's name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas, I found that the most relevant database for answering the question \"List"
  },
  {
    "experiment_query_id": "spider_dev_q59_idx59",
    "nl_query_text": "What is the number of cartoones written by Joseph Kuhr?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q60_idx60",
    "nl_query_text": "List the names of conductors in ascending order of age.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q61_idx61",
    "nl_query_text": "Find the first name and gender of student who have more than one pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "The most relevant database_id is: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q62_idx62",
    "nl_query_text": "Show the names of students who have a grade higher than 5 and have at least 2 friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q63_idx63",
    "nl_query_text": "Count the number of distinct store locations.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q64_idx64",
    "nl_query_text": "What is the code of airport that has the highest number of flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q65_idx65",
    "nl_query_text": "What is the name of the series that has the episode \"A Love of a Lifetime\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q66_idx66",
    "nl_query_text": "Show the names of singers and the total sales of their songs.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q67_idx67",
    "nl_query_text": "Find the number of orchestras whose record format is \"CD\" or \"DVD\".",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q68_idx68",
    "nl_query_text": "How many poker players are there?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q69_idx69",
    "nl_query_text": "How many matches were played in each year?",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "database_id: match_season"
  },
  {
    "experiment_query_id": "spider_dev_q70_idx70",
    "nl_query_text": "Find the first names of owners living in Virginia and the names of dogs they own.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is not enough information to determine the most relevant database_id."
  },
  {
    "experiment_query_id": "spider_dev_q71_idx71",
    "nl_query_text": "Show paragraph details for paragraph with text 'Korea ' .",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 3"
  },
  {
    "experiment_query_id": "spider_dev_q72_idx72",
    "nl_query_text": "Count the number of friends Kyle has.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q73_idx73",
    "nl_query_text": "What are the ids and names of all countries that either have more than 3 car makers or produce fiat model ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q74_idx74",
    "nl_query_text": "Return the names of the 3 countries with the fewest people.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q75_idx75",
    "nl_query_text": "What are the name, population, and life expectancy of the largest Asian country by land?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q76_idx76",
    "nl_query_text": "Which country has the most of TV Channels? List the country and number of TV Channels it has.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "database_id: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q77_idx77",
    "nl_query_text": "Find the type and weight of the youngest pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "pet_store"
  },
  {
    "experiment_query_id": "spider_dev_q78_idx78",
    "nl_query_text": "What are the country codes for countries that do not speak English?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q79_idx79",
    "nl_query_text": "What are the airline names and abbreviations for airlines in the USA?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q80_idx80",
    "nl_query_text": "Which are the car makers which produce at least 2 models and more than 3 car makers ? List the id and the maker .",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q81_idx81",
    "nl_query_text": "How many matches were played in 2013 or 2016?",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "match_season"
  },
  {
    "experiment_query_id": "spider_dev_q82_idx82",
    "nl_query_text": "Show all template type codes and the number of documents using each type.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q83_idx83",
    "nl_query_text": "Find the make and production time of the cars that were produced in the earliest year?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q84_idx84",
    "nl_query_text": "How many cartoons were written by \"Joseph Kuhr\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q85_idx85",
    "nl_query_text": "What are the names of countries that speak more than 2 languages, as well as how many languages they speak?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q86_idx86",
    "nl_query_text": "Give me all the information about hiring.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "database_id: employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q87_idx87",
    "nl_query_text": "Find the average age of students who do not have any pet .",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q88_idx88",
    "nl_query_text": "What are the names of conductors who have conducted orchestras founded after the year 2008?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q89_idx89",
    "nl_query_text": "How many car models are produced by each maker ? Only list the count and the maker full name .",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q90_idx90",
    "nl_query_text": "Find the year that has the most number of matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "match_season"
  },
  {
    "experiment_query_id": "spider_dev_q91_idx91",
    "nl_query_text": "What are the different models created by either the car maker General Motors or weighed more than 3500?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q92_idx92",
    "nl_query_text": "What are the names and release years for all the songs of the youngest singer?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q93_idx93",
    "nl_query_text": "Find the id of students who do not have a cat pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 3"
  },
  {
    "experiment_query_id": "spider_dev_q94_idx94",
    "nl_query_text": "List the title of all  Cartoons showed on TV Channel with series name \"Sky Radio\".",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q95_idx95",
    "nl_query_text": "Return the id and type code of the template that is used for the greatest number of documents.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q96_idx96",
    "nl_query_text": "Return the different names of cities that are in Asia and for which Chinese is the official language.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "city_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q97_idx97",
    "nl_query_text": "List all document ids with at least two paragraphs.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q98_idx98",
    "nl_query_text": "Which address holds the most number of students currently? List the address id and all lines.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "database_id: student_1"
  },
  {
    "experiment_query_id": "spider_dev_q99_idx99",
    "nl_query_text": "Show ids for all templates not used by any document.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q100_idx100",
    "nl_query_text": "List the vote ids, phone numbers and states of all votes.",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q101_idx101",
    "nl_query_text": "Show the stadium names without any concert.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q102_idx102",
    "nl_query_text": "find the id, name and age for visitors who visited some museums more than once.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "museum_visit",
    "raw_model_output": "database_id: museum_visit"
  },
  {
    "experiment_query_id": "spider_dev_q103_idx103",
    "nl_query_text": "Show the document id with paragraph text 'Brazil' and 'Ireland'.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q104_idx104",
    "nl_query_text": "Which shops' number products is above the average? Give me the shop names.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q105_idx105",
    "nl_query_text": "What is the zip code of the address in the city Port Chelsea?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "restaurant_1",
    "raw_model_output": "database_id: restaurant_1"
  },
  {
    "experiment_query_id": "spider_dev_q106_idx106",
    "nl_query_text": "List the title of all cartoon directed by \"Ben Jones\" or \"Brandon Vietti\".",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q107_idx107",
    "nl_query_text": "What are the template ids of any templates used in more than a single document?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q108_idx108",
    "nl_query_text": "What are the names and ranks of the three youngest winners across all matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas provided, I found that the most relevant database for answering the question is"
  },
  {
    "experiment_query_id": "spider_dev_q109_idx109",
    "nl_query_text": "Give the nationality that is most common across all people.",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine the most relevant database for answering the question \"Give the nationality that is most common across all"
  },
  {
    "experiment_query_id": "spider_dev_q110_idx110",
    "nl_query_text": "What is the name and capacity for the stadium with the highest average attendance?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: stadium"
  },
  {
    "experiment_query_id": "spider_dev_q111_idx111",
    "nl_query_text": "What is the total surface area of the continents Asia and Europe?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q112_idx112",
    "nl_query_text": "What are the first names and country codes for players who won both the WTA Championships and the Australian Open?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q113_idx113",
    "nl_query_text": "Give me the id, role and email of the professionals who did not perform any treatment on dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 3"
  },
  {
    "experiment_query_id": "spider_dev_q114_idx114",
    "nl_query_text": "What is the average life expectancy in African countries that are republics?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q115_idx115",
    "nl_query_text": "What is the name of country that has the shortest life expectancy in Asia?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q116_idx116",
    "nl_query_text": "What are the ids of the students who do not own cats as pets?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code you provided. However, I can help you determine which database is most"
  },
  {
    "experiment_query_id": "spider_dev_q117_idx117",
    "nl_query_text": "What is the semester which most student registered in? Show both the name and the id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q118_idx118",
    "nl_query_text": "What is the name of the conductor who has worked the greatest number of years?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q119_idx119",
    "nl_query_text": "List each language and the number of TV Channels using it.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "database_id: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q120_idx120",
    "nl_query_text": "Return the type code of the template type with the description \"Book\".",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q121_idx121",
    "nl_query_text": "How many singers are from each country?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q122_idx122",
    "nl_query_text": "What are the birth years and citizenships of the singers?",
    "true_db_id": "singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q123_idx123",
    "nl_query_text": "List the Episode of all  TV series showed on TV Channel with series name \"Sky Radio\".",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q124_idx124",
    "nl_query_text": "What is the code of the country with the most players?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q125_idx125",
    "nl_query_text": "Which year has most number of concerts?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q126_idx126",
    "nl_query_text": "Return the lowest version number, along with its corresponding template type code.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q127_idx127",
    "nl_query_text": "What is the number of continents?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q128_idx128",
    "nl_query_text": "What is the charge amount of the most expensive charge type?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"What is the charge amount of the most expensive charge type?\", we need to"
  },
  {
    "experiment_query_id": "spider_dev_q129_idx129",
    "nl_query_text": "What are the different models for the cards produced after 1980?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: card_1"
  },
  {
    "experiment_query_id": "spider_dev_q130_idx130",
    "nl_query_text": "List top 3 highest Rating  TV series. List the TV series's Episode and Rating.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q131_idx131",
    "nl_query_text": "What are the names and areas of countries with the top 5 largest area?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country, area, country_name, capital, population, density"
  },
  {
    "experiment_query_id": "spider_dev_q132_idx132",
    "nl_query_text": "How many owners temporarily do not have any dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"How many owners temporarily do"
  },
  {
    "experiment_query_id": "spider_dev_q133_idx133",
    "nl_query_text": "Compute the average age of all the dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or access external links. However, I can provide a general solution strategy for"
  },
  {
    "experiment_query_id": "spider_dev_q134_idx134",
    "nl_query_text": "Show names for all stadiums except for stadiums having a concert in year 2014.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q135_idx135",
    "nl_query_text": "What is the TV Channel of TV series with Episode \"A Love of a Lifetime\"? List the TV Channel's series name.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q136_idx136",
    "nl_query_text": "What are the minimum and maximum number of products across all the shops?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q137_idx137",
    "nl_query_text": "what are all the addresses including line 1 and line 2?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "customers_and_addresses",
    "raw_model_output": "database_id: customers_and_addresses"
  },
  {
    "experiment_query_id": "spider_dev_q138_idx138",
    "nl_query_text": "What are the names and ids of all countries with at least one car maker?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q139_idx139",
    "nl_query_text": "Find the average and maximum age for each type of pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "The most relevant database_id is: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q140_idx140",
    "nl_query_text": "What is the sname of every sing that does not have any song?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "The most relevant database_id is: music_4"
  },
  {
    "experiment_query_id": "spider_dev_q141_idx141",
    "nl_query_text": "What are the names of all courses that have some students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "The most relevant database_id is: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q142_idx142",
    "nl_query_text": "Please show the most common citizenship of singers.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q143_idx143",
    "nl_query_text": "List the name of teachers whose hometown is not `` Little Lever Urban District '' .",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q144_idx144",
    "nl_query_text": "How many courses in total are listed?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q145_idx145",
    "nl_query_text": "Find the production code and channel of the most recently aired cartoon .",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q146_idx146",
    "nl_query_text": "Which country does Airline \"JetBlue Airways\" belong to?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_company",
    "raw_model_output": "flight_company"
  },
  {
    "experiment_query_id": "spider_dev_q147_idx147",
    "nl_query_text": "What is the number of the cars with horsepower more than 150?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q148_idx148",
    "nl_query_text": "Show all distinct template type codes for all templates.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q149_idx149",
    "nl_query_text": "List the number of different series names and contents in the TV Channel table.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q150_idx150",
    "nl_query_text": "How many museums were opened after 2013 or before 2008?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q151_idx151",
    "nl_query_text": "Return the template type description of the template type with the code AD.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q152_idx152",
    "nl_query_text": "On average, when were the transcripts printed?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a specific database_id. However, I can provide a general approach"
  },
  {
    "experiment_query_id": "spider_dev_q153_idx153",
    "nl_query_text": "List the record company shared by the most number of orchestras.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q154_idx154",
    "nl_query_text": "List the names of employees and sort in ascending order of age.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q155_idx155",
    "nl_query_text": "How many dog pets are raised by female students?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you determine which database schema is most"
  },
  {
    "experiment_query_id": "spider_dev_q156_idx156",
    "nl_query_text": "What is the number of car models created by the car maker American Motor Company?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q157_idx157",
    "nl_query_text": "Return the names and template ids for documents that contain the letter w in their description.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q158_idx158",
    "nl_query_text": "Which template type code is used by most number of documents?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q159_idx159",
    "nl_query_text": "For model volvo, how many cylinders does the car with the least accelerate have?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't answer to the question you posted because it requires to access the database and run a query"
  },
  {
    "experiment_query_id": "spider_dev_q160_idx160",
    "nl_query_text": "For students who have pets , how many pets does each student have ? list their ids instead of names .",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q161_idx161",
    "nl_query_text": "Which student has enrolled for the most times in any program? List the id, first name, middle name, last name, the number of enrollments and student id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q162_idx162",
    "nl_query_text": "List the area codes in which voters voted both for the contestant 'Tabatha Gehling' and the contestant 'Kelly Clauss'.",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q163_idx163",
    "nl_query_text": "Give the airline with abbreviation 'UAL'.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q164_idx164",
    "nl_query_text": "Describe the section h.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a response that involves executing code. However, I can help you"
  },
  {
    "experiment_query_id": "spider_dev_q165_idx165",
    "nl_query_text": "Find the number of professionals who have ever treated dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 10"
  },
  {
    "experiment_query_id": "spider_dev_q166_idx166",
    "nl_query_text": "How many available features are there in total?",
    "true_db_id": "real_estate_properties",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas provided, I found that the most relevant database for answering the question \"How"
  },
  {
    "experiment_query_id": "spider_dev_q167_idx167",
    "nl_query_text": "What is the number of carsw ith over 6 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"What is the number of"
  },
  {
    "experiment_query_id": "spider_dev_q168_idx168",
    "nl_query_text": "List the name and tonnage ordered by in descending alphaetical order for the names.",
    "true_db_id": "battle_death",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q169_idx169",
    "nl_query_text": "List the names of the dogs of the rarest breed and the treatment dates of them.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q170_idx170",
    "nl_query_text": "How many high schoolers are there?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q171_idx171",
    "nl_query_text": "What is the average grade of students who have friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q172_idx172",
    "nl_query_text": "How much does each charge type costs? List both charge type and amount.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 2"
  },
  {
    "experiment_query_id": "spider_dev_q173_idx173",
    "nl_query_text": "What is the average age for all students who do not own any pets ?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the given question as it requires specific information about"
  },
  {
    "experiment_query_id": "spider_dev_q174_idx174",
    "nl_query_text": "How many conductors are there?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q175_idx175",
    "nl_query_text": "Find the highest rank of losers in all matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_injury",
    "raw_model_output": "game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q176_idx176",
    "nl_query_text": "Show the student IDs and numbers of friends corresponding to each.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "database_id: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q177_idx177",
    "nl_query_text": "What is the model of the car with the smallest amount of horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q178_idx178",
    "nl_query_text": "Which regions speak Dutch or English?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the question \"Which regions speak Dutch or English"
  },
  {
    "experiment_query_id": "spider_dev_q179_idx179",
    "nl_query_text": "What are the citizenships that are shared by singers with a birth year before 1945 and after 1955?",
    "true_db_id": "singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What are the citizenships that"
  },
  {
    "experiment_query_id": "spider_dev_q180_idx180",
    "nl_query_text": "Return the number of flights departing from Aberdeen.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q181_idx181",
    "nl_query_text": "How many concerts are there in year 2014 or 2015?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q182_idx182",
    "nl_query_text": "Return the number of flights.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_4",
    "raw_model_output": "flight_4"
  },
  {
    "experiment_query_id": "spider_dev_q183_idx183",
    "nl_query_text": "What are each owner's first name, last name, and the size of their dog?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you determine which database is most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q184_idx184",
    "nl_query_text": "What is the id and weight of every pet who is older than 1?",
    "true_db_id": "pets_1",
    "predicted_db_id": "body_builder",
    "raw_model_output": "The most relevant database_id is: body_builder"
  },
  {
    "experiment_query_id": "spider_dev_q185_idx185",
    "nl_query_text": "Show the names of high school students and their corresponding number of friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "database_id: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q186_idx186",
    "nl_query_text": "What are the names of all cartoons directed by Ben Jones?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q187_idx187",
    "nl_query_text": "find the names of loser and winner who played in the match with greatest number of minutes.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_injury",
    "raw_model_output": "game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q188_idx188",
    "nl_query_text": "What are the names and ids of every course with less than 2 sections?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q189_idx189",
    "nl_query_text": "Find the average age of losers and winners of all matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "game_1"
  },
  {
    "experiment_query_id": "spider_dev_q190_idx190",
    "nl_query_text": "Show the names of singers that have more than one song.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q191_idx191",
    "nl_query_text": "Which semesters do not have any student enrolled? List the semester name.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q192_idx192",
    "nl_query_text": "What are the names of documents that use templates with the code BK?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q193_idx193",
    "nl_query_text": "Show the name of teachers aged either 32 or 33?",
    "true_db_id": "course_teach",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "database_id: employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q194_idx194",
    "nl_query_text": "List the number of all matches who played in years of 2013 or 2016.",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "match_season"
  },
  {
    "experiment_query_id": "spider_dev_q195_idx195",
    "nl_query_text": "Find the first name of students who have cat or dog pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q196_idx196",
    "nl_query_text": "Show all template type codes and number of templates for each.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q197_idx197",
    "nl_query_text": "What are the titles of the cartoons sorted alphabetically?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q198_idx198",
    "nl_query_text": "What is the name of the conductor who has conducted the most orchestras?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q199_idx199",
    "nl_query_text": "What is the name of the shop that is hiring the largest number of employees?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_1",
    "raw_model_output": "database_id: store_1"
  },
  {
    "experiment_query_id": "spider_dev_q200_idx200",
    "nl_query_text": "Show all the grades of the high schoolers.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q201_idx201",
    "nl_query_text": "What is the number of distinct continents where Chinese is spoken?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "The most relevant database_id is: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q202_idx202",
    "nl_query_text": "What is Weekly Rank of TV series with Episode \"A Love of a Lifetime\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q203_idx203",
    "nl_query_text": "Find the name of the makers that produced some cars in the year of 1970?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q204_idx204",
    "nl_query_text": "What are the contestant numbers and names of the contestants who had at least two votes?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q205_idx205",
    "nl_query_text": "For each shop, return the number of employees working there and the name of the shop.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q206_idx206",
    "nl_query_text": "What are the ids, names, and descriptions for all documents?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q207_idx207",
    "nl_query_text": "What is the most common singer citizenship ?",
    "true_db_id": "singer",
    "predicted_db_id": "singer",
    "raw_model_output": "To find the most common singer citizenship, we need to examine the database schemas provided above and determine which"
  },
  {
    "experiment_query_id": "spider_dev_q208_idx208",
    "nl_query_text": "List the names and birth dates of people in ascending alphabetical order of name.",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"List the names and birth dates"
  },
  {
    "experiment_query_id": "spider_dev_q209_idx209",
    "nl_query_text": "Find the id, last name and cell phone of the professionals who live in the state of Indiana or have performed more than two treatments.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 7"
  },
  {
    "experiment_query_id": "spider_dev_q210_idx210",
    "nl_query_text": "Return the money rank of the player with the greatest earnings.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "The most relevant database_id is: poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q211_idx211",
    "nl_query_text": "Which makers designed more than 3 car models? List full name and the id.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q212_idx212",
    "nl_query_text": "Which continent speaks the most languages?",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q213_idx213",
    "nl_query_text": "What is the average number of injuries caused each time?",
    "true_db_id": "battle_death",
    "predicted_db_id": "game_injury",
    "raw_model_output": "game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q214_idx214",
    "nl_query_text": "What are the different template type codes?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q215_idx215",
    "nl_query_text": "Find the name of the employee who got the highest one time bonus.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q216_idx216",
    "nl_query_text": "What is the id and type code for the template used by the most documents?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q217_idx217",
    "nl_query_text": "Find the major and age of students who do not have a cat pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot answer this question as it appears to be a task that requires examining"
  },
  {
    "experiment_query_id": "spider_dev_q218_idx218",
    "nl_query_text": "Find the name of airports which do not have any flight in and out.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q219_idx219",
    "nl_query_text": "Count the number of dogs that went through a treatment.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q220_idx220",
    "nl_query_text": "How many airlines do we have?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q221_idx221",
    "nl_query_text": "What is the continent name which Anguilla belongs to?",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q222_idx222",
    "nl_query_text": "What is the number of car models that are produced by each maker and what is the id and full name of each maker?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q223_idx223",
    "nl_query_text": "Show me the cost of the most recently performed treatment.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q224_idx224",
    "nl_query_text": "How many TV Channel using language English?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q225_idx225",
    "nl_query_text": "Show the citizenship shared by singers with birth year before 1945 and after 1955.",
    "true_db_id": "singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you find the most relevant database for your question"
  },
  {
    "experiment_query_id": "spider_dev_q226_idx226",
    "nl_query_text": "What are the names and id of courses having at most 2 sections?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q227_idx227",
    "nl_query_text": "What are names of countries with the top 3 largest population?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "database_id: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q228_idx228",
    "nl_query_text": "List the first name and birth date of all players from the country with code USA.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "The most relevant database_id is: wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q229_idx229",
    "nl_query_text": "What are the names of the singers and number of concerts for each person?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q230_idx230",
    "nl_query_text": "Find the states where both owners and professionals live.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q231_idx231",
    "nl_query_text": "What is the name of the visitor who visited both a museum opened before 2009 and a museum opened after 2011?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "museum_visit",
    "raw_model_output": "database_id: museum_visit"
  },
  {
    "experiment_query_id": "spider_dev_q232_idx232",
    "nl_query_text": "What are airport names at City 'Aberdeen'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q233_idx233",
    "nl_query_text": "What are all the makers and models?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q234_idx234",
    "nl_query_text": "What is the maximum number of times that a course shows up in different transcripts and what is that course's enrollment id?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "database_id: student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q235_idx235",
    "nl_query_text": "List document IDs, document names, and document descriptions for all documents.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Control_Systems",
    "raw_model_output": "cre_Doc_Control_Systems"
  },
  {
    "experiment_query_id": "spider_dev_q236_idx236",
    "nl_query_text": "What are the names of high schoolers who have a grade of over 5 and have 2 or more friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "The most relevant database_id is: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q237_idx237",
    "nl_query_text": "What is the number of employees from each city?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q238_idx238",
    "nl_query_text": "List names of conductors in descending order of years of work.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q239_idx239",
    "nl_query_text": "How many 'United Airlines' flights depart from Airport 'AHD'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q240_idx240",
    "nl_query_text": "Which airlines have departing flights from both APG and CVO airports?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q241_idx241",
    "nl_query_text": "How many shops are there in each location?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_1",
    "raw_model_output": "database_id: store_1"
  },
  {
    "experiment_query_id": "spider_dev_q242_idx242",
    "nl_query_text": "What are the names , themes , and number of singers for every concert ?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q243_idx243",
    "nl_query_text": "Of all the contestants who got voted, what is the contestant number and name of the contestant who got least votes?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_1",
    "raw_model_output": "voter_1"
  },
  {
    "experiment_query_id": "spider_dev_q244_idx244",
    "nl_query_text": "What is the date and id of the transcript with at least 2 courses listed?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q245_idx245",
    "nl_query_text": "Give the names of nations that speak both English and French.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or determine the most relevant database from the provided list. However, I can"
  },
  {
    "experiment_query_id": "spider_dev_q246_idx246",
    "nl_query_text": "Count the number of countries in Asia.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q247_idx247",
    "nl_query_text": "Find all airlines that have flights from both airports 'APG' and 'CVO'.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q248_idx248",
    "nl_query_text": "Find the abbreviation and country of the airline that has fewest number of flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q249_idx249",
    "nl_query_text": "Show the hometowns shared by at least two teachers.",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code, but I can help you with your question."
  },
  {
    "experiment_query_id": "spider_dev_q250_idx250",
    "nl_query_text": "What are the details for the paragraph that includes the text 'Korea ' ?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q251_idx251",
    "nl_query_text": "Count the number of likes for each student id.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q252_idx252",
    "nl_query_text": "What are the African countries that have a  population less than any country in Asia?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you determine which database is most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q253_idx253",
    "nl_query_text": "Tell me the number of dogs that have not received any treatment .",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q254_idx254",
    "nl_query_text": "Count the number of countries for which Spanish is the predominantly spoken language.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "The most relevant database_id is: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q255_idx255",
    "nl_query_text": "Which professionals have done at least two types of treatments? List the professional id and cell phone.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "assets_maintenance",
    "raw_model_output": "After examining the database schemas, I would recommend the database_id \"assets_maintenance\" as the most"
  },
  {
    "experiment_query_id": "spider_dev_q256_idx256",
    "nl_query_text": "Which grade has the most high schoolers?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q257_idx257",
    "nl_query_text": "What are the Package Options of the TV Channels whose series names are Sky Radio?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "The most relevant database_id is: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q258_idx258",
    "nl_query_text": "What is the name of each teacher and what course they teach?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 2"
  },
  {
    "experiment_query_id": "spider_dev_q259_idx259",
    "nl_query_text": "What is the language spoken by the largest percentage of people in each country?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 51"
  },
  {
    "experiment_query_id": "spider_dev_q260_idx260",
    "nl_query_text": "What are the names of the winner and loser who played in the longest match?",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "game_1"
  },
  {
    "experiment_query_id": "spider_dev_q261_idx261",
    "nl_query_text": "Count the number of different nationalities.",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q262_idx262",
    "nl_query_text": "Show the name of the conductor that has conducted the most number of orchestras.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q263_idx263",
    "nl_query_text": "How many documents do we have?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"How many documents do we"
  },
  {
    "experiment_query_id": "spider_dev_q264_idx264",
    "nl_query_text": "Show the names of conductors that have conducted more than one orchestras.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q265_idx265",
    "nl_query_text": "Count the number of employees for each city.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "hr_1",
    "raw_model_output": "database_id: hr_1"
  },
  {
    "experiment_query_id": "spider_dev_q266_idx266",
    "nl_query_text": "What are the names of the teachers who teach at least two courses?",
    "true_db_id": "course_teach",
    "predicted_db_id": "college_3",
    "raw_model_output": "The most relevant database_id is: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q267_idx267",
    "nl_query_text": "How many different templates do all document use?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q268_idx268",
    "nl_query_text": "How many total tours were there for each ranking date?",
    "true_db_id": "wta_1",
    "predicted_db_id": "formula_1",
    "raw_model_output": "database_id: formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q269_idx269",
    "nl_query_text": "How is the math course described?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't answer that question as it appears to be a task that requires examining multiple database schemas to"
  },
  {
    "experiment_query_id": "spider_dev_q270_idx270",
    "nl_query_text": "What is the record company used by the greatest number of orchestras?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q271_idx271",
    "nl_query_text": "Show ids of all students who do not have any friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q272_idx272",
    "nl_query_text": "For each semester, what is the name and id of the one with the most students registered?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q273_idx273",
    "nl_query_text": "What is the name and id of the department with the most number of degrees ?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "college_3"
  },
  {
    "experiment_query_id": "spider_dev_q274_idx274",
    "nl_query_text": "which countries' tv channels are not playing any cartoon written by Todd Casey?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q275_idx275",
    "nl_query_text": "What is the document id and name with greatest number of paragraphs?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q276_idx276",
    "nl_query_text": "What are the names of poker players, ordered ascending by the number of final tables they have made?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q277_idx277",
    "nl_query_text": "List the section_name in reversed lexicographical order.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To find the most relevant database for the question \"List the section_name in reversed lexicographical order"
  },
  {
    "experiment_query_id": "spider_dev_q278_idx278",
    "nl_query_text": "What is the accelerate of the car make amc hornet sportabout (sw)?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q279_idx279",
    "nl_query_text": "What is the first, middle, and last name of the first student to register?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q280_idx280",
    "nl_query_text": "How many cartoons did each director create?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q281_idx281",
    "nl_query_text": "What is the average rank for winners in all matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "game_1"
  },
  {
    "experiment_query_id": "spider_dev_q282_idx282",
    "nl_query_text": "Show all document ids and the number of paragraphs in each document. Order by document id.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q283_idx283",
    "nl_query_text": "Find the manager name and district of the shop whose number of products is the largest.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "# Task: Examine all the database schemas provided above and determine which single database is most relevant for"
  },
  {
    "experiment_query_id": "spider_dev_q284_idx284",
    "nl_query_text": "Show names of teachers and the number of courses they teach.",
    "true_db_id": "course_teach",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q285_idx285",
    "nl_query_text": "List the emails of the professionals who live in the state of Hawaii or the state of Wisconsin.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 9."
  },
  {
    "experiment_query_id": "spider_dev_q286_idx286",
    "nl_query_text": "what are the different names of the singers that have sales more than 300000?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q287_idx287",
    "nl_query_text": "What are the descriptions for all the math courses?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "department_1"
  },
  {
    "experiment_query_id": "spider_dev_q288_idx288",
    "nl_query_text": "What are the ids of documents that have 2 or more paragraphs?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q289_idx289",
    "nl_query_text": "What major is every student who does not own a cat as a pet, and also how old are they?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no database schema that contains information about students, their majors, and"
  },
  {
    "experiment_query_id": "spider_dev_q290_idx290",
    "nl_query_text": "What is the average attendance of shows?",
    "true_db_id": "orchestra",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q291_idx291",
    "nl_query_text": "Show all document names using templates with template type code BK.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q292_idx292",
    "nl_query_text": "find the number of distinct country codes of all players.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q293_idx293",
    "nl_query_text": "How many friends does each student have?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q294_idx294",
    "nl_query_text": "Which distinct car models are the produced after 1980?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q295_idx295",
    "nl_query_text": "What are the population and life expectancies in Brazil?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q296_idx296",
    "nl_query_text": "What is the first name and country code of the oldest player?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the question \"What is the first name and"
  },
  {
    "experiment_query_id": "spider_dev_q297_idx297",
    "nl_query_text": "Please show the name of the conductor that has conducted orchestras founded after 2008.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q298_idx298",
    "nl_query_text": "What are flight numbers of flights departing from City \"Aberdeen \"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q299_idx299",
    "nl_query_text": "What are the distinct states and create time of all votes?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q300_idx300",
    "nl_query_text": "Which continent has the most diverse languages?",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q301_idx301",
    "nl_query_text": "How many unique languages are spoken in the world?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q302_idx302",
    "nl_query_text": "What is the description of the department whose name has the substring the computer?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q303_idx303",
    "nl_query_text": "How many car makers are there in france?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I am unable to determine the most relevant database for the given question based on"
  },
  {
    "experiment_query_id": "spider_dev_q304_idx304",
    "nl_query_text": "Find the package choice and series name of the TV channel that has high definition TV.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q305_idx305",
    "nl_query_text": "which countries' tv channels are playing some cartoon written by Todd Casey?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q306_idx306",
    "nl_query_text": "Find the kind of program which most number of students are enrolled in?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "game_1",
    "raw_model_output": "database_id: game_1"
  },
  {
    "experiment_query_id": "spider_dev_q307_idx307",
    "nl_query_text": "What are the country codes of countries where people use languages other than English?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"What are the country codes"
  },
  {
    "experiment_query_id": "spider_dev_q308_idx308",
    "nl_query_text": "What are the first, middle, and last names for everybody enrolled in a Bachelors program?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q309_idx309",
    "nl_query_text": "For a volvo model, how many cylinders does the version with least accelerate have?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a specific database_id. However, I can provide a general approach"
  },
  {
    "experiment_query_id": "spider_dev_q310_idx310",
    "nl_query_text": "What is the phone number of the man with the first name Timmothy and the last name Ward?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "restaurant_1",
    "raw_model_output": "restaurant_1"
  },
  {
    "experiment_query_id": "spider_dev_q311_idx311",
    "nl_query_text": "When did the episode \"A Love of a Lifetime\" air?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q312_idx312",
    "nl_query_text": "What are different nationalities of people and the corresponding number of people from each nation?",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: immigration_1"
  },
  {
    "experiment_query_id": "spider_dev_q313_idx313",
    "nl_query_text": "How much surface area do the countires in the Carribean cover together?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q314_idx314",
    "nl_query_text": "List the Episode of all TV series sorted by rating.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q315_idx315",
    "nl_query_text": "For the countries founded before 1930, what is the total number of distinct official languages?",
    "true_db_id": "world_1",
    "predicted_db_id": "election",
    "raw_model_output": "database_id: election"
  },
  {
    "experiment_query_id": "spider_dev_q316_idx316",
    "nl_query_text": "What are the names and birth dates of people, ordered by their names in alphabetical order?",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas, I found that the most relevant database for answering the question is \"people"
  },
  {
    "experiment_query_id": "spider_dev_q317_idx317",
    "nl_query_text": "What is the total number of languages used in Aruba?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q318_idx318",
    "nl_query_text": "What is the hometown of the youngest teacher?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't answer that question because it requires an analysis of the database schemas, but I don't"
  },
  {
    "experiment_query_id": "spider_dev_q319_idx319",
    "nl_query_text": "Return the names of the contestants whose names contain the substring 'Al' .",
    "true_db_id": "voter_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'll examine all the database schemas provided above and determine which single database is most relevant for answering the"
  },
  {
    "experiment_query_id": "spider_dev_q320_idx320",
    "nl_query_text": "Return the number of flights arriving in Aberdeen.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q321_idx321",
    "nl_query_text": "What is the most commmon hometowns for teachers?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas, I found that the most relevant database for answering the question \"What"
  },
  {
    "experiment_query_id": "spider_dev_q322_idx322",
    "nl_query_text": "Find the program which most number of students are enrolled in. List both the id and the summary.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "course_teach",
    "raw_model_output": "The most relevant database_id is: course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q323_idx323",
    "nl_query_text": "What are the first names of all players, and their total ranking points?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "database_id: soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q324_idx324",
    "nl_query_text": "How many different loser names are there?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "After examining the provided database schemas, I would recommend database_id: soccer_2 as the most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q325_idx325",
    "nl_query_text": "For each hometown, how many teachers are there?",
    "true_db_id": "course_teach",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "database_id: student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q326_idx326",
    "nl_query_text": "What model has the most different versions?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine which database is most relevant for answering the question \"What model has the most different versions?\","
  },
  {
    "experiment_query_id": "spider_dev_q327_idx327",
    "nl_query_text": "How many United Airlines flights go to City 'Aberdeen'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 2"
  },
  {
    "experiment_query_id": "spider_dev_q328_idx328",
    "nl_query_text": "find the code of the country where has the greatest number of players.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "database_id: soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q329_idx329",
    "nl_query_text": "What's the name of the course with most number of enrollments?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q330_idx330",
    "nl_query_text": "What are each professional's first name and description of the treatment they have performed?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "medicine_enzyme_interaction",
    "raw_model_output": "database_id: medicine_enzyme_interaction"
  },
  {
    "experiment_query_id": "spider_dev_q331_idx331",
    "nl_query_text": "How many continents are there?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q332_idx332",
    "nl_query_text": "Find the number of players for each hand type.",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the question \"Find the number of players for"
  },
  {
    "experiment_query_id": "spider_dev_q333_idx333",
    "nl_query_text": "What are the names of airports in Aberdeen?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q334_idx334",
    "nl_query_text": "How many people live in countries that do not speak English?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q335_idx335",
    "nl_query_text": "What is the series name of the TV Channel that shows the cartoon \"The Rise of the Blue Beetle\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q336_idx336",
    "nl_query_text": "List the names of poker players ordered by the final tables made in ascending order.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q337_idx337",
    "nl_query_text": "How many official languages are spoken in Afghanistan?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q338_idx338",
    "nl_query_text": "What are the names of the sections in reverse alphabetical order?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for answering the question \"What are the names of"
  },
  {
    "experiment_query_id": "spider_dev_q339_idx339",
    "nl_query_text": "Return the average earnings across all poker players.",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q340_idx340",
    "nl_query_text": "What are the names, countries, and ages for every singer in descending order of age?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q341_idx341",
    "nl_query_text": "What is the id of the pet owned by the student whose last name is 'Smith'?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I am unable to provide a database id."
  },
  {
    "experiment_query_id": "spider_dev_q342_idx342",
    "nl_query_text": "Count the number of flights departing from 'APG'.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q343_idx343",
    "nl_query_text": "What are the names of the high schoolers and how many friends does each have?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q344_idx344",
    "nl_query_text": "What is the minimum weight of the car with 8 cylinders produced in 1974 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q345_idx345",
    "nl_query_text": "Return the first name, last name and email of the owners living in a state whose name contains the substring 'North'.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "customers_and_addresses",
    "raw_model_output": "database_id: customers_and_addresses"
  },
  {
    "experiment_query_id": "spider_dev_q346_idx346",
    "nl_query_text": "How many continents speak Chinese?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q347_idx347",
    "nl_query_text": "What are 3 most highly rated episodes in the TV series table and what were those ratings?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q348_idx348",
    "nl_query_text": "Show ids for all templates that are used by more than one document.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q349_idx349",
    "nl_query_text": "How many transcripts are released?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q350_idx350",
    "nl_query_text": "Which model of the car has the minimum horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q351_idx351",
    "nl_query_text": "What are the arriving date and the departing date of the dogs who have gone through a treatment?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q352_idx352",
    "nl_query_text": "What is total bonus given in all evaluations?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q353_idx353",
    "nl_query_text": "What is the average, minimum, and maximum age of all singers from France?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "singer",
    "raw_model_output": "The most relevant database_id is: singer"
  },
  {
    "experiment_query_id": "spider_dev_q354_idx354",
    "nl_query_text": "Which shop has the most employees? Give me the shop name.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q355_idx355",
    "nl_query_text": "What are the names of conductors who have conducted at more than one orchestra?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q356_idx356",
    "nl_query_text": "Which professionals live in a city containing the substring 'West'? List his or her role, street, city and state.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas provided, I found that the most relevant database for answering the question is"
  },
  {
    "experiment_query_id": "spider_dev_q357_idx357",
    "nl_query_text": "What is the horsepower of the car with the largest accelerate?",
    "true_db_id": "car_1",
    "predicted_db_id": "wine_1",
    "raw_model_output": "wine_1"
  },
  {
    "experiment_query_id": "spider_dev_q358_idx358",
    "nl_query_text": "What is the TV Channel that shows the cartoon \"The Rise of the Blue Beetle!\"? List the TV Channel's series name.",
    "true_db_id": "tvshow",
    "predicted_db_id": "error",
    "raw_model_output": ""
  },
  {
    "experiment_query_id": "spider_dev_q359_idx359",
    "nl_query_text": "Find the average weight for each pet type.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q360_idx360",
    "nl_query_text": "What is the official language spoken in the country whose head of state is Beatrix?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"What is the official language"
  },
  {
    "experiment_query_id": "spider_dev_q361_idx361",
    "nl_query_text": "What is the total number of airlines?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q362_idx362",
    "nl_query_text": "List the contestant numbers and names, ordered by contestant name descending.",
    "true_db_id": "voter_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine the most relevant database, we need to find the database that contains the information about contestants and"
  },
  {
    "experiment_query_id": "spider_dev_q363_idx363",
    "nl_query_text": "What is the name of the country with the most car makers?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q364_idx364",
    "nl_query_text": "What is the maximum accelerate for all the different cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "bike_1",
    "raw_model_output": "bike_1"
  },
  {
    "experiment_query_id": "spider_dev_q365_idx365",
    "nl_query_text": "What is the air date of TV series with Episode \"A Love of a Lifetime\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q366_idx366",
    "nl_query_text": "Give the name, population, and head of state for the country that has the largest area.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q367_idx367",
    "nl_query_text": "find the id of tv channels that do not play any cartoon directed by Ben Jones.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q368_idx368",
    "nl_query_text": "What is the total population of Gelderland district?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What is the total population of"
  },
  {
    "experiment_query_id": "spider_dev_q369_idx369",
    "nl_query_text": "What are flight numbers of flights arriving at Airport \"APG\"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_1"
  },
  {
    "experiment_query_id": "spider_dev_q370_idx370",
    "nl_query_text": "Return the names of conductors that do not have the nationality \"USA\".",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q371_idx371",
    "nl_query_text": "What are the names of nations speak both English and French?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q372_idx372",
    "nl_query_text": "Give the name of the student with the most likes.",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I can't answer your question. I need more information about the question."
  },
  {
    "experiment_query_id": "spider_dev_q373_idx373",
    "nl_query_text": "Which languages are spoken by only one country in republic governments?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q374_idx374",
    "nl_query_text": "How many states are there?",
    "true_db_id": "voter_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q375_idx375",
    "nl_query_text": "Which city has most number of departing flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q376_idx376",
    "nl_query_text": "What are the first names of every student who has a cat or dog as a pet?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or determine which database is most relevant without more context."
  },
  {
    "experiment_query_id": "spider_dev_q377_idx377",
    "nl_query_text": "List the names of teachers in ascending order of age.",
    "true_db_id": "course_teach",
    "predicted_db_id": "student_1",
    "raw_model_output": "The most relevant database_id is: student_1"
  },
  {
    "experiment_query_id": "spider_dev_q378_idx378",
    "nl_query_text": "How many degrees does the engineering department offer?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q379_idx379",
    "nl_query_text": "Return the owner id, first name and last name of the owner who has the most dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"Return the owner id, first"
  },
  {
    "experiment_query_id": "spider_dev_q380_idx380",
    "nl_query_text": "Find the minimum grade of students who have no friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "database_id: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q381_idx381",
    "nl_query_text": "How many flights does airline 'JetBlue Airways' have?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q382_idx382",
    "nl_query_text": "How many courses are there?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "college_3"
  },
  {
    "experiment_query_id": "spider_dev_q383_idx383",
    "nl_query_text": "Give the language that is spoken in the most countries.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q384_idx384",
    "nl_query_text": "Find the number of owners who do not own any dogs at this moment.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q385_idx385",
    "nl_query_text": "What is the birth date of the poker player with the lowest earnings?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q386_idx386",
    "nl_query_text": "Show different citizenship of singers and the number of singers of each citizenship.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q387_idx387",
    "nl_query_text": "Which cities are in European countries where English is not the official language?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or determine the most relevant database_id based on the provided question."
  },
  {
    "experiment_query_id": "spider_dev_q388_idx388",
    "nl_query_text": "What is the smallest weight of the car produced with 8 cylinders on 1974 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q389_idx389",
    "nl_query_text": "Which professionals live in the state of Indiana or have done treatment on more than 2 treatments? List his or her id, last name and cell phone.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "medicine_enzyme_interaction",
    "raw_model_output": "database_id: medicine_enzyme_interaction"
  },
  {
    "experiment_query_id": "spider_dev_q390_idx390",
    "nl_query_text": "How many airlines are from USA?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q391_idx391",
    "nl_query_text": "Find the name and rank of the 3 youngest winners across all matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't execute the code provided. However, based on the question \"Find the name and rank"
  },
  {
    "experiment_query_id": "spider_dev_q392_idx392",
    "nl_query_text": "What are the different government forms and what is the total population of each for government forms that have an average life expectancy greater than 72?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q393_idx393",
    "nl_query_text": "What are the countries that are not playing cartoons written by Todd Casey?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q394_idx394",
    "nl_query_text": "Which professional did not operate any treatment on dogs? List the professional's id, role and email.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 7"
  },
  {
    "experiment_query_id": "spider_dev_q395_idx395",
    "nl_query_text": "Which grades have 4 or more high schoolers?",
    "true_db_id": "network_1",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q396_idx396",
    "nl_query_text": "List the final tables made and the best finishes of poker players.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q397_idx397",
    "nl_query_text": "What are the names of poker players?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q398_idx398",
    "nl_query_text": "Find the first name and country code of the oldest player.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q399_idx399",
    "nl_query_text": "What is the first, middle, and last name of the earliest school graduate?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question, I need to find the database that contains the information about school graduates."
  },
  {
    "experiment_query_id": "spider_dev_q400_idx400",
    "nl_query_text": "Return the ids of templates that have the code PP or PPT.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q401_idx401",
    "nl_query_text": "List the name of singers whose citizenship is not \"France\".",
    "true_db_id": "singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q402_idx402",
    "nl_query_text": "Give average earnings of poker players who are taller than 200.",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q403_idx403",
    "nl_query_text": "What are the first names of all players, and their average rankings?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q404_idx404",
    "nl_query_text": "How many players are from each country?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q405_idx405",
    "nl_query_text": "Find the names of the visitors whose membership level is higher than 4, and order the results by the level from high to low.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas, I found that the most relevant database for answering the question is database_id"
  },
  {
    "experiment_query_id": "spider_dev_q406_idx406",
    "nl_query_text": "Find the name and membership level of the visitors whose membership level is higher than 4, and sort by their age from old to young.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas provided, I can see that the question requires information about visitors and their membership"
  },
  {
    "experiment_query_id": "spider_dev_q407_idx407",
    "nl_query_text": "Find the number of pets for each student who has any pet and student id.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "pet_store."
  },
  {
    "experiment_query_id": "spider_dev_q408_idx408",
    "nl_query_text": "What is the count of singers?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q409_idx409",
    "nl_query_text": "What are the create dates, states, and phone numbers of the votes that were for the contestant named 'Tabatha Gehling'?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_1",
    "raw_model_output": "voter_1"
  },
  {
    "experiment_query_id": "spider_dev_q410_idx410",
    "nl_query_text": "How many different departments offer degrees?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q411_idx411",
    "nl_query_text": "Give the airport code and airport name corresonding to the city Anthony.",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport_id = airports.airport_code"
  },
  {
    "experiment_query_id": "spider_dev_q412_idx412",
    "nl_query_text": "What are the countries that have greater surface area than any country in Europe?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q413_idx413",
    "nl_query_text": "What is the earliest date of a transcript release, and what details can you tell me?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q414_idx414",
    "nl_query_text": "List the last name of the owner owning the youngest dog.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q415_idx415",
    "nl_query_text": "Show the number of high schoolers for each grade.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q416_idx416",
    "nl_query_text": "What are the names of high schoolers who have 3 or more friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q417_idx417",
    "nl_query_text": "What is the full name of each car maker, along with its id and how many models it produces?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q418_idx418",
    "nl_query_text": "What are the ids and names of each document, as well as the number of paragraphs in each?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you determine which database is most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q419_idx419",
    "nl_query_text": "What is the department description for the one whose name has the word computer?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q420_idx420",
    "nl_query_text": "How many ships ended up being 'Captured'?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q421_idx421",
    "nl_query_text": "How many different series and contents are listed in the TV Channel table?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q422_idx422",
    "nl_query_text": "Find the average number of staff working for the museums that were open before 2009.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 16"
  },
  {
    "experiment_query_id": "spider_dev_q423_idx423",
    "nl_query_text": "What is the abbreviation of the airilne has the fewest flights and what country is it in?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q424_idx424",
    "nl_query_text": "What is the name and directors of all the cartoons that are ordered by air date?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q425_idx425",
    "nl_query_text": "Give the flight numbers of flights arriving in Aberdeen.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q426_idx426",
    "nl_query_text": "What is the maximum mpg of the cars that had 8 cylinders or that were produced before 1980 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "phone_1",
    "raw_model_output": "phone_1"
  },
  {
    "experiment_query_id": "spider_dev_q427_idx427",
    "nl_query_text": "Show the stadium name and the number of concerts in each stadium.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 106"
  },
  {
    "experiment_query_id": "spider_dev_q428_idx428",
    "nl_query_text": "Show all countries and the number of singers in each country.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q429_idx429",
    "nl_query_text": "Which abbreviation corresponds to Jetblue Airways?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q430_idx430",
    "nl_query_text": "Give the mean life expectancy of countries in which English is not the official language.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q431_idx431",
    "nl_query_text": "What is the document id with 1 to 2 paragraphs?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id from the provided information."
  },
  {
    "experiment_query_id": "spider_dev_q432_idx432",
    "nl_query_text": "How many departments offer any degree?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_1",
    "raw_model_output": "database_id: college_1"
  },
  {
    "experiment_query_id": "spider_dev_q433_idx433",
    "nl_query_text": "What are the codes of the countries that do not speak English and whose government forms are not Republic?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q434_idx434",
    "nl_query_text": "What language is predominantly spoken in Aruba?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q435_idx435",
    "nl_query_text": "how many cars were produced in 1980?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q436_idx436",
    "nl_query_text": "Show the years in which orchestras that have given more than one performance are founded.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q437_idx437",
    "nl_query_text": "Give the ids of documents that have between one and two paragraphs.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id for the question \"Give the ids of documents"
  },
  {
    "experiment_query_id": "spider_dev_q438_idx438",
    "nl_query_text": "How many employees are there?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q439_idx439",
    "nl_query_text": "In which years cars were produced weighing no less than 3000 and no more than 4000 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q440_idx440",
    "nl_query_text": "What is name of the country that speaks the largest number of languages?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q441_idx441",
    "nl_query_text": "Show all template type codes and descriptions.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "document_management",
    "raw_model_output": "document_management"
  },
  {
    "experiment_query_id": "spider_dev_q442_idx442",
    "nl_query_text": "Count the number of conductors.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q443_idx443",
    "nl_query_text": "What is the name of the semester with no students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q444_idx444",
    "nl_query_text": "What are the names and descriptions for all the sections?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "document_management",
    "raw_model_output": "The most relevant database_id is: document_management"
  },
  {
    "experiment_query_id": "spider_dev_q445_idx445",
    "nl_query_text": "What are the names of the singers whose birth years are either 1948 or 1949?",
    "true_db_id": "singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q446_idx446",
    "nl_query_text": "How many dogs have an age below the average?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine which database is most relevant for answering the question \"How many dogs have an age below the"
  },
  {
    "experiment_query_id": "spider_dev_q447_idx447",
    "nl_query_text": "How many different store locations are there?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_product",
    "raw_model_output": "database_id: store_product"
  },
  {
    "experiment_query_id": "spider_dev_q448_idx448",
    "nl_query_text": "Find the number of visitors who did not visit any museum opened after 2010.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I am unable to determine the most relevant database for the given question based on"
  },
  {
    "experiment_query_id": "spider_dev_q449_idx449",
    "nl_query_text": "List the name, date and result of each battle.",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q450_idx450",
    "nl_query_text": "How much does the most expensive charge type costs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I can't assist with that request."
  },
  {
    "experiment_query_id": "spider_dev_q451_idx451",
    "nl_query_text": "List all song names by singers above the average age.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q452_idx452",
    "nl_query_text": "Count the number of poker players.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q453_idx453",
    "nl_query_text": "For each singer name, what is the total sales for their songs?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q454_idx454",
    "nl_query_text": "When is the first transcript released? List the date and details.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q455_idx455",
    "nl_query_text": "How many dogs have not gone through any treatment?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 8"
  },
  {
    "experiment_query_id": "spider_dev_q456_idx456",
    "nl_query_text": "For the cars with 4 cylinders, which model has the largest horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "wine_1",
    "raw_model_output": "wine_1"
  },
  {
    "experiment_query_id": "spider_dev_q457_idx457",
    "nl_query_text": "What are the names of poker players whose earnings is higher than 300000?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q458_idx458",
    "nl_query_text": "How many distinct nationalities are there?",
    "true_db_id": "poker_player",
    "predicted_db_id": "flight_1",
    "raw_model_output": "database_id: flight_1"
  },
  {
    "experiment_query_id": "spider_dev_q459_idx459",
    "nl_query_text": "How many dogs went through any treatments?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q460_idx460",
    "nl_query_text": "What is the first name of the student whose permanent address is different from his or her current one?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"What is the first name of the student whose permanent address is different from his"
  },
  {
    "experiment_query_id": "spider_dev_q461_idx461",
    "nl_query_text": "find id of the tv channels that from the countries where have more than two tv channels.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q462_idx462",
    "nl_query_text": "How many distinct countries do players come from?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q463_idx463",
    "nl_query_text": "Give the flight numbers of flights landing at APG.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q464_idx464",
    "nl_query_text": "Count the number of templates.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q465_idx465",
    "nl_query_text": "What is the code of airport that has fewest number of flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q466_idx466",
    "nl_query_text": "List the first name of all the professionals along with the description of the treatment they have done.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 5"
  },
  {
    "experiment_query_id": "spider_dev_q467_idx467",
    "nl_query_text": "How many countries speak both English and Dutch?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot determine which database is most relevant to the question \"How many countries"
  },
  {
    "experiment_query_id": "spider_dev_q468_idx468",
    "nl_query_text": "How many models does each car maker produce? List maker full name, id and the number.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q469_idx469",
    "nl_query_text": "What are the first names of the students who live in Haiti permanently or have the cell phone number 09700166582 ?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q470_idx470",
    "nl_query_text": "Count the number of paragraphs.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "csu_1",
    "raw_model_output": "database_id: csu_1"
  },
  {
    "experiment_query_id": "spider_dev_q471_idx471",
    "nl_query_text": "What is the average horsepower for all cars produced before 1980 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q472_idx472",
    "nl_query_text": "How many languages are spoken in Aruba?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q473_idx473",
    "nl_query_text": "Find the name and age of the visitor who bought the most tickets at once.",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you analyze the database schemas and determine which one"
  },
  {
    "experiment_query_id": "spider_dev_q474_idx474",
    "nl_query_text": "What are the name, independence year, and surface area of the country with the smallest population?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q475_idx475",
    "nl_query_text": "Give the names of countries with English and French as official languages.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q476_idx476",
    "nl_query_text": "Show all template type codes with less than three templates.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q477_idx477",
    "nl_query_text": "What are the opening year and staff number of the museum named Plaza Museum?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "museum_visit",
    "raw_model_output": "database_id: museum_visit"
  },
  {
    "experiment_query_id": "spider_dev_q478_idx478",
    "nl_query_text": "What is the first name, country code, and birth date of the player with the most winner rank points across all matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q479_idx479",
    "nl_query_text": "Return the grade for the high schooler named Kyle.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "The most relevant database_id is: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q480_idx480",
    "nl_query_text": "What are the names and descriptions of all the sections?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "document_management",
    "raw_model_output": "The most relevant database_id is: document_management"
  },
  {
    "experiment_query_id": "spider_dev_q481_idx481",
    "nl_query_text": "What are the first names and birth dates of players from the USA?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q482_idx482",
    "nl_query_text": "Which model has the most version(make) of cars?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q483_idx483",
    "nl_query_text": "Find the id and cell phone of the professionals who operate two or more types of treatments.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 15"
  },
  {
    "experiment_query_id": "spider_dev_q484_idx484",
    "nl_query_text": "Which airlines have at least 10 flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q485_idx485",
    "nl_query_text": "What are the emails of the professionals living in either the state of Hawaii or the state of Wisconsin?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can provide a general approach to solve this problem."
  },
  {
    "experiment_query_id": "spider_dev_q486_idx486",
    "nl_query_text": "What is the content of TV Channel with serial name \"Sky Radio\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q487_idx487",
    "nl_query_text": "Count the number of matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "database_id: soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q488_idx488",
    "nl_query_text": "What is the name of the different car makers who produced a car in 1970?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q489_idx489",
    "nl_query_text": "What is the last transcript release date?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"What is the last transcript release date?\", we need to examine the database schemas"
  },
  {
    "experiment_query_id": "spider_dev_q490_idx490",
    "nl_query_text": "What are the names of all European countries with at least 3 manufacturers?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q491_idx491",
    "nl_query_text": "Find the number of flights landing in the city of Aberdeen or Abilene.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q492_idx492",
    "nl_query_text": "What are the average ages of losers and winners across matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_injury",
    "raw_model_output": "game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q493_idx493",
    "nl_query_text": "Find the city with the largest population that uses English.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q494_idx494",
    "nl_query_text": "For each citizenship, how many singers are from that country?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q495_idx495",
    "nl_query_text": "What is the ship id and name that caused most total injuries?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "database_id: battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q496_idx496",
    "nl_query_text": "What is the name of the winner with the most rank points who participated in the Australian Open tournament?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "database_id: wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q497_idx497",
    "nl_query_text": "Give the names of poker players who have earnings above 300000.",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q498_idx498",
    "nl_query_text": "List singer names and number of concerts for each singer.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q499_idx499",
    "nl_query_text": "How many paragraphs for the document with name 'Summer Show'?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code you provided. However, I can help you determine which database is most"
  },
  {
    "experiment_query_id": "spider_dev_q500_idx500",
    "nl_query_text": "Show the names of all high schoolers in grade 10.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q501_idx501",
    "nl_query_text": "Which dogs are owned by someone who lives in Virginia? List the owner's first name and the dog's name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 11"
  },
  {
    "experiment_query_id": "spider_dev_q502_idx502",
    "nl_query_text": "Count the number of orchestras that have CD or DVD as their record format.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q503_idx503",
    "nl_query_text": "What are the names of the teachers and the courses they teach in ascending alphabetical order by the name of the teacher?",
    "true_db_id": "course_teach",
    "predicted_db_id": "course_teach",
    "raw_model_output": "The most relevant database_id is: course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q504_idx504",
    "nl_query_text": "What is the total population and average area of countries in the continent of North America whose area is bigger than 3000 ?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q505_idx505",
    "nl_query_text": "What are the name and results of the battles when the bulgarian commander is not 'Boril'",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q506_idx506",
    "nl_query_text": "What are the ids of high school students who do not have friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q507_idx507",
    "nl_query_text": "How many countries exist?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country."
  },
  {
    "experiment_query_id": "spider_dev_q508_idx508",
    "nl_query_text": "List the name of singers in ascending order of net worth.",
    "true_db_id": "singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q509_idx509",
    "nl_query_text": "What are the names of conductors, ordered by age?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q510_idx510",
    "nl_query_text": "Which template type code has most number of templates?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"Which template type code has most number of templates?\", we need to find the"
  },
  {
    "experiment_query_id": "spider_dev_q511_idx511",
    "nl_query_text": "What is the episode for the TV series named \"Sky Radio\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q512_idx512",
    "nl_query_text": "Tell me the owner id and last name of the owner who spent the most on treatments of his or her dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "assets_maintenance",
    "raw_model_output": "database_id: assets_maintenance"
  },
  {
    "experiment_query_id": "spider_dev_q513_idx513",
    "nl_query_text": "Show the ids of high schoolers who have friends and are also liked by someone else.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "The most relevant database_id is: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q514_idx514",
    "nl_query_text": "How many flights do we have?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q515_idx515",
    "nl_query_text": "Find the weight of the youngest dog.",
    "true_db_id": "pets_1",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q516_idx516",
    "nl_query_text": "Which airline has abbreviation 'UAL'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q517_idx517",
    "nl_query_text": "Find the total amount of bonus given in all the evaluations.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q518_idx518",
    "nl_query_text": "What is minimum and maximum share of TV series?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q519_idx519",
    "nl_query_text": "Give the total surface area covered by countries in Asia or Europe.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q520_idx520",
    "nl_query_text": "What are the arriving date and the departing date of all the dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "The most relevant database_id is: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q521_idx521",
    "nl_query_text": "Give the name, year of independence, and surface area of the country that has the lowest population.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q522_idx522",
    "nl_query_text": "Which countries have greater area than that of any country in Europe?",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "database_id: geo"
  },
  {
    "experiment_query_id": "spider_dev_q523_idx523",
    "nl_query_text": "Count the number of templates of the type CV.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q524_idx524",
    "nl_query_text": "What is the model for the car with a weight smaller than the average?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q525_idx525",
    "nl_query_text": "List the names of teachers who have not been arranged to teach courses.",
    "true_db_id": "course_teach",
    "predicted_db_id": "course_teach",
    "raw_model_output": "course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q526_idx526",
    "nl_query_text": "What is the description for the section named h?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you identify the most relevant database schema"
  },
  {
    "experiment_query_id": "spider_dev_q527_idx527",
    "nl_query_text": "Count the number of United Airlines flights arriving in ASY Airport.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q528_idx528",
    "nl_query_text": "What is the average expected life expectancy for countries in the region of Central Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no database_id that can answer the question \"What is the average"
  },
  {
    "experiment_query_id": "spider_dev_q529_idx529",
    "nl_query_text": "List the most common hometown of teachers.",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I am unable to determine which database is most relevant to the question. The"
  },
  {
    "experiment_query_id": "spider_dev_q530_idx530",
    "nl_query_text": "Count the number of documents that use the PPT template type.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q531_idx531",
    "nl_query_text": "What is the airport code of the airport with the most flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q532_idx532",
    "nl_query_text": "What are the maximum and minimum values of area codes?",
    "true_db_id": "voter_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q533_idx533",
    "nl_query_text": "What are the names of the people who teach math courses?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer this question, I will examine all the database schemas provided and determine which single database is most"
  },
  {
    "experiment_query_id": "spider_dev_q534_idx534",
    "nl_query_text": "How many car models were produced by the maker with full name American Motor Company?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q535_idx535",
    "nl_query_text": "Find the number of shops in each location.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_product",
    "raw_model_output": "database_id: store_product"
  },
  {
    "experiment_query_id": "spider_dev_q536_idx536",
    "nl_query_text": "Give the names of countries that are in Europe and have a population equal to 80000.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q537_idx537",
    "nl_query_text": "Return the different document ids along with the number of paragraphs corresponding to each, ordered by id.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Control_Systems",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Control_Systems"
  },
  {
    "experiment_query_id": "spider_dev_q538_idx538",
    "nl_query_text": "Give the number of Jetblue Airways flights.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q539_idx539",
    "nl_query_text": "What are the cities whose population is between 160000 and 900000?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "city_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q540_idx540",
    "nl_query_text": "What are the names of conductors whose nationalities are not \"USA\"?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q541_idx541",
    "nl_query_text": "Return the number of United Airlines flights leaving from AHD Airport.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q542_idx542",
    "nl_query_text": "Which language is spoken by the largest number of countries?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q543_idx543",
    "nl_query_text": "How many countries has more than 2 car makers ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q544_idx544",
    "nl_query_text": "find the package option of the tv channel that do not have any cartoon directed by Ben Jones.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q545_idx545",
    "nl_query_text": "What are flight numbers of Airline \"United Airlines\"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q546_idx546",
    "nl_query_text": "What are airlines that have some flight departing from airport 'AHD'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q547_idx547",
    "nl_query_text": "List the name of singers that do not have any song.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q548_idx548",
    "nl_query_text": "Which owner has paid for the most treatments on his or her dogs? List the owner id and last name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code, but I can help you to determine which database is most relevant for the"
  },
  {
    "experiment_query_id": "spider_dev_q549_idx549",
    "nl_query_text": "What is the first and second line for all addresses?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: customer_and_addresses"
  },
  {
    "experiment_query_id": "spider_dev_q550_idx550",
    "nl_query_text": "What is the id, line 1, and line 2 of the address with the most students?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is not enough information to determine the most relevant database_id for the question"
  },
  {
    "experiment_query_id": "spider_dev_q551_idx551",
    "nl_query_text": "For all of the 4 cylinder cars, which model has the most horsepower?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q552_idx552",
    "nl_query_text": "What are the locations and names of all stations with capacity between 5000 and 10000?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "train_station",
    "raw_model_output": "train_station"
  },
  {
    "experiment_query_id": "spider_dev_q553_idx553",
    "nl_query_text": "How many different forms of governments are there in Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "local_govt_mdm",
    "raw_model_output": "database_id: local_govt_mdm"
  },
  {
    "experiment_query_id": "spider_dev_q554_idx554",
    "nl_query_text": "What is the pixel aspect ratio and country of origin for all TV channels that do not use English?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q555_idx555",
    "nl_query_text": "How many flights land in Aberdeen or Abilene?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q556_idx556",
    "nl_query_text": "Return the average attendance across all shows.",
    "true_db_id": "orchestra",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I am unable to determine which database is most relevant for answering the question."
  },
  {
    "experiment_query_id": "spider_dev_q557_idx557",
    "nl_query_text": "What are the names of all stadiums that did not have a concert in 2014?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q558_idx558",
    "nl_query_text": "Find the number of cartoons directed by each of the listed directors.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q559_idx559",
    "nl_query_text": "Find the model of the car whose weight is below the average weight.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q560_idx560",
    "nl_query_text": "What is the date and id of the transcript with the least number of results?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "database_id: student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q561_idx561",
    "nl_query_text": "Which city has the most frequent destination airport?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q562_idx562",
    "nl_query_text": "How many high schoolers are there in grade 9 or 10?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q563_idx563",
    "nl_query_text": "What are the names of all the countries that became independent after 1950?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q564_idx564",
    "nl_query_text": "What is Kyle's id?",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What is Kyle's id?\""
  },
  {
    "experiment_query_id": "spider_dev_q565_idx565",
    "nl_query_text": "What are the different template type codes, and how many documents use each type?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q566_idx566",
    "nl_query_text": "What are the names of the employees who never received any evaluation?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "hr_1",
    "raw_model_output": "database_id: hr_1"
  },
  {
    "experiment_query_id": "spider_dev_q567_idx567",
    "nl_query_text": "Find the number of concerts happened in the stadium with the highest capacity .",
    "true_db_id": "concert_singer",
    "predicted_db_id": "concert_singer",
    "raw_model_output": "database_id: concert_singer"
  },
  {
    "experiment_query_id": "spider_dev_q568_idx568",
    "nl_query_text": "List the names of all winners who played in both 2013 and 2016.",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question."
  },
  {
    "experiment_query_id": "spider_dev_q569_idx569",
    "nl_query_text": "What is maximum and minimum death toll caused each time?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "database_id: battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q570_idx570",
    "nl_query_text": "How many players are there?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q571_idx571",
    "nl_query_text": "What are the different template type codes, and how many templates correspond to each?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q572_idx572",
    "nl_query_text": "Which unique cities are in Asian countries where Chinese is the official language ?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q573_idx573",
    "nl_query_text": "What are the id and names of the countries which have more than 3 car makers or produce the 'fiat' model?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q574_idx574",
    "nl_query_text": "What is the name and country of origin of every singer who has a song with the word 'Hey' in its title?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q575_idx575",
    "nl_query_text": "Show name, country, age for all singers ordered by age from the oldest to the youngest.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q576_idx576",
    "nl_query_text": "Return the names of all the poker players.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q577_idx577",
    "nl_query_text": "How many people live in Gelderland district?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not determine the most relevant database_id for the question \"How many people live in Gelderland"
  },
  {
    "experiment_query_id": "spider_dev_q578_idx578",
    "nl_query_text": "What is the total surface area of the countries in the Caribbean region?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q579_idx579",
    "nl_query_text": "Return the names of the 3 most populated countries.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q580_idx580",
    "nl_query_text": "Find the first name of students who have both cat and dog pets .",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I can't assist with that request. This task requires examining all the database"
  },
  {
    "experiment_query_id": "spider_dev_q581_idx581",
    "nl_query_text": "What is the language that is used by the largest number of Asian nations?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the question \"What is the language that is"
  },
  {
    "experiment_query_id": "spider_dev_q582_idx582",
    "nl_query_text": "What are all distinct countries where singers above age 20 are from?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "singer",
    "raw_model_output": "The most relevant database_id is: singer"
  },
  {
    "experiment_query_id": "spider_dev_q583_idx583",
    "nl_query_text": "Which airlines have a flight with source airport AHD?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q584_idx584",
    "nl_query_text": "Find the names of employees who never won any award in the evaluation.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q585_idx585",
    "nl_query_text": "Return the country codes for countries that do not speak English.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q586_idx586",
    "nl_query_text": "Find the number of distinct name of losers.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_injury",
    "raw_model_output": "database_id: game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q587_idx587",
    "nl_query_text": "Find the number of cities in each district whose population is greater than the average population of cities?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot determine which database is most relevant to the question. I need more"
  },
  {
    "experiment_query_id": "spider_dev_q588_idx588",
    "nl_query_text": "How many friends does the high school student Kyle have?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "The most relevant database_id is: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q589_idx589",
    "nl_query_text": "What are the manager name and district of the shop that sells the largest number of products?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q590_idx590",
    "nl_query_text": "List all airline names and their abbreviations in \"USA\".",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q591_idx591",
    "nl_query_text": "Sort all the shops by number products in descending order, and return the name, location and district of each shop.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q592_idx592",
    "nl_query_text": "How many templates do we have?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "document_management",
    "raw_model_output": "document_management"
  },
  {
    "experiment_query_id": "spider_dev_q593_idx593",
    "nl_query_text": "List the name and date the battle that has lost the ship named 'Lettice' and the ship named 'HMS Atalanta'",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "database_id: battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q594_idx594",
    "nl_query_text": "What are the different models wthat are lighter than 3500 but were not built by the Ford Motor Company?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q595_idx595",
    "nl_query_text": "How many people are there of each nationality?",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: immigration_1"
  },
  {
    "experiment_query_id": "spider_dev_q596_idx596",
    "nl_query_text": "What is the average age of the dogs who have gone through any treatments?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q597_idx597",
    "nl_query_text": "What is the zip code for Port Chelsea?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot answer that question. I need more information to determine which database is"
  },
  {
    "experiment_query_id": "spider_dev_q598_idx598",
    "nl_query_text": "Which region is the city Kabul located in?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q599_idx599",
    "nl_query_text": "What is the series name and country of all TV channels that are playing cartoons directed by Ben Jones and cartoons directed by Michael Chang?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q600_idx600",
    "nl_query_text": "How many different degrees are offered?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "The most relevant database_id is: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q601_idx601",
    "nl_query_text": "How many contestants did not get voted?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q602_idx602",
    "nl_query_text": "Show different hometown of teachers and the number of teachers from each hometown.",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: school_1"
  },
  {
    "experiment_query_id": "spider_dev_q603_idx603",
    "nl_query_text": "List the names of orchestras that have no performance.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q604_idx604",
    "nl_query_text": "What are the names of nations where both English and French are official languages?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q605_idx605",
    "nl_query_text": "What is the feature type name of feature AirCon?",
    "true_db_id": "real_estate_properties",
    "predicted_db_id": "real_estate_properties",
    "raw_model_output": "real_estate_properties"
  },
  {
    "experiment_query_id": "spider_dev_q606_idx606",
    "nl_query_text": "How many flights depart from City Aberdeen?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q607_idx607",
    "nl_query_text": "How many different types of pet are there?",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q608_idx608",
    "nl_query_text": "find the number of players for each country.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q609_idx609",
    "nl_query_text": "Find the codes of countries that have more than 50 players.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "The most relevant database_id is: soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q610_idx610",
    "nl_query_text": "What are the names of all high schoolers in grade 10?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q611_idx611",
    "nl_query_text": "What is the lowest grade of students who do not have any friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you identify the most relevant database."
  },
  {
    "experiment_query_id": "spider_dev_q612_idx612",
    "nl_query_text": "Return the names and surface areas of the 5 largest countries.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q613_idx613",
    "nl_query_text": "How many cities in each district have a population that is above the average population across all cities?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you identify the most relevant database for"
  },
  {
    "experiment_query_id": "spider_dev_q614_idx614",
    "nl_query_text": "Show distinct names of singers that have songs with sales more than 300000.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q615_idx615",
    "nl_query_text": "What are the ids of all tv channels that have more than 2 TV channels?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q616_idx616",
    "nl_query_text": "What are all the course names of the courses which ever have students enrolled in?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "The most relevant database_id is: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q617_idx617",
    "nl_query_text": "What are the names of students who have 2 or more likes?",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code provided. However, I can help you determine which database is most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q618_idx618",
    "nl_query_text": "For each continent, list its id, name, and how many countries it has?",
    "true_db_id": "car_1",
    "predicted_db_id": "geo",
    "raw_model_output": "database_id: geo"
  },
  {
    "experiment_query_id": "spider_dev_q619_idx619",
    "nl_query_text": "What is the total population and maximum GNP in Asia?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q620_idx620",
    "nl_query_text": "How many professionals have performed any treatment to dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 3"
  },
  {
    "experiment_query_id": "spider_dev_q621_idx621",
    "nl_query_text": "Find the total number of matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "game_1"
  },
  {
    "experiment_query_id": "spider_dev_q622_idx622",
    "nl_query_text": "Find the name and rank points of the winner who won the most times.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "database_id: soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q623_idx623",
    "nl_query_text": "What are the names of the dogs for which the owner has not spend more than 1000 for treatment ?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "body_builder",
    "raw_model_output": "database_id: body_builder"
  },
  {
    "experiment_query_id": "spider_dev_q624_idx624",
    "nl_query_text": "What is the count of the car models produced in the United States?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q625_idx625",
    "nl_query_text": "How many TV Channels use the English language?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q626_idx626",
    "nl_query_text": "What is the official language used in the country the name of whose head of state is Beatrix.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q627_idx627",
    "nl_query_text": "Return the template type code of the template that is used by a document named Data base.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q628_idx628",
    "nl_query_text": "What are the package options of all tv channels that are not playing any cartoons directed by Ben Jones?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q629_idx629",
    "nl_query_text": "How many people live in Asia, and what is the largest GNP among them?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q630_idx630",
    "nl_query_text": "What are the number of votes from state 'NY' or 'CA'?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q631_idx631",
    "nl_query_text": "Who owns the youngest dog? Give me his or her last name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 2"
  },
  {
    "experiment_query_id": "spider_dev_q632_idx632",
    "nl_query_text": "Show all template type codes that are not used by any document.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q633_idx633",
    "nl_query_text": "What is the average miles per gallon(mpg) of the cars with 4 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "phone_1",
    "raw_model_output": "phone_1"
  },
  {
    "experiment_query_id": "spider_dev_q634_idx634",
    "nl_query_text": "What are  the different countries with singers above age 20?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q635_idx635",
    "nl_query_text": "How many different winners both participated in the WTA Championships and were left handed?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q636_idx636",
    "nl_query_text": "What is the title of all the cartools that are on the TV Channel with the series name \"Sky Radio\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q637_idx637",
    "nl_query_text": "Which shops run with no employees? Find the shop names",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_1",
    "raw_model_output": "database_id: store_1"
  },
  {
    "experiment_query_id": "spider_dev_q638_idx638",
    "nl_query_text": "What is the number of makers of care in France?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q639_idx639",
    "nl_query_text": "What is the name of the breed with the most dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q640_idx640",
    "nl_query_text": "What are the dog name, age and weight of the dogs that were abandoned? Note that 1 stands for yes, and 0 stands for no in the tables.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q641_idx641",
    "nl_query_text": "What are the names and ids of all makers with more than 3 models?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q642_idx642",
    "nl_query_text": "How many countries have a republic as their form of government?",
    "true_db_id": "world_1",
    "predicted_db_id": "hr_1",
    "raw_model_output": "database_id: hr_1"
  },
  {
    "experiment_query_id": "spider_dev_q643_idx643",
    "nl_query_text": "How many flights depart from City 'Aberdeen' and have destination City 'Ashley'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "flights"
  },
  {
    "experiment_query_id": "spider_dev_q644_idx644",
    "nl_query_text": "What are the names of students who have no friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "database_id: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q645_idx645",
    "nl_query_text": "What is last date created of votes from the state 'CA'?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q646_idx646",
    "nl_query_text": "Find the first name and country code of the player who did the most number of tours.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q647_idx647",
    "nl_query_text": "What are each owner's first name and their dogs's name?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q648_idx648",
    "nl_query_text": "What is the produdction code and channel of the most recent cartoon ?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q649_idx649",
    "nl_query_text": "Count the number of documents.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Control_Systems",
    "raw_model_output": "cre_Doc_Control_Systems"
  },
  {
    "experiment_query_id": "spider_dev_q650_idx650",
    "nl_query_text": "Show the name and the release year of the song by the youngest singer.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q651_idx651",
    "nl_query_text": "Find the id and weight of all pets whose age is older than 1.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "database_id: pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q652_idx652",
    "nl_query_text": "Show name of all students who have some friends and also are liked by someone else.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "The most relevant database_id is: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q653_idx653",
    "nl_query_text": "What is the number of countries with more than 2 car makers ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q654_idx654",
    "nl_query_text": "Which employee received the biggest bonus? Give me the employee name.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q655_idx655",
    "nl_query_text": "Find the series name and country of the tv channel that is playing some cartoons directed by Ben Jones and Michael Chang?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q656_idx656",
    "nl_query_text": "Count the number of United Airlines flights that arrive in Aberdeen.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q657_idx657",
    "nl_query_text": "What are the song titles and singer names?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q658_idx658",
    "nl_query_text": "Which airlines have departures from CVO but not from APG airports?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q659_idx659",
    "nl_query_text": "What are the country code and first name of the players who won in both tourney WTA Championships and Australian Open?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q660_idx660",
    "nl_query_text": "What are the orchestras that do not have any performances?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q661_idx661",
    "nl_query_text": "What are the codes of template types that have fewer than 3 templates?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q662_idx662",
    "nl_query_text": "What are the names of the singers that have more than one songs?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q663_idx663",
    "nl_query_text": "Please show the record formats of orchestras in ascending order of count.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q664_idx664",
    "nl_query_text": "What are the names of high schoolers who have likes, and how many likes does each have?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q665_idx665",
    "nl_query_text": "What is the average horsepower of the cars before 1980?",
    "true_db_id": "car_1",
    "predicted_db_id": "wine_1",
    "raw_model_output": "wine_1"
  },
  {
    "experiment_query_id": "spider_dev_q666_idx666",
    "nl_query_text": "What is the number of distinct languages used around the world?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q667_idx667",
    "nl_query_text": "What airline serves the most flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q668_idx668",
    "nl_query_text": "What are the maximum and minimum share of performances whose type is not \"Live final\".",
    "true_db_id": "orchestra",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot provide a response that includes the database_id of the most relevant database"
  },
  {
    "experiment_query_id": "spider_dev_q669_idx669",
    "nl_query_text": "List the name of the conductor with the most years of work.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q670_idx670",
    "nl_query_text": "Which district has both stores with less than 3000 products and stores with more than 10000 products?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "store_product",
    "raw_model_output": "database_id: store_product"
  },
  {
    "experiment_query_id": "spider_dev_q671_idx671",
    "nl_query_text": "What is the money rank of the poker player with the highest earnings?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q672_idx672",
    "nl_query_text": "What are the distinct template type descriptions for the templates ever used by any document?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q673_idx673",
    "nl_query_text": "Who are enrolled in 2 degree programs in one semester? List the first name, middle name and last name and the id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q674_idx674",
    "nl_query_text": "Give the country codes for countries in which people speak langauges that are not English.",
    "true_db_id": "world_1",
    "predicted_db_id": "movie_1",
    "raw_model_output": "database_id: movie_1"
  },
  {
    "experiment_query_id": "spider_dev_q675_idx675",
    "nl_query_text": "Find the average rank of winners in all matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_injury",
    "raw_model_output": "database_id: game_injury"
  },
  {
    "experiment_query_id": "spider_dev_q676_idx676",
    "nl_query_text": "How many professionals did not operate any treatment on dogs?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q677_idx677",
    "nl_query_text": "What is the template type descriptions for template type code \"AD\".",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q678_idx678",
    "nl_query_text": "What is the weekly rank for the episode \"A Love of a Lifetime\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q679_idx679",
    "nl_query_text": "What are the ids of students who both have friends and are liked?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "The most relevant database_id is: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q680_idx680",
    "nl_query_text": "Return the record companies of orchestras, sorted descending by the years in which they were founded.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q681_idx681",
    "nl_query_text": "For each language, list the number of TV Channels that use it.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "database_id: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q682_idx682",
    "nl_query_text": "Find the first name of the students who permanently live in the country Haiti or have the cell phone number 09700166582 .",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q683_idx683",
    "nl_query_text": "Find the distinct breed type and size type combinations for dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "breed_2"
  },
  {
    "experiment_query_id": "spider_dev_q684_idx684",
    "nl_query_text": "Which language is the most popular on the Asian continent?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database_id from the provided information. The question asks about"
  },
  {
    "experiment_query_id": "spider_dev_q685_idx685",
    "nl_query_text": "Count the number of flights into ATO.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q686_idx686",
    "nl_query_text": "How many official languages does Afghanistan have?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q687_idx687",
    "nl_query_text": "How many flights fly from Aberdeen to Ashley?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport, airports, airlines, flights, routes"
  },
  {
    "experiment_query_id": "spider_dev_q688_idx688",
    "nl_query_text": "Which city has most number of arriving flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q689_idx689",
    "nl_query_text": "Among the cars that do not have the minimum horsepower , what are the make ids and names of all those with less than 4 cylinders ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q690_idx690",
    "nl_query_text": "What is the first name and country code of the player with the most tours?",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "The most relevant database_id is: wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q691_idx691",
    "nl_query_text": "Show different citizenships and the maximum net worth of singers of each citizenship.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q692_idx692",
    "nl_query_text": "What is the template type code for template type description \"Book\".",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q693_idx693",
    "nl_query_text": "What are the paragraph texts for the document with the name 'Customer reviews'?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer this question, I need to examine all the database schemas provided above and determine which single database"
  },
  {
    "experiment_query_id": "spider_dev_q694_idx694",
    "nl_query_text": "Show the names of high schoolers who have likes, and numbers of likes for each.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q695_idx695",
    "nl_query_text": "What grade is Kyle in?",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you determine which database schema is most relevant to"
  },
  {
    "experiment_query_id": "spider_dev_q696_idx696",
    "nl_query_text": "Show the names of high schoolers who have at least 3 friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q697_idx697",
    "nl_query_text": "What is the name and capacity of the stadium with the most concerts after 2013 ?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "# The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q698_idx698",
    "nl_query_text": "Return the number of airlines in the USA.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q699_idx699",
    "nl_query_text": "Find all airlines that have flights from airport 'CVO' but not from 'APG'.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q700_idx700",
    "nl_query_text": "Return the name, location and district of all shops in descending order of number of products.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas, I found that the most relevant database_id for answering the question is"
  },
  {
    "experiment_query_id": "spider_dev_q701_idx701",
    "nl_query_text": "What other details can you tell me about students in reverse alphabetical order?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "database_id: student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q702_idx702",
    "nl_query_text": "Where is the youngest teacher from?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer this question, we need to examine the provided database schemas to determine which one contains information about"
  },
  {
    "experiment_query_id": "spider_dev_q703_idx703",
    "nl_query_text": "Which airline has most number of flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q704_idx704",
    "nl_query_text": "Find the average age of the dogs who went through treatments.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "The most relevant database_id is: machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q705_idx705",
    "nl_query_text": "What is the average age of the visitors whose membership level is not higher than 4?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I can't assist with that. The provided text doesn't contain a question"
  },
  {
    "experiment_query_id": "spider_dev_q706_idx706",
    "nl_query_text": "Return the maximum and minimum shares for performances that do not have the type \"Live final\".",
    "true_db_id": "orchestra",
    "predicted_db_id": "musical",
    "raw_model_output": "The most relevant database_id is: musical"
  },
  {
    "experiment_query_id": "spider_dev_q707_idx707",
    "nl_query_text": "Give me the description of the treatment type whose total cost is the lowest.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't answer this question as it is not a valid question about the provided database schemas. The"
  },
  {
    "experiment_query_id": "spider_dev_q708_idx708",
    "nl_query_text": "How many likes does Kyle have?",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm just an AI, I don't have the ability to see the actual code or data for"
  },
  {
    "experiment_query_id": "spider_dev_q709_idx709",
    "nl_query_text": "Show me all grades that have at least 4 students.",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas, I found that the most relevant database for the question \"Show me"
  },
  {
    "experiment_query_id": "spider_dev_q710_idx710",
    "nl_query_text": "Give me Brazil\u2019s population and life expectancies.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q711_idx711",
    "nl_query_text": "What is the average GNP and total population in all nations whose government is US territory?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q712_idx712",
    "nl_query_text": "Show all paragraph ids and texts for the document with name 'Welcome to NY'.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q713_idx713",
    "nl_query_text": "What are the names of conductors as well as the corresonding orchestras that they have conducted?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q714_idx714",
    "nl_query_text": "What is the country with the most number of TV Channels and how many does it have?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q715_idx715",
    "nl_query_text": "What are the age and hometown of teachers?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot determine which database is most relevant to the question \"What are the"
  },
  {
    "experiment_query_id": "spider_dev_q716_idx716",
    "nl_query_text": "Find the name of the shops that do not hire any employee.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q717_idx717",
    "nl_query_text": "Which first names are used for professionals or owners but are not used as dog names?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the provided database schemas, I determined that the most relevant database for answering the question is \""
  },
  {
    "experiment_query_id": "spider_dev_q718_idx718",
    "nl_query_text": "Count the number of dogs of an age below the average.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q719_idx719",
    "nl_query_text": "What are the id, role, and first name of the professionals who have performed two or more treatments?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "medicine_enzyme_interaction",
    "raw_model_output": "database_id: medicine_enzyme_interaction"
  },
  {
    "experiment_query_id": "spider_dev_q720_idx720",
    "nl_query_text": "How many times at most can a course enrollment result show in different transcripts? Also show the course enrollment id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q721_idx721",
    "nl_query_text": "Count the number of different nationalities of conductors.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q722_idx722",
    "nl_query_text": "How many flights arriving in Aberdeen city?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airlines"
  },
  {
    "experiment_query_id": "spider_dev_q723_idx723",
    "nl_query_text": "List the dog name, age and weight of the dogs who have been abandoned? 1 stands for yes, and 0 stands for no.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q724_idx724",
    "nl_query_text": "What is the abbreviation of Airline \"JetBlue Airways\"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q725_idx725",
    "nl_query_text": "What are the final tables made and best finishes for all poker players?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q726_idx726",
    "nl_query_text": "What are the names of cities in Europe for which English is not the official language?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot provide a specific database_id for the given question as it requires an"
  },
  {
    "experiment_query_id": "spider_dev_q727_idx727",
    "nl_query_text": "List the first and last name of all players who are left / L hand in the order of birth date.",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1."
  },
  {
    "experiment_query_id": "spider_dev_q728_idx728",
    "nl_query_text": "How many likes correspond to each student id?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q729_idx729",
    "nl_query_text": "How many templates have template type code CV?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q730_idx730",
    "nl_query_text": "What the smallest version number and its template type code?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "imdb",
    "raw_model_output": "database_id: imdb"
  },
  {
    "experiment_query_id": "spider_dev_q731_idx731",
    "nl_query_text": "What is the maximum number of final tables made among poker players with earnings less than 200000?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q732_idx732",
    "nl_query_text": "What is the name of the museum that had no visitor yet?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "museum_visit",
    "raw_model_output": "The most relevant database_id is: museum_visit"
  },
  {
    "experiment_query_id": "spider_dev_q733_idx733",
    "nl_query_text": "Show names of teachers and the courses they are arranged to teach.",
    "true_db_id": "course_teach",
    "predicted_db_id": "course_teach",
    "raw_model_output": "database_id: course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q734_idx734",
    "nl_query_text": "Which airports do not have departing or arriving flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q735_idx735",
    "nl_query_text": "What are the names of the teachers ordered by ascending age?",
    "true_db_id": "course_teach",
    "predicted_db_id": "student_1",
    "raw_model_output": "The most relevant database_id is: student_1"
  },
  {
    "experiment_query_id": "spider_dev_q736_idx736",
    "nl_query_text": "What is the number of cars with more than 4 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q737_idx737",
    "nl_query_text": "list all cartoon titles and their directors ordered by their air date",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q738_idx738",
    "nl_query_text": "Return the id and name of the document with the most paragraphs.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q739_idx739",
    "nl_query_text": "What is all the information about hiring?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "database_id: employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q740_idx740",
    "nl_query_text": "What are the earnings of poker players, ordered descending by value?",
    "true_db_id": "poker_player",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q741_idx741",
    "nl_query_text": "Find the number of employees hired in each shop; show the shop name as well.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "shop_membership",
    "raw_model_output": "database_id: shop_membership"
  },
  {
    "experiment_query_id": "spider_dev_q742_idx742",
    "nl_query_text": "Show the names and grades of each high schooler.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q743_idx743",
    "nl_query_text": "Show location and name for all stadiums with a capacity between 5000 and 10000.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't execute the code. However, based on the given information, the most relevant database_id"
  },
  {
    "experiment_query_id": "spider_dev_q744_idx744",
    "nl_query_text": "Which Asian countries have a population that is larger than any country in Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country."
  },
  {
    "experiment_query_id": "spider_dev_q745_idx745",
    "nl_query_text": "What is the total number of singers?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q746_idx746",
    "nl_query_text": "In 1980, how many cars were made?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q747_idx747",
    "nl_query_text": "Give the total population and average surface area corresponding to countries in North America that have a surface area greater than 3000 .",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q748_idx748",
    "nl_query_text": "List each charge type and its amount.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 5"
  },
  {
    "experiment_query_id": "spider_dev_q749_idx749",
    "nl_query_text": "Which owner has paid the largest amount of money in total for their dogs? Show the owner id and zip code.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "body_builder",
    "raw_model_output": "The most relevant database_id is: body_builder"
  },
  {
    "experiment_query_id": "spider_dev_q750_idx750",
    "nl_query_text": "What are the ids for templates that are not used in any documents?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q751_idx751",
    "nl_query_text": "What is the name of the singer with the largest net worth?",
    "true_db_id": "singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q752_idx752",
    "nl_query_text": "What is the maximum and minimum share for the TV series?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q753_idx753",
    "nl_query_text": "Which breed do the most dogs have? Give me the breed name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q754_idx754",
    "nl_query_text": "What is the average weight and year for each year?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: people_inspace"
  },
  {
    "experiment_query_id": "spider_dev_q755_idx755",
    "nl_query_text": "What are the names of people who do not play poker?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q756_idx756",
    "nl_query_text": "What is the average and maximum capacities for all stadiums ?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: stadium_1"
  },
  {
    "experiment_query_id": "spider_dev_q757_idx757",
    "nl_query_text": "List the first and last name of all players in the order of birth date.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "The most relevant database_id is: soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q758_idx758",
    "nl_query_text": "For each stadium, how many concerts play there?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "performance_attendance",
    "raw_model_output": "database_id: performance_attendance"
  },
  {
    "experiment_query_id": "spider_dev_q759_idx759",
    "nl_query_text": "Show names of teachers and the courses they are arranged to teach in ascending alphabetical order of the teacher's name.",
    "true_db_id": "course_teach",
    "predicted_db_id": "course_teach",
    "raw_model_output": "course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q760_idx760",
    "nl_query_text": "Show the stadium name and capacity with most number of concerts in year 2014 or after.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q761_idx761",
    "nl_query_text": "What is the name of the course with the most students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "The most relevant database_id is: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q762_idx762",
    "nl_query_text": "Return the codes of countries that do not speak English and do not have Republics for governments.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q763_idx763",
    "nl_query_text": "Return the name of the airport with code 'AKO'.",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport, columns = [ airport.id ( integer | primary key | values: 1, 2"
  },
  {
    "experiment_query_id": "spider_dev_q764_idx764",
    "nl_query_text": "What are the notes of the death events which has substring 'East'?",
    "true_db_id": "battle_death",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine which database is most relevant for answering the question, we need to examine the schema of each"
  },
  {
    "experiment_query_id": "spider_dev_q765_idx765",
    "nl_query_text": "What are the different first names and ages of the students who do have pets?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or access external resources. However, I can help you determine which database is"
  },
  {
    "experiment_query_id": "spider_dev_q766_idx766",
    "nl_query_text": "What is the mobile phone number of the student named Timmothy Ward ?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "restaurant_1",
    "raw_model_output": "database_id: restaurant_1"
  },
  {
    "experiment_query_id": "spider_dev_q767_idx767",
    "nl_query_text": "What are the countries where either English or Dutch is the official language ?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q768_idx768",
    "nl_query_text": "Which owners live in the state whose name contains the substring 'North'? List his first name, last name and email.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: customer_and_orders"
  },
  {
    "experiment_query_id": "spider_dev_q769_idx769",
    "nl_query_text": "What is the name and capacity for the stadium with highest average attendance?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: stadium"
  },
  {
    "experiment_query_id": "spider_dev_q770_idx770",
    "nl_query_text": "Find the semester when both Master students and Bachelor students got enrolled in.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q771_idx771",
    "nl_query_text": "What are the number of concerts that occurred in the stadium with the largest capacity ?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q772_idx772",
    "nl_query_text": "How many different addresses do the students currently live?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q773_idx773",
    "nl_query_text": "Which department offers the most number of degrees? List department name and id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "department_1"
  },
  {
    "experiment_query_id": "spider_dev_q774_idx774",
    "nl_query_text": "What are the ids of the TV channels that do not have any cartoons directed by Ben Jones?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q775_idx775",
    "nl_query_text": "What is the name of each continent and how many car makers are there in each one?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q776_idx776",
    "nl_query_text": "Show the names of students who have at least 2 likes.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "The most relevant database_id is: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q777_idx777",
    "nl_query_text": "Which of the countries has the most car makers? List the country name.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q778_idx778",
    "nl_query_text": "What is the first name of every student who has a dog but does not have a cat?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a specific database_id."
  },
  {
    "experiment_query_id": "spider_dev_q779_idx779",
    "nl_query_text": "Which models are lighter than 3500 but not built by the 'Ford Motor Company'?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q780_idx780",
    "nl_query_text": "Give the code of the airport with the least flights.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q781_idx781",
    "nl_query_text": "Return the names of poker players sorted by their earnings descending.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q782_idx782",
    "nl_query_text": "Return the id of the document with the fewest paragraphs.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is no database schema provided in the text."
  },
  {
    "experiment_query_id": "spider_dev_q783_idx783",
    "nl_query_text": "What are the names of the countries that are in the continent of Europe and have a population of 80000?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q784_idx784",
    "nl_query_text": "Which language is the most popular in Aruba?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q785_idx785",
    "nl_query_text": "What is the id of the semester that had both Masters and Bachelors students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q786_idx786",
    "nl_query_text": "How many concerts occurred in 2014 or 2015?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q787_idx787",
    "nl_query_text": "What country is Jetblue Airways affiliated with?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_company",
    "raw_model_output": "flight_company"
  },
  {
    "experiment_query_id": "spider_dev_q788_idx788",
    "nl_query_text": "What is the Package Option of TV Channel with serial name \"Sky Radio\"?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "The most relevant database_id is: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q789_idx789",
    "nl_query_text": "What are the countries that have cartoons on TV that were written by Todd Casey?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "The most relevant database_id is: tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q790_idx790",
    "nl_query_text": "List the language used least number of TV Channel. List language and number of TV Channel.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q791_idx791",
    "nl_query_text": "What are the names of the teachers whose courses have not been arranged?",
    "true_db_id": "course_teach",
    "predicted_db_id": "course_teach",
    "raw_model_output": "The most relevant database_id is: course_teach"
  },
  {
    "experiment_query_id": "spider_dev_q792_idx792",
    "nl_query_text": "Return the number of likes that the high schooler named Kyle has.",
    "true_db_id": "network_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "None of the given database schemas contain information about high schoolers or likes."
  },
  {
    "experiment_query_id": "spider_dev_q793_idx793",
    "nl_query_text": "List the airport code and name in the city of Anthony.",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q794_idx794",
    "nl_query_text": "Find the name of tourney that has more than 10 matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "match_season"
  },
  {
    "experiment_query_id": "spider_dev_q795_idx795",
    "nl_query_text": "What region does Angola belong to and what is its population?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q796_idx796",
    "nl_query_text": "What are the id, name and membership level of visitors who have spent the largest amount of money in total in all museum tickets?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "museum_visit",
    "raw_model_output": "museum_visit"
  },
  {
    "experiment_query_id": "spider_dev_q797_idx797",
    "nl_query_text": "Find the government form name and total population for each government form whose average life expectancy is longer than 72.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q798_idx798",
    "nl_query_text": "Return the names of friends of the high school student Kyle.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "# The most relevant database_id is: network_2"
  },
  {
    "experiment_query_id": "spider_dev_q799_idx799",
    "nl_query_text": "Which countries in europe have at least 3 car manufacturers?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q800_idx800",
    "nl_query_text": "What are the ids, version numbers, and type codes for each template?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q801_idx801",
    "nl_query_text": "What are the names of the contestants whose names are not 'Jessie Alloway'",
    "true_db_id": "voter_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What are the names of the"
  },
  {
    "experiment_query_id": "spider_dev_q802_idx802",
    "nl_query_text": "What are the names of high schoolers who both have friends and are liked?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q803_idx803",
    "nl_query_text": "What is the airport name for airport 'AKO'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q804_idx804",
    "nl_query_text": "Find the name of the winner who has the highest rank points and participated in the Australian Open tourney.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q805_idx805",
    "nl_query_text": "Which countries have either English or Dutch as an official language?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "The most relevant database_id is: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q806_idx806",
    "nl_query_text": "How many car models are produced in the usa?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q807_idx807",
    "nl_query_text": "What is the average, minimum, and maximum age for all French singers?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q808_idx808",
    "nl_query_text": "What are the Asian countries which have a population larger than that of any country in Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q809_idx809",
    "nl_query_text": "How many countries does each continent have? List the continent id, continent name and the number of countries.",
    "true_db_id": "car_1",
    "predicted_db_id": "geo",
    "raw_model_output": "database_id: geo"
  },
  {
    "experiment_query_id": "spider_dev_q810_idx810",
    "nl_query_text": "What are the codes of countries with more than 50 players?",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_2",
    "raw_model_output": "The most relevant database_id is: soccer_2"
  },
  {
    "experiment_query_id": "spider_dev_q811_idx811",
    "nl_query_text": "What are the names of the singers who are not French citizens?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q812_idx812",
    "nl_query_text": "Which professionals have done at least two treatments? List the professional's id, role, and first name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "medicine_enzyme_interaction",
    "raw_model_output": "database_id: medicine_enzyme_interaction"
  },
  {
    "experiment_query_id": "spider_dev_q813_idx813",
    "nl_query_text": "Show the record companies shared by orchestras founded before 2003 and after 2003.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q814_idx814",
    "nl_query_text": "Find the number of distinct type of pets.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 6"
  },
  {
    "experiment_query_id": "spider_dev_q815_idx815",
    "nl_query_text": "Find the cities that have more than one employee under age 30.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you analyze the database schemas to determine which one"
  },
  {
    "experiment_query_id": "spider_dev_q816_idx816",
    "nl_query_text": "What is the name of the high schooler who has the greatest number of likes?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q817_idx817",
    "nl_query_text": "Show names of all high school students who do not have any friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "The most relevant database_id is: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q818_idx818",
    "nl_query_text": "What languages are only used by a single country with a republic government?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "The most relevant database_id is: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q819_idx819",
    "nl_query_text": "What is the average earnings of poker players with height higher than 200?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q820_idx820",
    "nl_query_text": "How much does the car accelerate that makes amc hornet sportabout (sw)?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q821_idx821",
    "nl_query_text": "Show the date of the transcript which shows the least number of results, also list the id.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q822_idx822",
    "nl_query_text": "How many documents are using the template with type code 'PPT'?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q823_idx823",
    "nl_query_text": "Which model saves the most gasoline? That is to say, have the maximum miles per gallon.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q824_idx824",
    "nl_query_text": "Find number of pets owned by students who are older than 20.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code, but I can help with determining the most relevant database."
  },
  {
    "experiment_query_id": "spider_dev_q825_idx825",
    "nl_query_text": "What are the students' first names who have both cats and dogs as pets?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't help you with that. The provided text does not contain any information about students having cats"
  },
  {
    "experiment_query_id": "spider_dev_q826_idx826",
    "nl_query_text": "Please show the different record companies and the corresponding number of orchestras.",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q827_idx827",
    "nl_query_text": "Count the number of paragraphs in the document named 'Summer Show'.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I can't assist you with this request since it is not a mathematical problem"
  },
  {
    "experiment_query_id": "spider_dev_q828_idx828",
    "nl_query_text": "What are the distinct battle names which are between bulgarian commander 'Kaloyan' and latin commander 'Baldwin I'?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q829_idx829",
    "nl_query_text": "What is the age and hometown of every teacher?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code you provided. However, I can help you identify the most relevant database"
  },
  {
    "experiment_query_id": "spider_dev_q830_idx830",
    "nl_query_text": "What is the car model with the highest mpg ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q831_idx831",
    "nl_query_text": "Show all paragraph texts for the document \"Customer reviews\".",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a direct answer to the question. However, I can help you"
  },
  {
    "experiment_query_id": "spider_dev_q832_idx832",
    "nl_query_text": "What are airlines that have flights arriving at airport 'AHD'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "The most relevant database_id is: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q833_idx833",
    "nl_query_text": "What are the ids and names of the battles that led to more than 10 people killed in total.",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q834_idx834",
    "nl_query_text": "How many cars have more than 4 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "The most relevant database_id is: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q835_idx835",
    "nl_query_text": "Show the property type descriptions of properties belonging to that code.",
    "true_db_id": "real_estate_properties",
    "predicted_db_id": "real_estate_properties",
    "raw_model_output": "database_id: real_estate_properties"
  },
  {
    "experiment_query_id": "spider_dev_q836_idx836",
    "nl_query_text": "How many type of governments are in Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "The most relevant database_id is: world_1"
  },
  {
    "experiment_query_id": "spider_dev_q837_idx837",
    "nl_query_text": "Find the id of the pet owned by student whose last name is \u2018Smith\u2019.",
    "true_db_id": "pets_1",
    "predicted_db_id": "student_1",
    "raw_model_output": "database_id: student_1"
  },
  {
    "experiment_query_id": "spider_dev_q838_idx838",
    "nl_query_text": "Find all airlines that have at least 10 flights.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q839_idx839",
    "nl_query_text": "Which dogs are of the rarest breed? Show their names and treatment dates.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code you provided. It seems like you want me to find the most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q840_idx840",
    "nl_query_text": "How many singers do we have?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q841_idx841",
    "nl_query_text": "What are the makers and models?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q842_idx842",
    "nl_query_text": "Find the total number of tours for each ranking date.",
    "true_db_id": "wta_1",
    "predicted_db_id": "formula_1",
    "raw_model_output": "The most relevant database_id is: formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q843_idx843",
    "nl_query_text": "How many degrees does the engineering department have?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "scientist_1",
    "raw_model_output": "The most relevant database_id is: scientist_1"
  },
  {
    "experiment_query_id": "spider_dev_q844_idx844",
    "nl_query_text": "What are the names of people who are not from Russia?",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "database_id: poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q845_idx845",
    "nl_query_text": "How many transcripts are listed?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q846_idx846",
    "nl_query_text": "Find the first name, country code and birth date of the winner who has the highest rank points in all matches.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q847_idx847",
    "nl_query_text": "find the minimum and maximum number of products of all stores.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_store",
    "raw_model_output": "department_store"
  },
  {
    "experiment_query_id": "spider_dev_q848_idx848",
    "nl_query_text": "How many pets have a greater weight than 10?",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q849_idx849",
    "nl_query_text": "Show names of teachers that teach at least two courses.",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but the provided information does not contain any question asking to show names of teachers that"
  },
  {
    "experiment_query_id": "spider_dev_q850_idx850",
    "nl_query_text": "What are the different ids and names of the battles that lost any 'Brig' type shipes?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q851_idx851",
    "nl_query_text": "Give the names of the nations that were founded after 1950.",
    "true_db_id": "world_1",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q852_idx852",
    "nl_query_text": "What are the full names of all left handed players, in order of birth date?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"What are the full names of all left handed players, in order of birth"
  },
  {
    "experiment_query_id": "spider_dev_q853_idx853",
    "nl_query_text": "What are the region and population of Angola?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q854_idx854",
    "nl_query_text": "How many cars has over 6 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q855_idx855",
    "nl_query_text": "What is the first, middle, and last name, along with the id and number of enrollments, for the student who enrolled the most in any program?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q856_idx856",
    "nl_query_text": "What are the names and grades for each high schooler?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q857_idx857",
    "nl_query_text": "What are flight numbers of flights arriving at City \"Aberdeen\"?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q858_idx858",
    "nl_query_text": "Show countries where a singer above age 40 and a singer below 30 are from.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q859_idx859",
    "nl_query_text": "What are the names of the nations with the 3 lowest populations?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q860_idx860",
    "nl_query_text": "What is the document name and template id for document with description with the letter 'w' in it?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q861_idx861",
    "nl_query_text": "How many teachers are there?",
    "true_db_id": "course_teach",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q862_idx862",
    "nl_query_text": "How much does the most recent treatment cost?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "machine_repair",
    "raw_model_output": "The most relevant database_id is: machine_repair"
  },
  {
    "experiment_query_id": "spider_dev_q863_idx863",
    "nl_query_text": "List all cartoon directed by \"Ben Jones\".",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q864_idx864",
    "nl_query_text": "Show the names of all of the high schooler Kyle's friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q865_idx865",
    "nl_query_text": "What is the maximum capacity and the average of all stadiums ?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: stadium"
  },
  {
    "experiment_query_id": "spider_dev_q866_idx866",
    "nl_query_text": "What is the content of the series Sky Radio?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q867_idx867",
    "nl_query_text": "What are the names of the singers who performed in a concert in 2014?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q868_idx868",
    "nl_query_text": "How many flights have destination ATO?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q869_idx869",
    "nl_query_text": "Which employee received the most awards in evaluations? Give me the employee name.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"Which employee received the most awards in evaluations? Give me the employee name.\","
  },
  {
    "experiment_query_id": "spider_dev_q870_idx870",
    "nl_query_text": "What are the major record formats of orchestras, sorted by their frequency?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q871_idx871",
    "nl_query_text": "What are the ids of documents that contain the paragraph text 'Brazil' and 'Ireland'?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not determine the most relevant database_id for the given question."
  },
  {
    "experiment_query_id": "spider_dev_q872_idx872",
    "nl_query_text": "Return the type code of the template type that the most templates belong to.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q873_idx873",
    "nl_query_text": "Give the name of the nation that uses the greatest amount of languages.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q874_idx874",
    "nl_query_text": "How many paragraphs in total?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide the result of executing a code. I can help you find the"
  },
  {
    "experiment_query_id": "spider_dev_q875_idx875",
    "nl_query_text": "Return the code of the template type that is most commonly used in documents.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q876_idx876",
    "nl_query_text": "What is the year that had the most concerts?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q877_idx877",
    "nl_query_text": "What are the names and locations of the stadiums that had concerts that occurred in both 2014 and 2015?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q878_idx878",
    "nl_query_text": "What is the area code in which the most voters voted?",
    "true_db_id": "voter_1",
    "predicted_db_id": "voter_2",
    "raw_model_output": "voter_2"
  },
  {
    "experiment_query_id": "spider_dev_q879_idx879",
    "nl_query_text": "Show names, results and bulgarian commanders of the battles with no ships lost in the 'English Channel'.",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q880_idx880",
    "nl_query_text": "What is the number of cars with a horsepower greater than 150?",
    "true_db_id": "car_1",
    "predicted_db_id": "wine_1",
    "raw_model_output": "wine_1"
  },
  {
    "experiment_query_id": "spider_dev_q881_idx881",
    "nl_query_text": "What are the cost and treatment type description of each treatment?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you analyze the database schemas to find the most"
  },
  {
    "experiment_query_id": "spider_dev_q882_idx882",
    "nl_query_text": "What are the ids and makers of all car makers that produce at least 2 models and make more than 3 cars?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q883_idx883",
    "nl_query_text": "How many different nationalities do conductors have?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q884_idx884",
    "nl_query_text": "Return the codes of countries for which Spanish is the predominantly spoken language.",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the given question based on the provided schema information"
  },
  {
    "experiment_query_id": "spider_dev_q885_idx885",
    "nl_query_text": "Count the number of different templates used for documents.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q886_idx886",
    "nl_query_text": "What are the different continents and the total popuation and average life expectancy corresponding to each, for continents that have an average life expectancy less than 72?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q887_idx887",
    "nl_query_text": "Find the total ranking points for each player and their first name.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "The most relevant database_id is: soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q888_idx888",
    "nl_query_text": "How long is the people\u2019s average life expectancy in Central Africa?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't answer that question because it requires a specific database schema to be examined, but the question"
  },
  {
    "experiment_query_id": "spider_dev_q889_idx889",
    "nl_query_text": "Find the last name of the students who currently live in the state of North Carolina but have not registered in any degree program.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "database_id: student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q890_idx890",
    "nl_query_text": "List the cost of each treatment and the corresponding treatment type description.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "medicine_enzyme_interaction",
    "raw_model_output": "medicine_enzyme_interaction"
  },
  {
    "experiment_query_id": "spider_dev_q891_idx891",
    "nl_query_text": "How many high schoolers are in each grade?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q892_idx892",
    "nl_query_text": "What are the ids and texts of paragraphs in the document titled 'Welcome to NY'?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not determine the most relevant database for the question \"What are the ids and texts of paragraphs"
  },
  {
    "experiment_query_id": "spider_dev_q893_idx893",
    "nl_query_text": "For each citizenship, what is the maximum net worth?",
    "true_db_id": "singer",
    "predicted_db_id": "singer",
    "raw_model_output": "The most relevant database_id is: singer"
  },
  {
    "experiment_query_id": "spider_dev_q894_idx894",
    "nl_query_text": "What is the average edispl of the cars of model volvo?",
    "true_db_id": "car_1",
    "predicted_db_id": "wine_1",
    "raw_model_output": "wine_1"
  },
  {
    "experiment_query_id": "spider_dev_q895_idx895",
    "nl_query_text": "How many different degree names are offered?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "college_3",
    "raw_model_output": "college_3"
  },
  {
    "experiment_query_id": "spider_dev_q896_idx896",
    "nl_query_text": "Show the name of singers whose birth year is either 1948 or 1949?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q897_idx897",
    "nl_query_text": "Which airlines have less than 200 flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q898_idx898",
    "nl_query_text": "How many singers are there?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q899_idx899",
    "nl_query_text": "What are the names of properties that are either houses or apartments with more than 1 room?",
    "true_db_id": "real_estate_properties",
    "predicted_db_id": "apartment_rentals",
    "raw_model_output": "The most relevant database_id is: apartment_rentals"
  },
  {
    "experiment_query_id": "spider_dev_q900_idx900",
    "nl_query_text": "What are the type codes and descriptions for all template types?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q901_idx901",
    "nl_query_text": "What is the total count of teachers?",
    "true_db_id": "course_teach",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q902_idx902",
    "nl_query_text": "What are the names of conductors, sorted descending by the number of years they have worked?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q903_idx903",
    "nl_query_text": "Find the number of left handed winners who participated in the WTA Championships.",
    "true_db_id": "wta_1",
    "predicted_db_id": "wta_1",
    "raw_model_output": "wta_1"
  },
  {
    "experiment_query_id": "spider_dev_q904_idx904",
    "nl_query_text": "Return the ids corresponding to templates with the description 'Presentation'.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q905_idx905",
    "nl_query_text": "What region is Kabul in?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q906_idx906",
    "nl_query_text": "Show all document ids, names and the number of paragraphs in each document.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q907_idx907",
    "nl_query_text": "What is the best rank of losers across all matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "game_1",
    "raw_model_output": "game_1"
  },
  {
    "experiment_query_id": "spider_dev_q908_idx908",
    "nl_query_text": "How many airports do we have?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q909_idx909",
    "nl_query_text": "What are the ids of templates with template type code PP or PPT?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q910_idx910",
    "nl_query_text": "How many visitors below age 30 are there?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"How many visitors below age"
  },
  {
    "experiment_query_id": "spider_dev_q911_idx911",
    "nl_query_text": "Show the name and theme for all concerts and the number of singers in each concert.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q912_idx912",
    "nl_query_text": "List the maximum weight and type for each type of pet.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q913_idx913",
    "nl_query_text": "Return the name of the high school student with the most friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q914_idx914",
    "nl_query_text": "What is the most populace city that speaks English?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining the database schemas provided, I found that the most relevant database for answering the question \"What"
  },
  {
    "experiment_query_id": "spider_dev_q915_idx915",
    "nl_query_text": "Give the city and country for the Alton airport.",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport_id = airports.airport_id"
  },
  {
    "experiment_query_id": "spider_dev_q916_idx916",
    "nl_query_text": "List the names of people that are not poker players.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q917_idx917",
    "nl_query_text": "Return the document id, template id, and description for the document with the name Robbin CV.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Tracking_DB",
    "raw_model_output": "cre_Doc_Tracking_DB"
  },
  {
    "experiment_query_id": "spider_dev_q918_idx918",
    "nl_query_text": "Show names of people whose nationality is not \"Russia\".",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"Show names of people whose nationality"
  },
  {
    "experiment_query_id": "spider_dev_q919_idx919",
    "nl_query_text": "How much does the youngest dog weigh?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer the question \"How much does the youngest dog weigh?\", we need to find the database with"
  },
  {
    "experiment_query_id": "spider_dev_q920_idx920",
    "nl_query_text": "What are the template ids with template type description \"Presentation\".",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "database_id: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q921_idx921",
    "nl_query_text": "Find the arriving date and the departing date of the dogs that received a treatment.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot determine the most relevant database for the question \"Find the arriving date"
  },
  {
    "experiment_query_id": "spider_dev_q922_idx922",
    "nl_query_text": "What is the description of the treatment type that costs the least money in total?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "After examining all the database schemas provided above, I found that the most relevant database for answering the question"
  },
  {
    "experiment_query_id": "spider_dev_q923_idx923",
    "nl_query_text": "What are the id and name of the museum visited most times?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm just an AI, I don't have the capability to access or browse external links. However"
  },
  {
    "experiment_query_id": "spider_dev_q924_idx924",
    "nl_query_text": "Return the birth date of the poker player with the lowest earnings.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q925_idx925",
    "nl_query_text": "What are the record companies of orchestras in descending order of years in which they were founded?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q926_idx926",
    "nl_query_text": "What are the record companies that are used by both orchestras founded before 2003 and those founded after 2003?",
    "true_db_id": "orchestra",
    "predicted_db_id": "orchestra",
    "raw_model_output": "orchestra"
  },
  {
    "experiment_query_id": "spider_dev_q927_idx927",
    "nl_query_text": "Which owner owns the most dogs? List the owner id, first name and last name.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't execute SQL code or access external databases, but I can help you analyze the provided databases"
  },
  {
    "experiment_query_id": "spider_dev_q928_idx928",
    "nl_query_text": "What are the package options and the name of the series for the TV Channel that supports high definition TV?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q929_idx929",
    "nl_query_text": "How many countries are listed?",
    "true_db_id": "car_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q930_idx930",
    "nl_query_text": "Find the name, population and expected life length of asian country with the largest area?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country"
  },
  {
    "experiment_query_id": "spider_dev_q931_idx931",
    "nl_query_text": "How many players are there for each hand type?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or access external links. However, I can help you analyze the given database"
  },
  {
    "experiment_query_id": "spider_dev_q932_idx932",
    "nl_query_text": "What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "phone_1",
    "raw_model_output": "phone_1"
  },
  {
    "experiment_query_id": "spider_dev_q933_idx933",
    "nl_query_text": "Show the ID of the high schooler named Kyle.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "database_id: network_1"
  },
  {
    "experiment_query_id": "spider_dev_q934_idx934",
    "nl_query_text": "What is the name of the singer who is worth the most?",
    "true_db_id": "singer",
    "predicted_db_id": "music_2",
    "raw_model_output": "music_2"
  },
  {
    "experiment_query_id": "spider_dev_q935_idx935",
    "nl_query_text": "What type of pet is the youngest animal, and how much does it weigh?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is not enough information to determine which database is most relevant to the question"
  },
  {
    "experiment_query_id": "spider_dev_q936_idx936",
    "nl_query_text": "What are the names of the stadiums without any concerts?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine which database is most relevant for answering the question without more information about the"
  },
  {
    "experiment_query_id": "spider_dev_q937_idx937",
    "nl_query_text": "What is the age of the oldest dog?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you analyze the database schemas to find the most"
  },
  {
    "experiment_query_id": "spider_dev_q938_idx938",
    "nl_query_text": "how many countries are in Asia?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q939_idx939",
    "nl_query_text": "What is the total number of countries where Spanish is spoken by the largest percentage of people?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot determine the most relevant database for the given question. The question requires"
  },
  {
    "experiment_query_id": "spider_dev_q940_idx940",
    "nl_query_text": "Return the nationalities for which there are two or more people.",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can't execute SQL code. However, I can help you determine which database schema is most relevant"
  },
  {
    "experiment_query_id": "spider_dev_q941_idx941",
    "nl_query_text": "Show the date and id of the transcript with at least 2 course results.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q942_idx942",
    "nl_query_text": "Find the maximum weight for each type of pet. List the maximum weight and pet type.",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q943_idx943",
    "nl_query_text": "What is the total number of people living in the nations that do not use English?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What is the total number of"
  },
  {
    "experiment_query_id": "spider_dev_q944_idx944",
    "nl_query_text": "Count the number of high schoolers in grades 9 or 10.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q945_idx945",
    "nl_query_text": "Which African countries have a smaller population than that of any country in Asia?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q946_idx946",
    "nl_query_text": "What are the different years in which there were cars produced that weighed less than 4000 and also cars that weighted more than 3000 ?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q947_idx947",
    "nl_query_text": "What are the towns from which at least two teachers come from?",
    "true_db_id": "course_teach",
    "predicted_db_id": "student_1",
    "raw_model_output": "The most relevant database_id is: student_1"
  },
  {
    "experiment_query_id": "spider_dev_q948_idx948",
    "nl_query_text": "Return the grade that has the greatest number of high schoolers.",
    "true_db_id": "network_1",
    "predicted_db_id": "college_3",
    "raw_model_output": "database_id: college_3"
  },
  {
    "experiment_query_id": "spider_dev_q949_idx949",
    "nl_query_text": "Find the districts in which there are both shops selling less than 3000 products and shops selling more than 10000 products.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute the code. However, I can help you find the most relevant database schema."
  },
  {
    "experiment_query_id": "spider_dev_q950_idx950",
    "nl_query_text": "What are the names of singers ordered by ascending net worth?",
    "true_db_id": "singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "music_1"
  },
  {
    "experiment_query_id": "spider_dev_q951_idx951",
    "nl_query_text": "What are the names of tournaments that have more than 10 matches?",
    "true_db_id": "wta_1",
    "predicted_db_id": "formula_1",
    "raw_model_output": "database_id: formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q952_idx952",
    "nl_query_text": "How many pets are owned by students that have an age greater than 20?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the given question as I cannot access the actual"
  },
  {
    "experiment_query_id": "spider_dev_q953_idx953",
    "nl_query_text": "What are the first, middle, and last names, along with the ids, of all students who enrolled in 2 degree programs in one semester?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q954_idx954",
    "nl_query_text": "Find all airlines that have fewer than 200 flights.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q955_idx955",
    "nl_query_text": "What are the different addresses that have students living there?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "database_id: student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q956_idx956",
    "nl_query_text": "What are the email, cell phone and home phone of each professional?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 13"
  },
  {
    "experiment_query_id": "spider_dev_q957_idx957",
    "nl_query_text": "What are the names of the teachers and how many courses do they teach?",
    "true_db_id": "course_teach",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q958_idx958",
    "nl_query_text": "What are the languages used by the least number of TV Channels and how many channels use it?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q959_idx959",
    "nl_query_text": "What is the degree summary name that has the most number of students enrolled?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q960_idx960",
    "nl_query_text": "What are the population, name and leader of the country with the largest area?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q961_idx961",
    "nl_query_text": "Find the owner id and zip code of the owner who spent the most money in total for his or her dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "body_builder",
    "raw_model_output": "database_id: body_builder"
  },
  {
    "experiment_query_id": "spider_dev_q962_idx962",
    "nl_query_text": "Whose permanent address is different from his or her current address? List his or her first name.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot answer this question as it appears to be incomplete. The question seems"
  },
  {
    "experiment_query_id": "spider_dev_q963_idx963",
    "nl_query_text": "Find the name and location of the stadiums which some concerts happened in the years of both 2014 and 2015.",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_1",
    "raw_model_output": "database_id: music_1"
  },
  {
    "experiment_query_id": "spider_dev_q964_idx964",
    "nl_query_text": "What is the maximum accelerate for different number of cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "database_id: car_1"
  },
  {
    "experiment_query_id": "spider_dev_q965_idx965",
    "nl_query_text": "What is the horsepower of the car with the greatest accelerate?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q966_idx966",
    "nl_query_text": "Find the number of dog pets that are raised by female students (with sex F).",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but I cannot provide a specific database_id. However, I can guide you on"
  },
  {
    "experiment_query_id": "spider_dev_q967_idx967",
    "nl_query_text": "How many battles did not lose any ship with tonnage '225'?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "The most relevant database_id is: battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q968_idx968",
    "nl_query_text": "Find the average grade of all students who have some friends.",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q969_idx969",
    "nl_query_text": "What are all of the episodes ordered by ratings?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q970_idx970",
    "nl_query_text": "Find the number of professionals who have not treated any dogs.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q971_idx971",
    "nl_query_text": "What is the largest amount of horsepower for the models with 3 cylinders and what make is it?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: 1"
  },
  {
    "experiment_query_id": "spider_dev_q972_idx972",
    "nl_query_text": "What is the grade of each high schooler?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_1",
    "raw_model_output": "network_1"
  },
  {
    "experiment_query_id": "spider_dev_q973_idx973",
    "nl_query_text": "How many different results are there for the battles?",
    "true_db_id": "battle_death",
    "predicted_db_id": "battle_death",
    "raw_model_output": "battle_death"
  },
  {
    "experiment_query_id": "spider_dev_q974_idx974",
    "nl_query_text": "Return the number of  airports.",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "airport"
  },
  {
    "experiment_query_id": "spider_dev_q975_idx975",
    "nl_query_text": "Find the last name of the student who has a cat that is age 3.",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the given question as I don't have enough"
  },
  {
    "experiment_query_id": "spider_dev_q976_idx976",
    "nl_query_text": "List the title of all cartoons in alphabetical order.",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q977_idx977",
    "nl_query_text": "What are all the song names by singers who are older than average?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q978_idx978",
    "nl_query_text": "What is the version number and template type code for the template with version number later than 5?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q979_idx979",
    "nl_query_text": "How many car makers are there in each continents? List the continent name and the count.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q980_idx980",
    "nl_query_text": "Which professionals have operated a treatment that costs less than the average? Give me theor first names and last names.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "assets_maintenance",
    "raw_model_output": "The most relevant database_id is: assets_maintenance"
  },
  {
    "experiment_query_id": "spider_dev_q981_idx981",
    "nl_query_text": "What are the codes of template types that are not used for any document?",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "cre_Doc_Template_Mgt",
    "raw_model_output": "The most relevant database_id is: cre_Doc_Template_Mgt"
  },
  {
    "experiment_query_id": "spider_dev_q982_idx982",
    "nl_query_text": "How many flights depart from 'APG'?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "database_id: flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q983_idx983",
    "nl_query_text": "Sort employee names by their age in ascending order.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "department_management",
    "raw_model_output": "department_management"
  },
  {
    "experiment_query_id": "spider_dev_q984_idx984",
    "nl_query_text": "Give the mean GNP and total population of nations which are considered US territory.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q985_idx985",
    "nl_query_text": "Which continent is Anguilla in?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q986_idx986",
    "nl_query_text": "List the earnings of poker players in descending order.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q987_idx987",
    "nl_query_text": "What is the maximum horsepower and the make of the car models with 3 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q988_idx988",
    "nl_query_text": "What are the country codes of the different countries, and what are the languages spoken by the greatest percentage of people for each?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_id: country_public_safety"
  },
  {
    "experiment_query_id": "spider_dev_q989_idx989",
    "nl_query_text": "find the name of employee who was awarded the most times in the evaluation.",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "employee_hire_evaluation",
    "raw_model_output": "database_id: employee_hire_evaluation"
  },
  {
    "experiment_query_id": "spider_dev_q990_idx990",
    "nl_query_text": "How many countries have governments that are republics?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q991_idx991",
    "nl_query_text": "When was the last transcript released?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q992_idx992",
    "nl_query_text": "Find the role, street, city and state of the professionals living in a city that contains the substring 'West'.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q993_idx993",
    "nl_query_text": "Return the version numbers and template type codes of templates with a version number greater than 5.",
    "true_db_id": "cre_Doc_Template_Mgt",
    "predicted_db_id": "formula_1",
    "raw_model_output": "formula_1"
  },
  {
    "experiment_query_id": "spider_dev_q994_idx994",
    "nl_query_text": "Which flight numbers correspond to United Airlines flights?",
    "true_db_id": "flight_2",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "database_2"
  },
  {
    "experiment_query_id": "spider_dev_q995_idx995",
    "nl_query_text": "What are the names of the teachers whose hometown is not `` Little Lever Urban District '' ?",
    "true_db_id": "course_teach",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To answer this question, we need to find the database that contains information about teachers and their hometowns"
  },
  {
    "experiment_query_id": "spider_dev_q996_idx996",
    "nl_query_text": "Show titles of songs and names of singers.",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q997_idx997",
    "nl_query_text": "What are the average and maximum number of tickets bought in all visits?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "To determine which database is most relevant for answering the question, we need to look for the database that"
  },
  {
    "experiment_query_id": "spider_dev_q998_idx998",
    "nl_query_text": "What is the first name and gender of the all the students who have more than one pet?",
    "true_db_id": "pets_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you analyze the given question and suggest which database"
  },
  {
    "experiment_query_id": "spider_dev_q999_idx999",
    "nl_query_text": "Count the number of employees",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "company_1",
    "raw_model_output": "company_1"
  },
  {
    "experiment_query_id": "spider_dev_q1000_idx1000",
    "nl_query_text": "Which dogs have not cost their owner more than 1000 for treatment ? List the dog names .",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code, but I can help you with your problem."
  },
  {
    "experiment_query_id": "spider_dev_q1001_idx1001",
    "nl_query_text": "Return the country name and the numbers of languages spoken for each country that speaks at least 3 languages.",
    "true_db_id": "world_1",
    "predicted_db_id": "geo",
    "raw_model_output": "geo"
  },
  {
    "experiment_query_id": "spider_dev_q1002_idx1002",
    "nl_query_text": "Who is enrolled in a Bachelor degree program? List the first name, middle name, last name.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q1003_idx1003",
    "nl_query_text": "What are all the possible breed type and size type combinations?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a specific database_id, but I can help you analyze the schema"
  },
  {
    "experiment_query_id": "spider_dev_q1004_idx1004",
    "nl_query_text": "Who is the earliest graduate of the school? List the first name, middle name and last name.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q1005_idx1005",
    "nl_query_text": "What are the titles of all cartoons directed by Ben Jones or Brandon Vietti?",
    "true_db_id": "tvshow",
    "predicted_db_id": "tvshow",
    "raw_model_output": "tvshow"
  },
  {
    "experiment_query_id": "spider_dev_q1006_idx1006",
    "nl_query_text": "What is the name of the winner who has won the most matches, and how many rank points does this player have?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "driver_standings."
  },
  {
    "experiment_query_id": "spider_dev_q1007_idx1007",
    "nl_query_text": "What are the countries having at least one car maker? List name and id.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q1008_idx1008",
    "nl_query_text": "Find the first names that are used for professionals or owners but are not used as dog names.",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to identify the most relevant database for the given question. The question requires a database"
  },
  {
    "experiment_query_id": "spider_dev_q1009_idx1009",
    "nl_query_text": "Which states have both owners and professionals living there?",
    "true_db_id": "dog_kennels",
    "predicted_db_id": "dog_kennels",
    "raw_model_output": "database_id: dog_kennels"
  },
  {
    "experiment_query_id": "spider_dev_q1010_idx1010",
    "nl_query_text": "What is the name of the high schooler who has the greatest number of friends?",
    "true_db_id": "network_1",
    "predicted_db_id": "network_2",
    "raw_model_output": "network_2"
  },
  {
    "experiment_query_id": "spider_dev_q1011_idx1011",
    "nl_query_text": "What are the nationalities that are shared by at least two people?",
    "true_db_id": "poker_player",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "The most relevant database_id is: immigration_1"
  },
  {
    "experiment_query_id": "spider_dev_q1012_idx1012",
    "nl_query_text": "What is the total number of unique official languages spoken in the countries that are founded before 1930?",
    "true_db_id": "world_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code, but I can help with a text-based solution."
  },
  {
    "experiment_query_id": "spider_dev_q1013_idx1013",
    "nl_query_text": "What is the average transcript date?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_transcripts_tracking",
    "raw_model_output": "student_transcripts_tracking"
  },
  {
    "experiment_query_id": "spider_dev_q1014_idx1014",
    "nl_query_text": "Which cities do more than one employee under age 30 come from?",
    "true_db_id": "employee_hire_evaluation",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm sorry, but there is not enough information to determine which database is most relevant for answering the"
  },
  {
    "experiment_query_id": "spider_dev_q1015_idx1015",
    "nl_query_text": "What are the last name of the students who live in North Carolina but have not registered in any degree programs?",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_assessment",
    "raw_model_output": "database_id: student_assessment"
  },
  {
    "experiment_query_id": "spider_dev_q1016_idx1016",
    "nl_query_text": "What are the full names of all players, sorted by birth date?",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I'm not able to determine the most relevant database for the question \"What are the full names of"
  },
  {
    "experiment_query_id": "spider_dev_q1017_idx1017",
    "nl_query_text": "What are the birth year and citizenship of singers?",
    "true_db_id": "singer",
    "predicted_db_id": "music_4",
    "raw_model_output": "music_4"
  },
  {
    "experiment_query_id": "spider_dev_q1018_idx1018",
    "nl_query_text": "Find the number of matches happened in each year.",
    "true_db_id": "wta_1",
    "predicted_db_id": "match_season",
    "raw_model_output": "match_season"
  },
  {
    "experiment_query_id": "spider_dev_q1019_idx1019",
    "nl_query_text": "Which city is the most frequent source airport?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q1020_idx1020",
    "nl_query_text": "What is the total ticket expense of the visitors whose membership level is 1?",
    "true_db_id": "museum_visit",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or provide a direct answer to the question. However, I can help you"
  },
  {
    "experiment_query_id": "spider_dev_q1021_idx1021",
    "nl_query_text": "what is the name and nation of the singer who have a song having 'Hey' in its name?",
    "true_db_id": "concert_singer",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code or have access to the provided database schemas."
  },
  {
    "experiment_query_id": "spider_dev_q1022_idx1022",
    "nl_query_text": "Find the average ranking for each player and their first name.",
    "true_db_id": "wta_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you determine which database is most relevant for the"
  },
  {
    "experiment_query_id": "spider_dev_q1023_idx1023",
    "nl_query_text": "List all the student details in reversed lexicographical order.",
    "true_db_id": "student_transcripts_tracking",
    "predicted_db_id": "student_1",
    "raw_model_output": "student_1"
  },
  {
    "experiment_query_id": "spider_dev_q1024_idx1024",
    "nl_query_text": "What is the average miles per gallon of all the cards with 4 cylinders?",
    "true_db_id": "car_1",
    "predicted_db_id": "extraction_error",
    "raw_model_output": "I can not execute code. However, I can help you to determine which database is most relevant for"
  },
  {
    "experiment_query_id": "spider_dev_q1025_idx1025",
    "nl_query_text": "Which airlines have a flight with destination airport AHD?",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q1026_idx1026",
    "nl_query_text": "What are the names of the countries with no car makers?",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  },
  {
    "experiment_query_id": "spider_dev_q1027_idx1027",
    "nl_query_text": "Find the total number of players.",
    "true_db_id": "wta_1",
    "predicted_db_id": "soccer_1",
    "raw_model_output": "soccer_1"
  },
  {
    "experiment_query_id": "spider_dev_q1028_idx1028",
    "nl_query_text": "What is the average weight for each type of pet?",
    "true_db_id": "pets_1",
    "predicted_db_id": "pets_1",
    "raw_model_output": "pets_1"
  },
  {
    "experiment_query_id": "spider_dev_q1029_idx1029",
    "nl_query_text": "Give the name of the country in Asia with the lowest life expectancy.",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q1030_idx1030",
    "nl_query_text": "Give the average life expectancy for countries in Africa which are republics?",
    "true_db_id": "world_1",
    "predicted_db_id": "world_1",
    "raw_model_output": "world_1"
  },
  {
    "experiment_query_id": "spider_dev_q1031_idx1031",
    "nl_query_text": "Return the money rank of the poker player with the greatest height.",
    "true_db_id": "poker_player",
    "predicted_db_id": "poker_player",
    "raw_model_output": "poker_player"
  },
  {
    "experiment_query_id": "spider_dev_q1032_idx1032",
    "nl_query_text": "Give the flight numbers of flights leaving from Aberdeen.",
    "true_db_id": "flight_2",
    "predicted_db_id": "flight_2",
    "raw_model_output": "flight_2"
  },
  {
    "experiment_query_id": "spider_dev_q1033_idx1033",
    "nl_query_text": "Among the cars with more than lowest horsepower, which ones do not have more than 3 cylinders? List the car makeid and make name.",
    "true_db_id": "car_1",
    "predicted_db_id": "car_1",
    "raw_model_output": "car_1"
  }
]